## TRANSCRIPT
0:00     
hello uh hello sir good morning how are you good     
0:07     
morning all right hi hello all right well welcome um so I     
0:14     
guess uh Leila you had something you wanted to present yes sure uh     
0:47     
So ju just give me a second i'll share the screen a moment all right     
1:01     
hello so is my screen visible yes yes so yeah     
1:11     
so so this is basically a survey paper which uh gives us a brief introduction     
1:17     
about and uh and it gives in-depth analysis of various hypograph neural networks that were done till 2024 so it     
1:25     
was published in like a very good conference so this is uh this is probably the tier one conference in like     
1:33     
publishing like graph related works so this is ACM KDDD uh it was published uh     
1:38     
2024 and uh so we'll go detail into that and uh so we'll see like uh you know how     
1:45     
these can be helpful for our specific uh you know data and all so I have made a separate uh slides also which could     
1:52     
detail a little bit about uh you know uh I mean like how these are useful for us     
1:57     
and I have mentioned like uh the pros and cons related to you know how it can be like a pro for us procon for us and     
2:05     
for our specific data and I have some you know uh a glimpse of visualizations     
2:10     
which can explain a little bit of hypograph uh know this how can we model the high order interactions and all so     
2:17     
let's go first understand so the abstract so so higher order interactions     
2:23     
are ubiquitous in real world complex systems and applications so investigation of deep learning for high     
2:29     
order interactions become like a valuable agenda for data mining and machine learning communities and uh as     
2:36     
networks of high order interactions are expressed mathematically as hyperraphs     
2:41     
and this and like hyperraph neural networks have emerged like a powerful tool to understand repres     
2:47     
representations uh representation learning on hypographs so given this trend the the uh they have     
2:54     
presented the first survey for like hyperraph neural network with an in-depth step-by-step guide so here     
3:00     
first what they try to do is they break up the existing hyperraph into four design components so how do they design     
3:06     
like input features next input structures then message packing message     
3:11     
passing mechanisms and how do we train these training strategies so and also     
3:17     
they examine uh how these hypograph neural networks addresses and learns     
3:22     
different higher order interactions so and also they give a small overview of     
3:27     
related applications and recommendations for like medical science binformatics time series analysis computation and all     
3:34     
so this is broadly a detailed survey of like how hyperraph input structures     
3:39     
input features message packing mechanisms and also the how do we train these neural networks so this is     
3:44     
basically all about it so yeah so let's go into the introduction part so here     
3:50     
the high order interactions are like pervasive in real world complex systems and applications these relations     
3:56     
describe multi-way or group wise interactions so here we call our uh you     
4:01     
know the higher order interactions that may happen in our clans as anosis right     
4:06     
I mean I may be wrong but uh it's more like that so these anesmosis can be you     
4:14     
know can be seen and you know exploited using these hypography networks and from     
4:19     
the physical system and microbial communities and brain functions so so     
4:25     
for in with specific to that brain functions we consider the functional connectivity matrices or the structural     
4:32     
connectivity matrices and build the graphs upon it like instead of traditional graphs we apply hyperraphs     
4:37     
to understand higher order interactions among distinguished parts of the brain so that's one way of doing it so so when     
4:45     
I worked with Mayul we actually published a work into this space so so how these hypographs actually improve     
4:50     
the performance in know brain functionality for like autism spectrum and all so and social networks which is     
4:58     
commonly seen like the citations and all and uh higher order interactions in like     
5:03     
Facebook the Twitter the Pinterest and all so yeah the higher order interactions reveal uh several     
5:09     
structural patterns that we can see uh uh unobserved in like pair wise     
5:16     
counterparts because there is no uh you know uh that's it's it limited with the diic connections but whereas here we can     
5:23     
have like a you know a grouped connection called like hyper among all these things so we can more have like     
5:29     
more informed dynamics so so for example they have been shown to affect correlate with synchronization     
5:36     
physical system bacteria invasion inhibition and microbial communities cortical dynamics in the brain and uh     
5:42     
contagion in the social network so these are the various applications that can be seen and also we can apply to our uh     
5:48     
thing which we are working on which is C elegance you know embryogenesis so the hyperraph mathematically express the     
5:55     
higher order networks uh or higher order interactions where the nodes and the hyper edges     
6:02     
respectively uh represent the entities of the higher order interactions so we'll go detail into mathematics of how     
6:08     
these are represented in the later sections but right now we'll just see like you know the basics like what uh     
6:16     
existing literature have done so instead of having a straight two node edgetoedge     
6:21     
pair wise grass hyper edge can actually connect any number of nodes offering hyper edge uh hyperraphs as an advantage     
6:28     
in the descriptive path uh for instance we can see in the figure one where here     
6:33     
we have like co-author publication instead of having like a diic connection between these authors so we can have a     
6:40     
nice hyperraph which can give a more stronger intuition rather than a simple datic     
6:46     
connections and uh yes so next we'll see that hyperraphs are extensively utilized     
6:52     
and the demand grew and make predictions on them estimate hyper node uh     
6:57     
properties or identify missing hyper edges hypograph have shown the promise in solving such problems for example it     
7:05     
has shown like uh significant application including missing metabolic reactions and brain classification     
7:11     
traffic forecasting production recommendation and more so so before     
7:17     
delving deep into like you know these various things so they decided to you     
7:22     
know separate the hyperraph neural networks into like three main components first is encoding the training and the     
7:30     
application part so the encoding purely focuses on how do the hyperraph neural     
7:35     
network effectively capture these higher order interaction and next in the training how     
7:41     
do the how do encoded higher order interactions so these are like specific higher order interaction that they have     
7:47     
you know fixed with and now what are the training objectives that they wanted to     
7:52     
plug in to get like you know the desired outcome so so how are they doing that so     
8:00     
here they have given how do they model the higher order interactions this can be a nice intuition uh taxonomy they     
8:07     
have provided so here we can see that you know uh the external information can be     
8:14     
shared like a feature or a label and uh or one can take uh a     
8:20     
structural information or an identity information in order to encode uh you know the input features so that's     
8:28     
one way of doing and uh how do we design the input structure one we can use the     
8:34     
reductive transformation which may sometimes you know they are uh uh once we model the I mean like     
8:43     
hyperraph into the space sometimes you can't retrieve it back so these are deductive so non- dredective are like     
8:48     
star line and tensor and these are like click and adaptive so we'll go today so I'll today discuss about like how do we     
8:56     
encode the input feature and how do we encode the input structure so today's agenda would be discussing     
9:03     
very detailed into these two specific topics so next uh if time permits I'll     
9:08     
in the next week I'll detail about like different mechan uh message passing mechanisms and then how can we train in     
9:16     
order to know extract the best messages and you know able to model these     
9:21     
hyperraphs uh and understand these pair wise interactions what could be a best     
9:26     
objective to do that so yeah so for today's agenda we'll discuss about like     
9:32     
input features this red and the yellow component so yeah so preliminaries uh     
9:37     
these are the details of like uh this uh how do we represent so here uh as you     
9:44     
can see so G is the hyperraph we say so V here is the set of nodes and E is not     
9:50     
a direct connection but a hyper edge so instead of uh an adjacency matrix we     
9:56     
here have an incidence matrix where uh it is it has like a 01 components     
10:02     
whether there is a hyper edge connection or not and the shape of it is like v * v so this is the cardinality of uh you     
10:09     
know the node and the cardality of edges the shapes so x is the input feature or     
10:15     
the node feature that uh each and every node uh you know carries and y is the     
10:20     
hyper edge feature so what does the hyper edge carries the information that is also one of the important thing so     
10:27     
here PL is basically embedding of the node and QL is basically hyper edges at     
10:33     
each layer and how are they incident so this we're going to discuss later on so today's focus could be like till here     
10:40     
and we'll see like you know and N is basically the incident hyper edge of the node BI so this is I N represents N byN     
10:49     
identity matrix so which has all the diagonal values one and remaining upper triangle and lower triangle elements     
10:56     
zero so this is the indicator function so indicator function says that you know     
11:01     
if a specified condition exists it's one else it's zero so if the condition pass     
11:07     
it says true then it's one it's not zero so here is the matrix of IR and I comma     
11:14     
J entry of the matrix M sigma is the sigmoid function or nonlinear activation     
11:19     
sigmoid or softmax or ru so yeah so how do they design this uh uh I mean like     
11:27     
before designing the encoder design guidance we have two steps so as we have seen that step uh first would be how do     
11:34     
we design our input feature how would we design our input structure so here I have detailed uh today's topic of     
11:41     
discussion so maybe I'll uh show it on the slide and some from     
11:47     
the paper so design features to reflect uh you know region of interest that is     
11:52     
first the second is uh design uh how do we transform these features that could     
11:57     
become under like deductive transformations and uh non-ductive transformations so here are the external     
12:03     
features for the labels so uh I mean like what are the external features of the labels so here we extract features     
12:10     
that come from external sources such as uh timestamped events text fields which     
12:16     
are not available in the hypograph so that we are trying to embed into the hyperraph so those are called as     
12:22     
external features suppose we have text or images or some so and we want to     
12:28     
encode that information inside the node so that is called as external features or the label so how do we integrate that     
12:35     
so we concatenate these features external features or labels with the     
12:40     
hyperraphs node or hyper edge representations so one can incorporate inside a node or we can incorporate as a     
12:46     
hyper edge so that these features uh are not directly assigned but uh these are     
12:53     
assigned based on the you know the kind of uh relationship that they have     
12:58     
suppose we cannot directly put a given suppose we are trying to understand interaction between three images suppose     
13:05     
say uh we cannot embed the entire image information inside the node so no for     
13:13     
the node feature whereas we can have like a short representations of the image and we can encode the information     
13:20     
about that uh you know uh you mean the feature inside that uh node instead of     
13:25     
encoding the image we can encode the feature of that image so that's how this external feature of a label thing works     
13:32     
so what is the pro what is the advantage of doing uh this thing so it provides a complimentary context     
13:39     
by that is like that's not captured by graph because graph do not really have     
13:45     
the information of the image encoding characteristics so we directly encode     
13:50     
the information from the image saying that okay these set of feature of features are important for the     
13:57     
interaction so it interacts properly based on that information so that's one of the advantage so it enhances the     
14:04     
model ability to learn from the specific cues that we have already you know curated and designed uh in that space uh     
14:12     
so temporal features can help track of time based changes in the interaction so in the temp so when we are trying to     
14:17     
design a temporal so say that so we start from like embryogenesis of the first time stamp and second time stamp     
14:24     
and and it goes on so the interactions that we are trying to understand so we can encode it in a certain fashion that     
14:31     
we wanted to I mean like uh that we desire so that's that can be done doing     
14:37     
no that external feature or labels the disadvantages can be seen or the cons     
14:42     
can be seen like uh so there may be noisy sometimes incomplete or inconsistent if     
14:48     
the external data is not well maintained so if the you know uh if we haven't     
14:53     
bought the correct representations that associate with the two neighborhoods or     
14:58     
the two nodes or we are trying to you know uh model the interaction then it     
15:04     
generally fails so the pre-processing step uh requires like a little uh you     
15:10     
know we have to make ensure that we do a correct line of pre-processing instead of you know just focus on like you know     
15:16     
just dumping up the data or uh you know sometimes the external information might not directly indicate the high order     
15:23     
interaction suppose we are trying to find out relationship between you know three images where these three are you     
15:29     
know do not even have a proper high order interaction so then uh this becomes like a not so useful thing to do     
15:36     
so there's there may be a misalignment and we have to you know model it properly with careful you know when     
15:43     
we're trying to model the any certain like embryogenesis for like uh you know     
15:50     
uh clans without having a proper mathematical rigor of you know how do     
15:55     
how do these things work and we give in a certain fashion they might not work so     
16:00     
there can be a misalignment with the patterns so this can be seen so that's one so then we have the structural     
16:07     
features so yeah so that's the main uh idea of     
16:12     
like external features or labels there are the some of the papers that they have cited like bag of word vectors     
16:19     
where they have a large corpus of data instead of feeding that large corpus of data they have given this to like a bag     
16:25     
of words vectors and these these vectors are fed into that each and every node so     
16:31     
there are like several other papers which have used this type of work and they have mentioned that you know this     
16:36     
can be done in several uh I mean like there are several methods which have done and they have incorporated things     
16:43     
in such a way so and next we jump into the structural features     
16:49     
so ES structural features     
16:56     
yeah so in the structural features so we derive directly from the hyperraph     
17:03     
topology so if you have a correct topology so we we extract the structural     
17:08     
features from the topology itself for example when we are trying to compute uh you know uh when we are trying to     
17:16     
understand the structural features of a you know uh evolving uh graph or     
17:22     
evolving you know the C eleance suppose we we believe that you know the from the     
17:27     
parent and the child actually grows from like you know AB cell to AB and all so     
17:34     
this lineage graph when we are trying to you know design a structural feature then we can say that the the interaction     
17:40     
that happens from like you know uh the parent cell to the two child so we can     
17:47     
you know design this as like a structural feature saying that this is the parent two child interaction and we     
17:52     
can you know design an hyper edge on top of that so this hyper edge that we use on top of these three children I mean     
17:59     
like two children and one parent could be seen like a structural features because we ex we extracted the the same     
18:05     
from the topology and when we are trying to learn the characteristics I mean like suppose what is the similar I mean like     
18:12     
uh how are these related based on the distances so that we can encode based on the structural features so this is one     
18:19     
way of doing it so when we capture the inherent relationship patterns and like you know we can capture higher order     
18:25     
connectivity that defines in the you know the higher order interactions so     
18:31     
based on the given structure we can able to you know identify the things properly based on the given structure itself so     
18:38     
here we do not uh specifically need not encode certain additional details into     
18:43     
it but we can understand the things that you know which actually interacts with     
18:49     
what and all so who interacts with whom so this can be nicely seen in this graphs so what can be particularly     
18:55     
robust in settings where external features are absent or sparse so this can be very advantageous     
19:02     
in that scenario and where it can be disadvantages so the extraction and     
19:07     
computation of structural features can be little computationally expensive I mean like and uh there needs like a     
19:14     
little more data set because in order to train such no complexities and uh so may     
19:20     
require some parametric tuning in order to design these hyperraphs that could be a little problem that that's not a big     
19:26     
problem but you know while when we have like a big big big corpus we want to try to uh you know get some uh certaintity I     
19:35     
mean like structural features from like know small small motives then it becomes like a problem so the possibility of     
19:42     
information redundancy if too many similar structures are there then there is that so if the structure uh the     
19:49     
motives that we regularly see in that you know the hypographs that we design     
19:54     
that uh we the the problem comes is these can be repetitive in certain parts     
19:59     
and these repetitions can also get captured in this when we take the structural features so that can be the     
20:05     
like a second disadvantage so what people have come up with is uh there is another thing called     
20:12     
identity features in identity features what we have is we assign a unique     
20:17     
identifier to each node suppose uh the lineage graph is the best example for     
20:22     
this because it's a unique identifier because it starts from a parent     
20:28     
specified parent and then it slowly evolves into like a child and the child has like two uh I mean like the child     
20:35     
becomes a parent to other two children and that the graph grows accordingly so this is one of the good way to do it so     
20:43     
what they do is in like uh when these are you know specified accordingly then     
20:49     
uh the thing that can be seen is uh for the identity features this uh the higher     
20:56     
order I mean like the higher order interactions can be captured using a random vectors so when we realize uh     
21:05     
initialize using a random vector embeddings then we do not specifically know I mean like it's everything is like     
21:12     
uh unique right so we try to understand the higher order relationships between     
21:18     
uh by assigning some random feature I mean like uh random hyper nodes and then     
21:23     
we learn the hyper nodes oh sorry hyper edges so we design a hyper edge randomly     
21:28     
we initialize it and then learn based on the the interactions that we accordingly so this is particularly seen in many of     
21:34     
the cases so what is the advantage of this is guarantees that each entity has     
21:40     
its own unique representation and which can be beneficial to ask for like recommendation systems and all and also     
21:46     
for art as which I have mentioned and acts a fallback mechanism when external or structural signals are pretty much     
21:53     
weak so and the disadvantages that we can     
21:58     
seen is by themselves identity features do not capture relational or contextual nuances between the entities so they do     
22:06     
not I mean like we cannot say that okay these can be a hyper that we can form between these I mean like features no no     
22:14     
we can't do that so we assign randomly and we learn based on the interaction that happens and next uh this can this     
22:21     
can have like increase of overfitting if you have less uh amount of data so so     
22:27     
basically the whatever the random higher order connections that we interactions that we're trying to uh design they     
22:35     
might not uh exactly fall into the space they might not uh you know uh there are     
22:41     
some interaction that it fixes and it does not learn so it generally or either     
22:47     
it has like a certain pattern and it fix it like that and it memorizes but not you know generalizes so pure     
22:54     
generalization can be seen in certain cases where uh you know if we are using solely the identity     
23:01     
information so these are like the different feature extraction mechanisms that we have seen other than the di     
23:08     
connections between the simple uh GNN so now what they say is we have     
23:17     
uh how do we express uh hyperraph to uh reflect the higher order interactions so     
23:23     
how do we express them so how do we have to construct an hyperraph in order to learn better features so here there is a     
23:31     
good example of you know the hyperraph so this is one way of doing that is click expanded graph and another is star     
23:37     
expanded graph so they divided this type of uh expressions into like reductive     
23:44     
transformations and also non-redictive transformations so what in reductive transformations they do is so let's go a     
23:51     
little into reductive transformations yeah so in reductive     
23:59     
transformation first is click expansion so what click does is so so each it     
24:05     
tries to transform the hyperraph that is at each hyper edge into a complete     
24:12     
graph with a click okay i mean like the connections that that are being     
24:17     
established with the nodes with every pair of the nodes it tries to form a click based on that click     
24:24     
uh this forms a uh I mean like this type of graph suppose uh let us take example     
24:29     
here that could be very useful for us right now so here we can see a hyperraph     
24:35     
saying that 1 2 3 4 5 so these are the five nodes and A is one hyper H B is     
24:42     
hyper H C is a hyper H so A is having hyper H between 1 3 and four node 1 3     
24:49     
and four so hence we have an interaction between 1 3 and four so this is a click that we have formed so this is 1x3 this     
24:56     
is 2x3 and this is 1x 3 why this 3 and four has 2x3 instead of 1x3 is so here     
25:04     
this three uh 3x and 3 and four shares the hyper edge between both a and b so     
25:11     
so suppose it has a 1 3 and 4 are saying that 1 2 3 is 1x 3 to 4 is 1x 3 and 1 to     
25:20     
4 is 1x 3 so it gives 1 so 1x 3 + 1x 3 + 1x 3 1 so in the similar fashion for the     
25:27     
b we get that so but whereas this three and four shares the common two     
25:32     
interactions from uh both the hyper edges so that is a and b so it gives the     
25:38     
three so that this uh so let's go understand uh how is this click expansion done and     
25:45     
all so this computes edge wise weights between uh I mean like and factors such     
25:50     
as node pair co-occurrence frequencies so this how many times so as we have seen three and four occurs twice and so     
25:57     
we give the node pairs frequency as 2x3 and the normalization is done accordingly the weight attempts to     
26:03     
mitigate the distortion from the various complete clicks so the way it is attempts to you know give a proper     
26:10     
unification I mean like uniformed distribution of the weights it designs and uh which has like more number of uh     
26:17     
you know courrences it has more uh weight so what is the advantage of this     
26:22     
it allows direct use of established hyperraph for the spectral spectral filters and this is simple and well     
26:30     
understood approach but what is the main disadvantage is it loses some of the structural information about like exact     
26:37     
hyper edge membership so the higher order interactions they might be they might not exactly be the way the clique     
26:43     
has constructed because the hyper edge structure is replaced by pair wise connections again so and sometimes this     
26:50     
might lead to redundant connectivity so which so these are the main two disadvantages that we see and also the     
26:57     
expansions that we do right I mean like using these approaches they cannot be     
27:02     
again transformed back so I mean we cannot recover the hyperraph back once     
27:08     
we construct back into this sometimes so so that is the main disadvantage so     
27:13     
adaptive expansion so the change here is for the adaptive expansion as we have     
27:18     
seen in the clique expansion we have decided some set of you know weights uniformly divided instead of design     
27:26     
uh I mean like defining certain uniform weights we actually give selective weights or learnable weights saying per     
27:34     
say that you know there are certain interactions that are unnecessary but we have added a node so then we remove that     
27:41     
or there is a little more importance that we add to it so instead of forming a full click they selectively add or     
27:48     
weigh edges with a hyperraph based on fulldriven criteria like it's distance based node based feature vectors and all     
27:56     
and also one can identify two anchor nodes per hyper edge based on the     
28:01     
maximum feature difference and one connects those anchors to the other nodes generally this strategy helps to     
28:07     
you know change uh the features or the representations or the interactions     
28:13     
little bit and gives little priority to those have like a good data characteristics suppose per say we say     
28:19     
that uh we have the interaction between two networks saying one network has like very limited information or missing     
28:25     
information then we simply discard based on the adaptive expansion so this is one way of doing so what it does is it     
28:32     
mitigates the noise by avoiding unnecessary or redundant edges and tailor a proper you know weighted     
28:38     
specific more preserving higher order interaction nuances this can be preserved but disadvantage is uh it's     
28:44     
very complex to implement because we do not specifically know what exactly to give a proper weight on top of that and     
28:51     
these distance measures are uh not very useful in the real world     
29:00     
uh so this is very potentially sensitive to very parametric settings and you know noise and not really applicable on the     
29:08     
real world data so what now we are trying to understand is non-redictive transformation one is star expansion so     
29:15     
here we see that uh this is one of the non-redictive expansion we have seen the star expansion here instead of forming a     
29:23     
key what we do is we design A B C as two different entities saying that these are     
29:30     
three hyper nodes these hyper edges which are connected to one     
29:36     
uh and four and three so 1 3 and four A is connected so as the hyper edge is     
29:43     
there so here this has an interaction instead of forming uh you know so     
29:49     
instead of having a you know uh what do you say uh a click type of     
29:54     
representation so these representations can be uh simply a biparted graph where     
30:00     
uh uh a act like a edges or the hyper edges what the nodes the blue color ones     
30:06     
acts like a hyper nodes or simply nodes so this way we can actually encode the     
30:13     
information into the you know we can understand the you know the better higher order relationships can be     
30:18     
acquired using this bipartite approach oh sorry     
30:24     
uh so yeah sorry     
30:32     
sorry this fully preserves the original hypograph information and uh hyper can     
30:38     
be recovered easily i mean like exactly and it's a clear separation between nodes and hyper edges it facilitates     
30:44     
specific I mean like we can specify which kind of message passing that we can use so that it can give better     
30:50     
outcomes and uh the disadvantage of this is doubles the number of nodes and     
30:56     
increases the complexity of the graph so algorithms must be now handle the bipartite structure and the adapt the     
31:04     
adaptation mechanisms that we use for traditional traditional graphs may not be exactly useful here so we have to     
31:10     
design our own way of training you know these type of networks so the another     
31:16     
one is line expansion so this converts each pair of nodes into you know two     
31:22     
distinguished nodes into a new graph so this connects new nodes if they share     
31:27     
the same original node or the hyper edge or uh this distributes the information     
31:32     
into higher dimensional structure where the relationship among the pairs are explicitly encoded so the main advantage     
31:40     
of this line expansion is this maintains exact membership of the nodes to hyper edges this can capture more complex     
31:46     
interaction and serve like node to hyperate association similar to that of star expansion and this may result in uh     
31:54     
like very large amount of uh you know number of nodes and thus it increases     
32:00     
the comp computational cost and uh sometimes the grass can graph can be     
32:05     
very sparse the information that may not be useful and it's quite difficult to interpret from this line expansion so     
32:13     
finally this is like a tensor expansion so we represent uh a given hyper graph     
32:19     
into a higher order tensor we call it k uniform hyperraphs or k order tensor where each entry indicates the specific     
32:28     
set of nodes form a hyper edge or not so so we have like a so basically this     
32:35     
tensor expansion is basically imparted into like uh this uh pytorch geometric     
32:41     
and these other mechanisms where you know all the features are like saved into a k tensor uniform hypograph tensor     
32:48     
and these tensors multiply and give like a you know more distinguished higher     
32:55     
order interactions so there is no loss of information in this tensor expansion the original hyperraph structure is well     
33:03     
maintained and uh it also allows like multilinear algebraic uh techniques to     
33:10     
analyze higher order interactions so we use matrix multiplication we use uh hard     
33:16     
product to understand you know each two I mean like each hyper h2 multiple nodes     
33:22     
relationship so we can use uh you know a simple matrix multiplication across     
33:27     
tensors and we can see how are these uh you know the distributed higher order     
33:32     
interactions are spread and uh yeah so this is also computationally expensive     
33:39     
and impractical for scaling the graphs which is non-uniform and all     
33:45     
so so these are the two different     
33:50     
uh sections that I've covered for today so if we have any doubts then I'll be     
33:58     
more than open and helpful to discuss okay     
34:04     
well that's great uh thank you uh so any questions or comments     
34:17     
yes I've got one leis did I hear you say that you could     
34:24     
turn sensitial relationships in a lineage tree into a hyperraph     
34:30     
uh sorry I missed something i'm sorry one second okay if you take take uh take     
34:36     
the nematode lineage tree yes uh there's plenty of information that     
34:43     
there are sensitial uh cells yes in it can you make a hyperraph representing     
34:50     
these sensial relationships uh so what can we do is uh initially we     
34:59     
can start off with a random initiation of saying that you know these are sensation relationships per se and later     
35:08     
on we can use uh our methods we can learn slowly adapting that okay it may     
35:14     
not be this but these are no sensational relationships that actually are so we     
35:20     
can do that soon okay because we Bradley and I have been discussing     
35:27     
getting a sensitial graph of a neatode and uh perhaps a hyperraph way approach     
35:34     
would work yeah so I mean another question kind of     
35:40     
related to that are the sort of reticulated well I guess you have reticulating relationships in graphs     
35:49     
yeah nature but like you know sometimes you have birectionality     
35:54     
sometimes you have loops things like that right so learning those kind of     
35:59     
relationships alongside the actual topology in other words it's connected in a certain way but we don't really     
36:05     
know what I mean you kind of know for message passing what the flow might be like what goes from where other so it's     
36:14     
learning that pattern as well as the topology ology right okay yeah but I I'm proposing that     
36:23     
it would be very worthwhile to write a paper on hyperraphs as representations     
36:30     
of relationships uh in the lineage tree of sea     
36:36     
elegance very explicit uh and uh it would take mining     
36:42     
literature uh all I found so far is people talking about up to 44 different sensitial     
36:51     
tissues which is quite a bit uh uh and uh     
36:59     
Whoops okay uh so making such a uh modified uh lineage     
37:10     
tree uh would be quite worthwhile     
37:17     
i mean you know Bradley and I have been talking about making a simple one with     
37:23     
just one sensitial tissue but perhaps one could go for all 44 sensitial     
37:30     
tissues okay and do it right and do it     
37:35     
thoroughly and uh I think it could have a major     
37:40     
impact because the uh the lineage tree is not the correct     
37:47     
tree you know as as hallow as it may     
37:57     
be okay uh sir uh can you give me a little more context about uh you know     
38:03     
this cential uh you know interactions little bit I mean little     
38:09     
confused biologically you can explain then probably I can map into yeah I'm saying so far as the data allows     
38:16     
obviously if you can't be sure it becomes vague or you leave it out but     
38:23     
people are claiming 44 sensitial tissues perhaps at least Some of them     
38:28     
there's enough information so that we could make maybe a hyperraph of it right     
38:36     
okay and I think this could be very important in the development of the uh     
38:42     
segans embryo     
38:47     
also uh I wanted to show some kind of visualizations I tried on so may they     
38:54     
may not actually used be very much useful right now but uh maybe later on     
39:01     
the reason why I've tried this uh you know the visualization is I'm trying to     
39:07     
understand where exactly I wanted to have like a higher order interaction in     
39:12     
order to do so the visualization has been become a little impediment to start off     
39:18     
okay yeah I did a quick search uh a minute ago and um I found that there are     
39:23     
only 203 papers on sensitial relationships in the in the elegance     
39:33     
i mean that's yeah so amongst those might be some with data     
39:39     
that we can actually use so what I was trying to understand here is uh so I wanted to see like how in     
39:45     
threedimensional structure so how the you know parent so here we have uh first     
39:52     
time stamp what we have done is uh so I wanted to see like this is the parent it     
39:58     
has split into like aba and uh here the comes like these are from which the     
40:06     
children are rel I mean like you know defined like a nodes here like round and from which it has evolved the parent is     
40:14     
red color this uh what I call is you know the square one what I have done is     
40:19     
this is for first time point the next time point I've seen that okay so what are the new set of things that were     
40:25     
added so I've given green color to them so I'm trying to understand so here it     
40:30     
started with this a yeah so here it started with a b1 and     
40:37     
then it expanded and slowly it expanded handed so how are these relationships modeled so in the three-dimensional in a     
40:45     
proper threedimensional so I have checked whether these locations are correct or not and yes they are and uh     
40:52     
so I'm able to see in a three-dimensional space that how are these cells the parent has divided into     
40:59     
the the two children so and then for the next time step I I did for only for like you know     
41:08     
uh initial uh experiment sort of because I wanted to see like how they split actually so     
41:16     
most of the time what happens is they randomly you know evolve and they you     
41:21     
know when I do the entire for the all the time points it gets cluttered so I     
41:27     
have just plotted it for like three time points have seen like how they are moving now based on proximities based on     
41:33     
the distances so based on you know this so we can model the hyperraphs per say     
41:39     
suppose now this portion so this E3 BA1 this portion this     
41:46     
portion this all can be modeled using a hypograph I mean like hyperode suppose     
41:51     
and these all can be node we do not know the exact higher order relationships     
41:56     
that we can model but we can start off saying that okay these uh set of you     
42:02     
know things can be done so this is the the data that I have found from like uh     
42:08     
the temporal data that I have found uh from like uh DVO     
42:15     
graph so so this this can be done for all the time points but it gets     
42:20     
cluttered so I just visualized for only few time points and this is reproducible     
42:25     
and probably I can you know can share the code     
42:31     
yeah that's in a notebook right yeah yeah please share that     
42:37     
yes I'll Yes     
42:43     
so in the report I haven't mentioned the three-dimensional uh I mentioned only     
42:49     
the 2D but my idea was like in the similar type I mean like the expansions happen in the the you know the     
42:56     
embryogenesis happen with these expansion and these are the lineage tree that it grows in this fashion and now I     
43:03     
wanted to model the the relationships among this not just the diotic but the     
43:08     
know the using a hyper edge so how do we do that so that's why I was reading that     
43:13     
paper which could be a exact app app to structure that we have to choose is it a structural features or is it like you     
43:20     
know the the identity features or uh you     
43:26     
know like uh you know we externally provide some certain set of features or how do we you know design this     
43:33     
transformation do we have to do it in a star graph or do we have to do it in a hyper tensor way so I was starting off     
43:39     
that initially next we can go into the message passing mechanisms and see that you know what exactly can be put into     
43:46     
this space and we can go into much detail you know so I think this was like     
43:52     
uh uh a kind of and this is pro this is the main     
43:57     
advantage of this is we can see it we can modify it and we can zoom it and we     
44:03     
can uh you know play around with this visualizations where exactly it's been modeled and we get all the coordinates     
44:10     
and uh is it a daughter node or a mother node a parent node we get the information about that     
44:17     
yeah so yeah so I mean the interesting So could     
44:24     
you bring that back up yes so     
44:32     
it is okay uh so you see these on the left here this this uh threedimensional     
44:39     
space with all the cells so the cells that you're marking are the nuclei of     
44:44     
each cell course you have right multiple generations so that's the expansion part     
44:50     
and then so cential cell would be like if you have an expansion     
44:56     
and they kind of spread out you know they they're division events and spatially they spread out a cential     
45:05     
uh expansion might be like where they divide to a point where the     
45:12     
uh nuclei are in different spaces but they're all sort of in cell so they they reticulate in the sense that they're not     
45:20     
different cells but they still you know there's still these independent nuclei     
45:25     
which we're tracking because if you look in the lineage tree you'll see that like     
45:30     
the way they represent it is it's a it's a bifurcating tree goes down many generations and then they treat every     
45:39     
uh centum as like a series of different cells that are terminal differentiated     
45:46     
meaning that they divide they have this identity as an a mature cell and then     
45:52     
that's the end of it they don't talk about it I mean they don't show like how     
45:58     
you know they don't the the lineage tree representation usually is don't describe or don't make the distinction between     
46:05     
like specific nuclei like we have here and cell bodies so each of these nuclei     
46:11     
are presumed to be an independent cell body albeit like you know different     
46:18     
generations so you have one cell body that two cell bodies and two nuclei and     
46:24     
those kind of like have their place but then the cential case you have all these nuclei that are maybe distributed in     
46:32     
space but are now part of the same cell body that is like one single thing and     
46:39     
so that's you'll see that um you don't see that in the lineage tree and it's     
46:44     
you know it's kind of interesting because if you take I was actually playing around with this from the data     
46:50     
for ferinx and you can collect like all the cells that they mentioned for ferinx and     
46:57     
kind of put them in a representation it's kind of hard to represent it i mean you have the they're     
47:03     
not contiguous in the lineage tree so they're different parts of the lineage tree but ideal you know ostensibly     
47:10     
they'll be in a similar or they'll be in sort of a uh might be like an arc along in in a     
47:19     
spatial location like this set of spatial locations that might form an arc or some continuous feature so you know     
47:26     
you might think well if I look at this lineage tree down here on the left you     
47:32     
have the red cells that are discontinuous uh the green cells are     
47:37     
discontinuous you might actually have like in the case of a centum all the     
47:42     
green cells forming like a ridge in this space right right you know along an arc     
47:48     
or they could be discontinuous but the cell body kind of snakes around and     
47:53     
incorporates them right and then you know that would kind of be a hyper no a     
47:58     
hyper note right right right     
48:04     
yes yeah so I mean that might be like a unique opportunity to like define a     
48:10     
hyperode like if we know right that's kind of like you know the preprint that uh I put out yes you know that was what     
48:18     
was being described is that you have these labels but the labels are not just     
48:23     
like you know like linguistic they're actually phenomenological you have these     
48:30     
like it's you know all the nuclei within a cell body all the     
48:35     
nuclei of a certain cell type like the neuron we have those categories that are     
48:41     
naturally in the in the biology so we you know we don't have to guess as to     
48:46     
what the hyper nodes contain they contain things but then how do you get     
48:52     
to those things because you have this process of cell division and differentiation to get there     
49:00     
so I mean in sea elegance it's pretty it's pretty deterministic but in like other     
49:06     
organisms where you have regulative development it's not it's very different     
49:11     
and it's going to have different constituents in those um in those systems     
49:18     
right so I mean like uh no technically or no     
49:24     
so first we can you know say that these two belongs to a same hyper node and then green all belongs to a same hyper     
49:31     
node saying that you know this one and two are the based on the connections we can say that they have like an overlap     
49:38     
and uh and the blue ones that we have here so they do belong to an next three     
49:43     
levels of hyper nodes so they evolve over the time like these higher order interactions this just an assumption but     
49:50     
so this can be modeled in that way saying that you know these three cells were actually have come up from the same     
49:57     
parent so they do share the similar uh you know the cell body although the     
50:04     
nuclei are like separated but they have shared the cell body they might share     
50:09     
the certain set of biological features underlying and so the evolution happens     
50:15     
and know have these set of connections so we can I think modeling those could be little interesting     
50:24     
line yeah yeah that sounds good     
50:32     
so that looks good also those kind of visualizations we're doing things like that uh I think they're featured in one     
50:39     
of the papers i'll send you the paper that has they're not modeled in well they're kind of the same kind of     
50:46     
considering the graph from a 3D perspective like that okay so we have     
50:51     
but like this i mean it's basically the same intuition and we didn't you know we     
50:57     
were actually doing things like calculating angles in the spaces so it's interesting but yeah I mean it's it uh     
51:04     
we're talking about like so you have other types of features in those graphs     
51:09     
like if you have a parent um cell like two daughter     
51:15     
cells the cell division event that that represents is where the nuclei one     
51:22     
nucleus becomes two there's the cell body that divides and cells migrate then those cells have     
51:30     
differenti from the from the parent but then you can     
51:36     
calculate sort of an angle phase angle between the two daughters oh oh the idea     
51:43     
would be like you take that phase angle and you'd say what is the similarity of     
51:49     
the phase angle maybe between uh you know within a a     
51:55     
sublineage or like uh between the anterior and the posterior it's just     
52:01     
another parameter you can use to look at cell division and what it's doing you     
52:06     
know what what's going on you might have cells that migrate far away you might have cells that don't migrate at all you     
52:14     
can describe that process i mean you can describe the distance between them but you can also describe that phasing     
52:21     
right based on the threedimensional     
52:27     
coordinates we can say that you know from here it started here so so based on that we can determine the phase angle     
52:34     
so we can more than that you can get the flow of cells over time     
52:40     
sorry you can get the flow of cells over time     
52:45     
where does the cell move right you can draw an arrow right that gives you a     
52:52     
kind of flow right okay uh Bradley do we have any flow     
52:59     
diagrams for C elegance um I don't think I don't know i think we had worked out     
53:07     
some diagrams i don't know if they're flow diagrams per se i'll have to pull them up because I     
53:13     
have some examples but I don't Okay     
53:20     
yeah i mean because in a sense as the embryo grows there's a flow of cells and     
53:25     
how do we visualize that flow right uh you could uh for example     
53:33     
uh every time you get a cell uh it divides and then start the then there's     
53:39     
a later division you could use the centrid of the     
53:46     
uh of the second generation of cells okay to make an arrow yeah     
53:55     
now uh of course that won't that won't show you the divergence if they if they end up far     
54:05     
apart that's why I say it's a problem is how to represent     
54:10     
this but I think it's very interesting     
54:15     
uh these these hyperraphs and perhaps flow graphs etc might give us new ways     
54:22     
of looking at the the development     
54:28     
So you were suggesting uh there are like 200 papers that were uh present for like     
54:33     
you know this sensial nature of C elegance you're trying to model so can you send one of one of them interesting     
54:41     
no I can't send one i can't I'm having trouble downloading them for some stupid reason i'll work on later you can send     
54:48     
it the dial object identifier or I'll send you I'll send you all 200 so you     
54:53     
can skim over them okay but I I was trying to download     
55:00     
them it's not working for some reason     
55:05     
i think they're having trouble at the company that has it     
55:12     
okay so the literature is there uh whether how much of it is more than     
55:18     
qualitative I don't know     
55:23     
okay but I think it's time to move on and get away from uh uh the what was it     
55:32     
sultan Who did the original uh Oh Solstston yeah Soulston solst he did the     
55:39     
original uh tree but what the That was in the 1970s     
55:44     
he did it by hand and I think it's time to go beyond that considering the claims     
55:49     
of all these sensitial cells okay time time to understand what's     
55:56     
actually happening not what you have no Nobel prize for     
56:06     
yeah that's good uh so hum did you have anything to add i mean you turned your     
56:15     
camera i don't have anything     
56:21     
okay     
56:27     
sir uh also I wanted to mention that uh I haven't spent uh you know much time     
56:34     
reviewing the you know the report it could be good if I had little more time     
56:40     
given to you so that was really last minute i should have mailed you at least     
56:46     
a week earlier so that you could have reviewed and give me a good suggestion or feedback based on that so did you     
56:52     
look into Yeah yeah i'll I'll take a look at it or     
56:58     
I'll be evaluating proposals so     
57:04     
yeah yeah give me some time to get Yes yeah     
57:11     
all right all right so I guess that's it for this week yes sir thank you Leith for great     
57:19     
presentation and discussion right all     
57:25     
right see you next week thank you thank you bye all right so some follow up from     
57:31     
today's meeting i wanted to highlight two papers that maybe put some of the     
57:37     
stuff we did today in context so the first paper is this paper dynamic morphoskeletons in development this is a     
57:44     
paper I put into a lecture morphogenesis which was on the YouTube channel uh this is by a number     
57:52     
of authors from Harvard U Cavali Institute so this is     
57:59     
where they're using flows particularly the bron and coherent structures which     
58:05     
are mathematical tools to describe flows to think about morphagenesis in     
58:11     
development biology so why don't we look at the abstract u morphagenetic flows in     
58:18     
developmental biology are characterized by the coordinated motion of thousands of cells that organize into tissues so     
58:25     
in a lot of organisms you have these flows of cells as they're dividing and     
58:31     
they move through cell migration to different locations embryo and those     
58:38     
flows we can characterize them sometime you know these are systems with many     
58:43     
many more cells we see and see elements and we can characterize those cells as     
58:48     
sort of having a certain pattern migration so we're interested in is     
58:54     
looking at movement as a flow and then describing them uh in in terms of     
59:02     
hydrodnamics and so you know this is something that a tool that people use um     
59:09     
they use a lronian we'll get into the mathematics     
59:17     
um so naturally raising the question of how this collective organization rises     
59:24     
using only the kinematics of tissue deformation which naturally integrates local and global mechanisms along cell     
59:31     
paths so they basically use a differencing mechanism to look at the     
59:37     
divergence of cells and their relative position so they're using the kinematics     
59:43     
in this hydrodnamic flow we identify the dynamic morphoskeletons behind     
59:50     
morphogenesis so they're the way they define dynamic morphoskeletons is the evolving     
59:56     
centerpieces of multisellular trajectory so you have these trajectories of these     
1:00:03     
cells they're basically going to be similar because they involve in these flows and you can characterize sort of     
1:00:11     
the most common trajectories from this analysis these features are modeled in     
1:00:17     
parameter 3 frame and variant and robust measurement errors it can be computed     
1:00:23     
from unfiltered cell velocity so the data the kind of data we need for     
1:00:29     
this is to have sort of an origin point and a set of points in time that change     
1:00:37     
so a cell sort of an initial location and then you know for each cell that     
1:00:44     
we're analyzing and then those cells will move over time and you take this     
1:00:49     
trajectory you can have the continuous case you can have the discrete case and you can calculate that trajectory from     
1:00:56     
time z to time n and so then we also want to look at the difference between     
1:01:02     
say two or more cells and look at how those trajectories kind of converge     
1:01:08     
so this is a different type of tool than say like cell it's actually maybe a     
1:01:14     
level above cell whereas in cell tracking we get specific information for     
1:01:20     
each cell the location of its nucleus in this case we have that     
1:01:28     
tracking it over time but we're looking at sort of the derivative of motion and     
1:01:34     
we're characterizing that in terms of a trajectory and cells     
1:01:40     
uh in terms of different cells and how their their position relates to one     
1:01:46     
another we reveal the spatial attractors and repellers of the embryo by quantifying it     
1:01:53     
information that to simple trajectory inspection so this is where again we're     
1:01:59     
using this derivative of just simply projecting out the positions and calculating body line for a function we     
1:02:06     
could also use orarian methods that are local and typically dependent soan methods don't give us the kind of     
1:02:13     
temporal resolution across the embryo computing these dynamic     
1:02:19     
oroskeletons in wild type mutant chick fly embryos we find that they capture     
1:02:24     
only footprint of normor for genetic features reveal new ones quantitatively     
1:02:30     
distinguish between figure one shows the dynamic     
1:02:35     
morphoskeleton itself so you have salad in membrane and it move in many     
1:02:41     
different directions from its origin so it has an origin and it moves either outward a     
1:02:50     
number of directions or it remains stable so it remains stable we don't     
1:02:55     
have to worry if these particles move in a certain direction and want to track     
1:03:00     
that position changes in its position time so in a it kind of shows the     
1:03:07     
context of this in an embryo and b it shows the cell velocity field which is     
1:03:13     
the area around the cell that conditions how far and fast particle characterized     
1:03:19     
by its on the left we have the oilian approach on the right we     
1:03:25     
have so in oilarian approach we have the cell velocity field and we have this     
1:03:31     
clump of cells in the middle and we can describe them and We have this sort of     
1:03:37     
point model which is interesting but the langian approach allows us to     
1:03:43     
characterize some position relative to the flow field so you have more     
1:03:48     
information as it evolves so on the right we have a lrangian coherent     
1:03:54     
structures so there are a number of methods that we can use for a lronian coherent structure with memory so has a     
1:04:03     
memory as you can see on the left uh the first measure is the FTLE measure or the     
1:04:09     
one that they're going to focus on this paper and this is the finite time     
1:04:14     
exponent so music can use a leopnov exponent to characterize the time     
1:04:20     
evolution and this is a finite equation for finite time so that means that you go from t0 to     
1:04:27     
tn and you measure that many points and you characterize the trajectory over     
1:04:32     
those points so this reason they're using the fleece in this paper is because it's computationally simple you     
1:04:40     
have uh your integral you're integrating from t0 to t n and within that interval     
1:04:47     
you're calculating the velocity and then changes in that velocity so that's     
1:04:53     
basically what you're getting so it allows you to give it gives you this leverian flow map which is given by this     
1:05:00     
equation one which maps the initial positions of cells and you could have talking so     
1:05:07     
nuclei here we could also map the change in position of cell membranes or of cell     
1:05:14     
bodies if we can characterize a cell body and do all of those things depends on our input so this shows sort of a     
1:05:22     
conceptual view of the FTLE so you can see in figure 2A you     
1:05:29     
have this origin point which is X0 and you can see that that X0 can evolve to a     
1:05:37     
number of end states of XT which have a range of values so typically you have maybe the     
1:05:45     
information for the initial point and then the next point because it's a     
1:05:50     
continuous measure so we're characterizing it as a continuous measure but we have discrete data we     
1:05:56     
have to sort of interpolate a lot of these points so we go from our origin     
1:06:02     
point x0 to different points of xt and those different points of xt form     
1:06:08     
this ridge so we end up with this ridge of values um that are described we can describe     
1:06:15     
that mathematically we can also describe a bridge of values going from origin     
1:06:22     
points to another set of origin points so we can go forward in time     
1:06:29     
backward in time we can go from origin to destination or destination back to     
1:06:36     
origin and then in uh figure 2B we actually can look at these bridges in     
1:06:44     
terms of the outcomes of these origin points so you go from many different     
1:06:49     
origin points in this case evolving out to this ridge which are the sort of it's     
1:06:55     
like a front in space so you have this ridge that's described and you get a     
1:07:02     
sense of these bridges over time so you have like the origin point you have many     
1:07:07     
different uh points in time you're calculating these ridges for each cell     
1:07:14     
as it evolves or family of cells and you end up with this ridge in space that     
1:07:20     
describe sort of the the end point or the the consequence of these     
1:07:25     
trajectories so you can calculate the trajectory and you can calculate this bridge then the idea would be in m cell     
1:07:33     
migration you would have an origin point in a series of ridges that would describe sort of     
1:07:39     
wavefronts of migration so the migration would follow this wavefront outward the     
1:07:45     
second paper is from real society interface and this is called tissues and     
1:07:50     
networks of cells or it's generative rules of complex organ so the abstract     
1:07:56     
reads network analysis is a well-known and powerful tool in molecular biology so they're using network analysis to     
1:08:02     
look at tissues more recently it has been introduced into biology tissues can     
1:08:08     
readily be translated into spatial networks such as cells are represented by nodes intracellular connections by     
1:08:16     
edges this discretization of cellular organization enables mathematical approaches rooted in network science to     
1:08:23     
be applied towards the understanding of tissue structure and function here we     
1:08:28     
describe how such tissue abstractions can enable the principles that underpin tissue formation and function we have     
1:08:35     
covered we provide an introduction into biologically relevant network measures     
1:08:40     
and present an overview of different areas of developmental biology where these approaches have been applied we     
1:08:47     
then summarize the general development rules underpinning tissue topology generation finally we discuss how     
1:08:53     
generative models can help link the developmental rule back to the tissue topologies     
1:09:00     
so they're basically what they want to do is derive these local developmental rules and how they can give rise to     
1:09:06     
observed topological properties in multi cellular systems so they don't get into uh     
1:09:14     
hyperet networks here hyperraphs but they do give um a good     
1:09:20     
overview of this field and especially in terms of how that relates to general     
