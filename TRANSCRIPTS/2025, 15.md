## TRANSCRIPT
0:02     
good morning hi how are you good morning     
0:09     
okay i don't know if will anyone else be here today but we'll see yeah any     
0:16     
updates on your end or uh yes sorry I've been uh trying to     
0:21     
understand uh uh like uh we have discussed in the last time that uh you     
0:28     
know how do we know save the representations I mean like uh the input features and uh you know the input     
0:35     
structure so next I've been trying to understand uh you know the message know     
0:41     
I've read the message passing mechanisms and uh know the way that we wanted to     
0:46     
train the neural network so that part the hypographs and uh I was thinking in     
0:51     
the direction that you know to understand you know holistically from     
0:56     
starting to designing input feature then structure then message passing and to     
1:02     
fix the pipeline so that you know we can start thinking about it you know how     
1:07     
which component should be you know at each stage so that it will become like a     
1:13     
you know good way of learning the you know embryogenesis of C elegance how can we order the higher order     
1:19     
interactions how can we I mean like how how can we you know model the higher order interactions what could be the     
1:25     
best possible way and uh yes uh I would like to discuss about the paper little     
1:33     
in detail good morning hi are you um managing to write up my rewrite my     
1:41     
thesis and I'm trying for about seven pages oh okay for a paper yeah yeah I     
1:49     
think I can do it oh good yeah anyway that's all I'm doing oh wow yeah     
1:56     
good luck yeah I can start it right now sir so first uh I will start uh from the     
2:03     
paper itself because this is what I have for our purpose like I mentioned pros     
2:08     
and cons but before that I wanted to know go deep into     
2:15     
the the paper itself     
2:21     
okay so to give a quick recap so last week we discussed about the encoding uh     
2:28     
I mean like the input features and the input structure so here we understood     
2:33     
what kind of uh you know input feature that we can give it to a you know hyperraphs one is like external     
2:40     
information like uh providing the information that it does not uh have and     
2:46     
the structural information which directly accounts the information that was that is from the graph itself and uh     
2:53     
you know design it encodes certain uh you know extracts the properties of the     
2:58     
structural features and it encodes into the input feature itself and identity information so treating each and every     
3:05     
representation as a unique value and assigning uh you know hyper nodes in such a way that assigning hyper in such     
3:13     
a way that the set of hyperodes come together to form a higher order interaction that's one way uh these are     
3:19     
the three different ways of doing input features and input structure we have seen reductive transformation the     
3:25     
transformations which we do and after doing so we cannot you know bring the graph back so that is like click and     
3:31     
adaptive expansions and non- deductive is something that you know we have a hyperraph structure we can transform     
3:38     
into certain uh you know uh you know like uh transform into certain way and     
3:44     
we can bring it back so like star line and tensor expansions so this we discussed last week and this week we're     
3:51     
going to understand about the message passing mechanisms and the objective     
3:56     
uh that is related to that and you know we can go jump into the message passing     
4:02     
so here last time sorry sir uh if if you have any questions you can just     
4:07     
interrupt me in meanwhile itself because Yeah yeah yeah     
4:14     
okay so anyone can interrupt in the meanwhile if they have any questions so here is the graph so     
4:22     
suppose G is the graph and we are the the hyperodes and E is the hyper edge so     
4:29     
the H is the incidence matrix and the X are the node features and Y are the     
4:35     
hyper edge features the edge that carries so P is the embedding nodes of     
4:41     
the L layer so when we design a graph neural network so we believe that it has     
4:46     
like L number of layers so we call it L layer uh which is denoted like this     
4:54     
and uh the hyper edges are designed by like Q and uh so here we can see that the     
5:03     
know the shape of the P health layer is like uh you know the cardinality of V     
5:10     
times the K so here K is something we uh design because each layer has different     
5:17     
dimension so that's K is the I mean like the how many number of dimensions it has     
5:23     
the incident hyper edges is given by like N subscript the hyper node     
5:30     
and I N is N byN identity matrix so I is indicator function sigma is like     
5:36     
nonlinear activation function and M is basically a matrix i determines the row     
5:43     
and J determines the column or I comma J determines a single entry in a matrix     
5:49     
okay so let's go into like the part that we     
5:54     
were discussing like left off last week so yeah so message passing so how do we     
6:03     
pass the messages in order to have like uh that makes the higher order     
6:08     
interaction more effective so they use this this this some people call it as     
6:14     
message passing mechanism some people call it as aggregation function some people call it as neural message message     
6:21     
passing functions so it's like no uh you know it's just a jargon that varies here     
6:27     
and around but uh the concept is just the same how are we trying to you know encode these representations and how are     
6:34     
we identifying which node carries the correct information so that we make a     
6:40     
right prediction so while doing this message passing I mean like message passing     
6:46     
aggregations schemes in the higher order interactions uh we have the three     
6:52     
questions they they compute these three questions the first is whose messages should be     
6:57     
aggregated so who's in the sense which set of nodes or which set of hyper edges     
7:03     
so what messages should be aggregated and how should be they aggregated so these are the three different line of     
7:10     
questions that we going to see so so first we'll start with that whose     
7:15     
message should be aggregated that is target selection so target in the sense which set of nodes so that you know we     
7:22     
aggregate this this set of messages so for the message passing so we generally     
7:29     
should decide like which message to aggregate based on the structural expression of the hyperraph that we have     
7:35     
and provided the representation examples suppose we take an example of like one click expansion so where we do a click     
7:44     
expansion this this click expansion you know uh you know again forms a uh you     
7:51     
know uh interaction that is similar to that of direct nodes that we generally see so it's very similar to that of     
7:58     
simple graph neural networks and uh the one class one click expansion based on     
8:03     
hyperraph neural networks was was seen by uh few set of so we often see there     
8:10     
are like certain interaction techniques that we see so similar to that of hyperraph spectral expansion here we     
8:15     
have a new type of hyperraph spectra uh spectral expansion where they call it as simplifying hyperraph neural networks     
8:22     
this is one of the good paper that was uh published in uh uh ICML like I     
8:28     
believe it's in recent 2024 paper where they have simplified how can we use a oneclick expansion to that of hyperraphs     
8:36     
so that we can use it similar to that of when we once we do this uh one click     
8:41     
expansion it becomes like a diic uh single uh you know uh single order     
8:47     
interactions I mean like there is no higher order interaction there's a datic interaction so on top of this direct     
8:52     
interaction how can we induce this hyperraph properties using that is what they have tried off and uh I would uh     
8:59     
you know make some like points like what like uh what are the pros and cons of     
9:05     
these approach so so in target selection this     
9:11     
uh the in the approach that the information pass between nodes are connected by hyper edge and focus on the     
9:18     
interaction between the pairs of nodes so the main disadvantage that we see is again we get a diotic interaction and     
9:25     
the higher order interactions or the indirect interactions so are not     
9:30     
captured and uh these may have become little computationally expensive because     
9:36     
it does not consider full structure of the hyper edges so so that's the main uh two differences     
9:44     
and uh no one click expansion and all next we see one star expanded grass so     
9:51     
where we try to you know expand from the given you know you have this hyperraph     
9:56     
based star expansion message this message can occur from a set of node groups to that of like hyper edge group     
10:03     
and we can do the same like hyper edge group to the node group so and uh for     
10:08     
that what they do is so at the first layer so we know that p stands for like     
10:14     
hyper nodes and q stands for hyper edges so the P the L layer of each node     
10:19     
carries information of the previous layer and aggregates using a nonlinear     
10:25     
neural network here we can say it as an MLP or we can take another uh you know     
10:30     
uh small transformer block but you know it's the way of representing so they use     
10:35     
a nonlinear transformation to trans transform this set of features into certain space and there what they     
10:42     
perform is they try to find out the you know uh you know the message passing     
10:47     
between these two set of layers and later this they encode this information     
10:53     
into an another MLP so this is more like you know designing uh you know     
10:58     
uh uh three different set of uh interactions uh and bringing all them     
11:05     
together like you know we use different different MLPS so that uh you know to     
11:10     
form this higher order interactions believing that you know these edges randomly captures these set of nodes the     
11:17     
features that they have observed and then and uh yeah so subsequent ly in     
11:25     
equation five that we see that this transformation and in equation six we     
11:30     
see that the node embeddings are up updated by aggregating transformed embeddings of the incident hyper edge so     
11:38     
here this is our incident hyper edge and this is our previous note hyper uh edge     
11:45     
features and based on that they aggregate into this and this has this     
11:51     
uses another nonlinear transformation in order to aggregate the full-fledged you know the hyper node level features so so     
12:01     
I think this was morely extracted uh from the paper like the simplifying     
12:06     
graph neural networks so this is a paper which goes more into details of how this     
12:12     
was performed how they use this MLP block in order to acquire the higher     
12:18     
order interactions they have detailed very thing uh I mean like very clearly in this and they have like lots of     
12:24     
experiments which are not uh they do not belongs to the h I mean like uh clans     
12:30     
but they really are a little bit of close relation so that you know we can     
12:35     
think of like you know how this has been done so this is one line of work they have so they use standard set of data     
12:42     
sets and they have shown that this method is kind of effective so so that's one way so in     
12:49     
Uh so this is one way of representing the sorry so you have any questions uh not     
12:57     
right now no okay so yeah so this is the target selection so I mean like this     
13:03     
which set of nodes that we want to aggregate uh you know the messages so is     
13:08     
the first next so what kind of messages that we passed or in order to aggregate     
13:13     
as to given so the message itself we call it as a a representation or a message representation so the I mean     
13:21     
like after choosing the targets once we choose okay these are set of the hyper nodes or hyper edges that we wanted to     
13:28     
you know focus on uh in our case we can see it as like you know the hyper hedge     
13:33     
forming the cential tissues uh believing that okay these all form uh these are     
13:39     
the all the networks or these are cells that we form the cential tissues so believing that we can say that okay     
13:46     
these are our target uh selected target for you know we have five selected targets five sential     
13:54     
tissues and say that you know what kind of message that is to be given to them in order to you know make our process     
14:01     
complete so so what are the messages that we agreate is after choosing the     
14:06     
message target the next step is we determine the the representation so what     
14:11     
kind of representations are like in hypograph network we use embeddings of the previous layers as a message so so     
14:20     
generally at each and every layer so once the interaction happens and then it     
14:25     
pushes to another layer and the the layer one and the layer two so suppose     
14:30     
we have like three layers so each extracts a certain set of nuances so     
14:35     
that it the you know the graph or the hypograph will will be able to get the     
14:43     
all the necessary information at the right particular node so at each layer we have you know extracted decent level     
14:50     
of representations so when we say like we have five cent cential tissues and we     
14:57     
have a hypograph of like five uh five hyper edges and some uh no uh set of     
15:04     
nodes and they have their own connections using this hyper edge so when we say that uh I mean like uh the     
15:11     
message representation then here what we mean is so uh I mean like when typically     
15:18     
the previous once we give this to a hyper hyperraph neural network then it tries to understand that maybe the first     
15:25     
three are the I mean like you know it will try to understand the connection between the cential tissues uh of three     
15:32     
cential tissues in the first layer maybe five in the next layer and it tries to aggregate the information in the I mean     
15:39     
like third layer it's not exactly how it's done but in a similar fashion each and every layer has its own lay     
15:45     
representation but it tries to understand uh like uh you know tries to     
15:50     
acquire the features that are relevant to that of the the hyper graph that we     
15:55     
build so so this is like one of the important step that we use so we have uh     
16:03     
no we have hyper edge consistent messages so this is like a standard approach like most of     
16:10     
the significant papers use that so they call it as a unified you know uh     
16:16     
representations because each and you know when there is a hyper edge connection between all the set of nodes     
16:23     
or the hyper nodes then we believe that it has to share the information equally distributing to all of them and that's     
16:30     
the main concept of this hyper edge consistent messages so how it's done is     
16:35     
simply it aggregates the information from each of the layer and then it uses a small epsilon     
16:42     
uh as a simple learnable parameter and it tweaks upon that by using a simple uh     
16:48     
nonlinear function so theta is basically a trainable parameter and this is a     
16:54     
aggregation of like the hyper edges and the hyper nodes at uh the previous layer     
16:59     
that's encoded and uh yeah that is done with a uniform way so all the hyper     
17:06     
edges gets the same information uh so all the hyper nodes get the same information from the hyper edges so and     
17:13     
hyper edge dependent messages so each and edge carries a different layer of     
17:18     
information and this is passed using uh uh I mean like it's not exactly given to     
17:25     
each and every node in a fixed or uniform fashion so they use an MLP in     
17:31     
exactly we see here they use an MLP so the main use of MLP here is they transform uh you know uh suppose we have     
17:39     
10 number of you know hyper edges one hyper edge and like 10 number of hyper     
17:45     
nodes and it's uniformly it's uniformly distributed in the previous case but in this case it distributes based upon     
17:52     
certain waiting mechanism while training the hyperraph neural network we will be     
17:57     
able to you know tune these weights and get the weights appropriate to the problem that we are trying to solve so     
18:04     
this is uh one way of trying to do it so this is hyper edge dependent messages     
18:09     
and there are certain works where they have used that you know this what's next is one of the paper which introduces     
18:16     
within order positional encoding to adapt to the certain node messages for     
18:21     
each target so suppose we say that you know there are only certain set of cential tissues important inside a hyper     
18:28     
edge then we give little importance to to them instead of giving a uniform way     
18:34     
of you know giving importance so that you know it makes a stronger connection while doing so so this is one of the way     
18:40     
to do it and uh here uh the f represents     
18:47     
uh a node centrality matrix and t and ft are represents the you know the number     
18:52     
of centrality measures that we use and t centrality measure score of like node vi     
18:58     
the order of the element c is set of uh capital c and     
19:04     
uh like they are trying to explain this exact paper what they have done in the what's next so and it's the order of c     
19:11     
and there That's how they are able to calculate this VO within order positional encoding and using that     
19:17     
they'll be able to map it to the you know each and every node and hyper edge hyper node so that you know the hyper     
19:24     
edge has its you know the waiting mechanism and it becomes like a uh hyper     
19:29     
edge dependent messages so next uh once we have got uh given the features     
19:36     
exactly to the hyper nodes and the hyper edges and we distributed it in a certain     
19:41     
fashion now how do we aggregate these i mean like you know when we are doing try     
19:47     
to you do a task called like uh you know uh based on the given network uh can you     
19:54     
determine whether this uh you know this whole uh hyperraph system of C elegance     
20:00     
is able to will be able to survive or not suppose we take that case then we     
20:05     
use aggregation functions so that the information is totally aggregated and then we get to a desire desired label     
20:13     
and we classify or we do some sort of things or either we can do like you know     
20:19     
uh how much weightage that each and every uh I mean like uh the message or     
20:25     
the node the hybrid should carry depends on the aggregation function that we use     
20:30     
so there are like several functions that previously people have done in the     
20:36     
standard uh graph neural network we see summation we see multiplication we see     
20:42     
uh uh pooling we see uh I mean like uh uh simple MLP also an aggregator or we     
20:49     
use like transformer layers as an aggregator there are like several other mechanisms that we use in graph network     
20:55     
what we try to do in this hyperraphs is we use a fixed pooling mechanism so     
21:01     
fixed pooling is basically uh this includes the summation the average and     
21:07     
these are the certain which I have mentioned previously they use these are like fixed propagations instead of     
21:13     
adapting to certain hyper edges we used a fixed I mean like believing that you know this node should be aggregated     
21:20     
using the sum function so the hyper edge whatever the nodes carry the information all the hyper edge you do a simple     
21:27     
summation or simple multiplication so that's one way of doing and     
21:32     
uh so in learnable pooling what we use is so there are like several other     
21:38     
mechanisms uh several hyperraphs they use pooling functions and attention mechanisms and do a waiting     
21:47     
strategy for the aggregation so so these are more of like target agnostic     
21:53     
attention so what we mean by target agnostic is so as we design certain set     
21:58     
of targets here in this space right believing that you know this has to only we are focusing on these specific set of     
22:05     
hyper edges or the these specific set of hyper nodes for that what we try to do is we apply this attention so that only     
22:14     
the targets gets more uh you know the significant weighted proportion and this     
22:20     
target a uh aware attention gives more weight or more uh you know uh what do     
22:28     
you call uh you know significance to that of the you know the hyper edges or     
22:33     
the hyper nodes that we wanted to do so so all say transformer and uh so this is multi head     
22:42     
detection that they have used for the in that case so here here suppose the S is     
22:49     
basically the you know the embed embedding of the incident hyper edge for     
22:54     
that embedding what they're trying to do is first apply an MLP and then use a you     
23:00     
know a waiting function which can be you know probably a learnable parameter and     
23:05     
we apply on top of that an MLP with that and we try to understand the attention so here we see this more looks like and     
23:13     
uh you know uh this uh MLP of S will give give us uh you know uh n     
23:20     
dimensional feature and MLP of S gives the same n dimensional features so the     
23:26     
difference here is here we use for like t comma 1 which means that you know for     
23:31     
the previous time step and here t comma 2 is basically the next time step so that's how we try to embed the     
23:37     
information and when we multiply these two then we should be able to get a good     
23:42     
uh you know dot dot product or the you know similarity between like how this     
23:48     
you know attention got imparted into that the layers that we want so once we are doing this attention so if we apply     
23:56     
it more then uh the value gets the the values that are lesser important will     
24:02     
come close to zero and that are more important will will go close to the the value that we desire or that is one so     
24:10     
that's how we give like certain uh waiting mechanisms here the layer normalization and other are basically to     
24:17     
you know balance out a certain things called like you know a gradient explosion or uh you know uh uh I mean     
24:26     
like uh you know uh a gradient explosion or something like uh     
24:32     
uh what what exactly it is so the weights all become zero and that's     
24:38     
that's one of the case so yeah so yeah so this these are the certain uh ways of     
24:44     
doing it this is a multi head attunction and this is uh one of the good paper where they have applied uh uh hypograph     
24:52     
graph neural networks uh this is one way where they have done they apply attention at each and every layer at     
25:00     
each and every embedding node so here we can see that attention is applied at you know the hyper node and hyper edges of     
25:06     
the previous layer and then they encode using a theta which is a parameter so it's like a learnable so these are the     
25:13     
and and this is again aggregated using a sigmoid function so so so this is more     
25:20     
like a uh this this functions act more like a global type attention and these     
25:27     
act like a local type attention so these are applied inside the hyper edges that we want and this is again globally     
25:35     
aggregated from all the you know hyper edges that we have and the hyper nodes     
25:40     
that are inside that hyper edges so this is more if we have little more time then     
25:46     
we can go into details of that paper but as of now let's get to you know the overall structure     
25:53     
so so this is uh this is what uh like more of message passing mechanism so     
25:59     
what are the targets that we select the first thing what kind of messages that we want to pass is the second and the     
26:05     
third is what kind of uh uh uh the third is so what kind of     
26:13     
aggregation function that we are trying to use in order to get the desired representations so these are the three     
26:19     
different stages of applying you know uh message passing mechanisms to the     
26:27     
hypographs so once this message is passed what we want to do is we wanted     
26:32     
to learn to do certain tasks so these are like you know uh I mean like uh     
26:40     
let's go little to the taxonomy to see a little change yeah so we have seen that     
26:46     
you know we have seen node to node node to hyper edge these are the different message passing     
26:53     
uh mechanisms that we can see and this message passing once the message is passed we have to learn to aggregate the     
27:02     
correct messages into the the right set of hyper edges and right set of hyper nodes so for that we need a objective     
27:10     
function or the loss function that is very much aligned to the problem that we are trying to solve so that is the last     
27:17     
step which is like the objective function here so here in the objective function so there are like generally we     
27:24     
try to do a three bit this this paper mostly focus on these three task one is     
27:29     
learning to classify i mean like you know classifying these set of nodes as     
27:34     
uh you know saying we build a hyperraph and we say that okay these set of nodes     
27:41     
actually corresponds to uh I mean like these set of nodes actually corresponds     
27:46     
to uh cential cells in order to do that what is the correct objective function     
27:52     
so we have learned that message passing mechanisms and how do we design what kind of information that we give into     
27:58     
that is all we have known so how do we classify them saying that you know these     
28:03     
set of you know cells are you know form like an higher order interaction and form a sential tissues or say that you     
28:10     
know these form like you know motor neurons these form more like a you know sensory neuron and these form like a you     
28:18     
know uh like a different set of neuronal groups that we actually see so how can     
28:24     
we you know formulate and you know learn to classify learn uh learning to     
28:30     
contrast or learning to generate these set of things so these are like the objective function is one of the     
28:35     
fundamentally important thing that we do to get our hyperraph neural network to     
28:40     
you know learn so so hyperraph neural network can learn uh higher order     
28:47     
interactions by classifying hyper edges uh either positive or negative saying that you know these are the set of you     
28:54     
know yes or no binary type interaction so a positive hyper edge is a ground truth or a true hyper edge and the     
29:00     
negative hyper edge is generally a huristic or a generated hyper edge or we     
29:06     
say it's false so you have predicted wrong suppose that's a fake hyperage     
29:11     
so so this is a table that I'll discuss later because this is more of uh what     
29:17     
let's like it's a more of uh you know all the things aggregated in the same place saying that you know uh what we     
29:25     
have done till now and what are the papers that were published and what are the years of them so I will discuss about this uh once I once I'm completed     
29:33     
with the uh objective so the huristic negative sampling they call it so what     
29:40     
What they say is uh they have a negative hyper edge containing k random nodes and     
29:47     
they believe that they form small motives so each negative hyper edge contains a randomly chosen kense nodes     
29:54     
so these are negatively sampled motives and we also have negatively sampled     
30:00     
clicks which are like each negative hyper edge is generated by replacing a randomly chosen node in a positive hyper     
30:08     
edge with another randomly chosen node that is adjacent to remaining nodes so     
30:13     
it is like negative how these negative sampling works is so we have     
30:20     
uh say that these do not uh you know uh come close together or these do not have     
30:26     
any you know uh similarity then we call it as like a negative     
30:33     
uh so like in this negative what we're trying to do is so first we determine     
30:39     
the size and we determine the number of motives and the number of clicks so once we determine that then probably we can     
30:46     
do that probable negative sampling so based on that rule so how can we do that     
30:51     
so so how can we learn this negative sampling so there are like several objective functions that can be applied     
30:59     
at uh you know uh like uh we can Say that you know in order to classify we     
31:04     
can use a simple uh you know saying that you know this is this color and no not     
31:10     
color exactly this is this node by based on the color we can you know classify all of them saying that you know at this     
31:16     
stage it is this and next stage it is this so we can probably do that using that and     
31:24     
uh and learning to contrast so learning to contrast is basically used uh     
31:30     
contrastive learning so in contrastive learning the main idea is to learn in a     
31:35     
self-s supervised fashion so we do not have any specific set of nodes or you     
31:42     
know nodes that we wanted to classify we do not have the information of you know     
31:47     
which belongs to which set of features then we use this contrastive learning principle so what happens in contrast to     
31:55     
learning principle is when we are trying to you know design uh you know a     
32:00     
classification network so this contrastive learning mostly gets the good embedding representations that is     
32:07     
required for our graph okay uh suppose we say that I have a graph which is     
32:14     
uh which has these set of connections uh suppose this is our graph believing that     
32:20     
you know one is connected to two one is connected to three and three is connected to two and this is the graph     
32:26     
that we have so what contrastive learning approach does is so this graph     
32:32     
is our original graph so what it does is uh it removes the information in     
32:39     
randomly it removes the information randomly in each of the node suppose it     
32:44     
removes 50% information in one 30% in three and 40% in four so 50 30 40%     
32:53     
information and it removes certain hyper edges uh I mean like it removes the     
32:59     
connection between you know one and two uh two and four one and four two and four and now it becomes a very new     
33:08     
graph so the information is lost and certain set of hyperies are lost so it     
33:13     
arguments in this way so this is called a graph augumentation so saying that we     
33:18     
remove certain set of nodes and certain set of edges that is the one transformation the other transformation     
33:25     
is randomly remove certain set of nodes and edges of certain features instead of     
33:31     
removing 1 three and four it remove three two and four so that is another set of uh you know augumentation of the     
33:38     
same graph now what it does is it gives to and it gives to our neural network     
33:44     
hyperraph neural network and when it's given to the hyperraph neural network so it says to the hyperraph neural network     
33:51     
that these two graphs are the same and it will learn in that fashion so when it     
33:56     
learns that these two hyperraph pairs are the same then     
34:02     
uh I mean like the the model or the hyperraph neural network tries to learn that okay these two hypographs I mean     
34:10     
like the hypographs are same believing that whatever the information that we have given so this is done at a multiple     
34:19     
times I mean like you know different different iterations it tries to do that so I can give a small uh you know view     
34:26     
like example for that uh uh I mean like uh I can simply use     
34:33     
uh uh kind of a notebook     
34:43     
or so where shall I show to make it more little intuitive     
34:52     
uh so uh yeah so probably I'll try to visualize them in the next week and     
34:58     
bring it so that you know this part is more clear so it does two different augumentations so so this tow is     
35:05     
automation augumentation strategy it transforms the given graph of hyper nodes and hyper into different space and     
35:12     
it encodes two hyperraph uh two hyperraphs so what they call it this has     
35:18     
like hyperraph view pair so this is a V1 and V2 and it gives to the network and     
35:24     
tells the network that these are the two the same and it learns in that fashion so this is generally a contrastive based     
35:30     
learning so that's one way and there are like other ways like you know they use uh ME so so masked autoenccoder they     
35:40     
mask certain set of hyperodes and hyper edges and during the prediction they try to predict that okay these are masked     
35:47     
try to predict the full so uh these are like you know certain set of missing features try to predict the     
35:53     
full and that's how it learns to you know acquire the representation that's also one way so here we see there is a     
36:00     
contrastive loss it tries to find out similarity between the first argumentation of uh so here P we call it     
36:07     
as as we have seen P is like the hyperodes the information in the hyperodes and P dash is P single P prime     
36:15     
or P dash is uh single view and P prime or P dash is     
36:22     
basically a second view they try to find the similarity in order to find similarity we can use cosine or you know     
36:29     
uh ukidian distance or basically any type of distance that could actually     
36:34     
work and we use this loss function this is called uh uh normalized temperature     
36:41     
uh uh you know cross entropy loss this toao is basically the temperature     
36:48     
coefficient that is used to train you know tweak the model's performance in the vision it's mostly uh this this this     
36:56     
loss was I mean like adapted from like computer vision where they use it for uh     
37:02     
you know uh self-supervised visual representation learning the similarly we used in the graph where     
37:09     
uh these are like two different graph views and we try to minimize the similarity between those two so we     
37:16     
maximize the similarity between those two views so that the representation what the model has acquired it gives uh     
37:23     
it acquires like greater features and aggregations what we are trying to understand so this is one way and this     
37:29     
is at the hyper edge level this is at a hyper node level so the difference here we see is instead of encoding the     
37:36     
information in the hyper node here we use hyper edge so that's the only     
37:41     
difference that we see and uh at the membership level what we try to do is     
37:47     
where we suppose the Q dash is an anchor or uh you know we have uh you know an     
37:54     
another set of uh anchor or basically the image that uh so not image so the     
38:00     
graph that we actually used is basically called as an anchor and from that we     
38:06     
designed or derived two representations one this first transformation and the other transformation so first prime and     
38:13     
second prime so this anchor is basically uh this acts like an anchor and tries to     
38:19     
you know maximize the representation between these two [Music] so yeah uh so     
38:27     
so yeah so I mean like these are the different types of laws that they have previously have used and uh recently     
38:35     
there are like more number of works not recently 2020 and 2022 the most works     
38:41     
are like predominantly in the line of uh the self-supervised uh fashion because     
38:47     
that is actually giving more performance than a supervised version of GCN suppose you take a simple GCN and a     
38:53     
self-supervised GCN so the self-supervised version is performing more better in terms of understanding     
38:59     
the features and uh in most of the real world scenarios we do not have like explicit labels so this is one of the     
39:06     
good way of trying to learn the representations so and learning to generate uh so once I have mentioned uh     
39:14     
that MAE version is basically trying to you know you have a graph with missing     
39:19     
set of features and missing set of hyper edges and this hyperraph it try to predict the missing set of nodes using a     
39:28     
you know masked autoenccoder and it tries to predict uh the complete graph     
39:33     
from the missing set of uh hypograph it tries to complete the complete hypograph     
39:38     
so that is one of the type of generating ground truths higher order interactions and we assign certain because we already     
39:45     
know that you know these are the certain you know this is how the structure it has to be and when we are trying to     
39:51     
predict in the future then it aligns with the ground truth that it actually has so so the hyper graph argumentation     
40:00     
the node and hyper graph subset encoding the loss functions are details similar to that of that only and uh yeah it's     
40:09     
more of uh the similar strategy that they encode it's not uh not a change     
40:16     
that we see here and uh because uh that these two parts we need a little more in     
40:22     
depth of understanding with cleaner visualizations because it's more intuitive     
40:28     
This is more of theory overall explanation but when I try to make sure that you know how these functions     
40:34     
actually make difference using uh you know designing a graph or in a matrix and multiplying them and showing that     
40:41     
you know how is the change that makes more sense using a good visualizations you know so probably I'll do in the next     
40:49     
set of uh weeks when I have more time dedicated to this so we have seen that     
40:55     
you know we have uh the redactive structured uh I mean like we how do we     
41:01     
design a certain type of structures the redactive transformations are non-redactive so the type of embeddings     
41:08     
that we have edge dependent or not and we have certain set of aggregational principles are they learnable or not so     
41:15     
these are the set of works that you know people have uh these people have curated     
41:20     
for our uh you know some interest so we can see     
41:28     
that you know uh we are mostly focusing on non reduct uh you know deductive     
41:34     
transformations uh as this so we want uh more of uh you know non-directive     
41:41     
transformations like star line and tensor because we we wanted to you know     
41:47     
plot a hyperraph and plot the higher order interactions learn them and once     
41:52     
we learn those set of interactions we wanted to you know retrieve back the hyperraph and see where exactly it's     
41:58     
reflecting so so the non-redjective transformations are more of useful for our case and uh for the target selection     
42:06     
and the message passing mechanisms we we want to learn uh the hyper edge features     
42:12     
because and it should not be uniform because it varies from one set of hyper     
42:18     
edge to that of other so it should be variant it should not be uniform so we     
42:24     
use a learnable uh uh pooling mechanisms or uh whatever uh the mechanisms that we     
42:30     
have described so we want to focus only on the learnable parameters so here we have seen the target selection and uh so     
42:38     
the representation how do we encode the messages so we we will encode only the messages that uh are dependent on the     
42:46     
set of messages that we already acquired so and we have to apply a set of local     
42:53     
and global attention too because when we apply global attention the whole hyperraphs get uh you know get uh you     
43:00     
know uh a certain set of uh you know feature dependencies but whereas when we     
43:06     
apply locally we can say that okay local region is the motives that we build across certain hyper edges so we have an     
43:13     
hyper edject clause a small motive believing that this has to have a certain set of attention then we have to     
43:19     
use uh you know local attention in that space and uh coming to the objective     
43:27     
function we can try out using uh a supervised mechanism because already we have labels at uh you know each and     
43:34     
every you know time stamp of the embryogenesis and also we have uh uh or     
43:39     
we can apply the self-s supervised version trying to understand you know okay these are the set of hyperraph     
43:46     
interaction can you give me the best interaction based on you know uh the random uh set of you know the missing     
43:53     
nodes and edges by reconstructing from the given graph does it uh will it be     
43:59     
able to acquire like good representation so we can try out like no this uh different message passing mechanisms     
44:06     
first and uh we can next try out certain set of uh training principles one is     
44:12     
supervised way and one is uh contrastive way because contrastive way is yet to     
44:17     
proven good results in the previous uh you know standard citation networks and     
44:22     
other so I believe that you know the contrastive learning could be also useful so so we have I mean like of my     
44:30     
thought for the given data I think non-redictive transformation is the right approach then we have to try out     
44:37     
two different two or three different type of message passing mechanisms And then after this we can try out     
44:44     
learning that in a you know supervised or unsupervised fashion so so this is     
44:49     
what from the paper and they have also given uh and these are the set of works which     
44:55     
we are trying to understand i said uh you know uh when I said uh these I mean     
45:01     
like uh when we are using only non-redictive transformations so     
45:07     
deductive no then we can focus only on this work and later when we say that you     
45:13     
know uh edge dependent then mostly are edge dependent so and then uh are there     
45:20     
aggregation learnable so we focus on like the learnable uh aggregations and     
45:26     
uh there is edge dependence and uh and there is no deduction so we can just     
45:33     
focus on certain neural networks that group into this and read about more what     
45:38     
it's like and uh go make a progress based on     
45:44     
this so any questions really some interesting things you brought up you     
45:50     
know you talk about message passing that's like a standard thing in graph neural networks so if we're applying it     
45:57     
to biology what are the messages so the messages are well they can be a lot of     
46:04     
things actually um you can have like you know different dependencies which be     
46:11     
messages um and and so in biology we have actual messages sometimes     
46:18     
uh we have like signaling but we also have things that get inherited so those     
46:24     
are also messages so you know that there are a lot of opportunities to like kind of play with     
46:31     
the message passing idea quite um you know extensively and see how that you     
46:38     
know different types of messages it would require different types of data sets or maybe uh synthetic data sets to     
46:45     
see what you know how those networks would be different or what their     
46:50     
learning capacity is so if we had one network for example that was signaling based and another that was     
46:59     
inheritancebased and maybe a third where you transmitted forces like this is where you have a mechanical network and     
47:06     
it's like things going from one place to another that's those are all message passing but they different place you     
47:14     
know different types of networks different types of learning uh capacities and things like that     
47:22     
um let's see yeah and then you know there is this uh part let's see if I can     
47:28     
find it here uh where you did the different ver like um there were     
47:34     
different views of the network so this is something in machine learning people have done a lot with like different data     
47:41     
or different views so could you go into that a little bit more     
47:46     
uh should I explain or uh if you give me a little time I can you know sketch it     
47:52     
out a little bit in more detail yeah yeah that'd be good     
48:00     
oh well I just wondered what kind of math that was looks like linear algebra to me a lot of it yeah a lot of it is     
48:08     
linear algebra okay says I think they're using this     
48:14     
with the tense segrity models somewhat some of it looks familiar     
48:21     
yeah there's probably mathematical continuity between like a lot of machine     
48:26     
learning and stuff like 10 seconds or at least you can state it in the same way so that's interesting interesting     
48:34     
yeah they start off with crystalallography to name the nodes because it's 3D right     
48:41     
and then and then they go into well it looks like some of this math with     
48:48     
it so I I would like to be able to do this     
48:58     
uh just give me a second so that uh I'll     
49:26     
Oh well I'll turn this off sometimes it interferes     
50:01     
yeah so I'll share my screen once and I'll try to explain     
50:08     
uh what uh can be seen     
50:16     
like I mean suppose uh     
50:50     
Suppose this is our hypograph and when we are trying to do type of a starrass     
50:56     
type of expansion saying that okay these are uh set of uh the you know the hyper     
51:04     
edges or the I don't think you're projecting the right thing     
51:10     
oh     
51:20     
sorry I can see your face and then I can see your uh PowerPoint slides     
51:25     
That's it     
51:52     
there we go i see now yeah so I'll try     
51:57     
to make     
52:21     
it so are we able to see the full screen yes yes okay okay it's     
52:31     
great suppose these are the type of graph that we wanted to build i'm drawing just a rough graph so that for     
52:36     
an intuition it will look good and     
52:55     
just so this is just a star expansion and uh we can take it like another     
53:01     
expansion too so this just for the intuition I have drawn so that uh you know it's just representation     
53:10     
represented similar to that of the hyperraph that we build here so here this node has interaction between the     
53:16     
hyper edge green color and the blue color so similarly these are the hyper edges so green and the red sorry and     
53:23     
this yellow shares with the red and the yellow neighborhood so I have uh Okay     
53:29     
Ron already interpreted that part probably I can remove     
53:35     
that okay this is not it this is it yes     
53:41     
so this this has like two edge connections so this is how our     
53:46     
uh know we know embedded this hypograph into know this our uh the given     
53:53     
hypograph now what we are trying to do is suppose we have     
53:59     
uh doing a transformation t1 saying this is has like a transformation what we do     
54:05     
what I what I'm trying to do is this carries an information of I suppose one i1 i2 i3 and here each and every node     
54:14     
carries information this hyper nodes carry information of suppose per say like uh     
54:21     
uh like g1 g2 2 G3 I say gra I mean like     
54:28     
uh node right so it's better to use n1 n2 n3 n1 n2 and n3 and this carries uh     
54:36     
you know set of informations so when I do this transformation uh whatever the information present in the green node     
54:44     
may be reduced to a percentage of 20 so this is the information that is present     
54:49     
in that n1 suppose and these are the features that it has what I'm trying to do is mask these set of features and     
54:57     
keep it empty so that it will try to you know the whatever the information it has     
55:03     
it has been reduced only a certain part of information is given and we are trying to remove certain set of uh no     
55:09     
nodes so then this transformation should look like so so here one we have removed     
55:16     
one connection to this and this node is suppose uh empty one one node suppose     
55:22     
this is empty and uh here we have three connections and here I'm missing out one     
55:28     
more node so this is also empty and uh this has a connection between this so     
55:36     
which I have removed so I the removed connections I'm you showcasing it like a     
55:41     
dotted line so that it'll be clear and uh here I am trying to remove the     
55:50     
connection so this is how our uh new hyper graph looks like so this is one     
55:56     
transformation and the information that was present here inside each and every node is again compressed or reduced by     
56:04     
some proportionate randomly so we have algorithms that do you know remove     
56:10     
certain set of information this is called transformation one or view one we     
56:15     
say these are like different views next we try to you know use another transformation t2 which has a different     
56:22     
set of uh uh you know uh alignment so this this removes certain set of nodes     
56:29     
and certain set of information and uh it looks little different so suppose this     
56:34     
is our uh hypograph so instead of removing these     
56:40     
two it removes these two information and this is connected this was connected previously this might not get connected     
56:46     
so this is dotted so this is how the pair of interactions can be removed i     
56:52     
just gave an example for hyperraph in graphs this is like a standard scenario so what now it does is so we have trans     
56:58     
transformation one and transformation two so uh in the next uh step what it does     
57:06     
is what it does is uh we have transformation uh transformation one and     
57:13     
uh sorry and transformation two it gives to     
57:18     
a graph neural network or hyperraph neural network and it gives us saying the     
57:25     
representation that this is R1 and this is R2 so whatever the loss function that     
57:31     
I have explained exponential of uh you know the representation one comma representation two uh divided by toa     
57:39     
this is the parameter temperature parameter by summation of the suppose we     
57:44     
are doing it by the nodes so summation of exp of     
57:50     
representation of suppose this is i ra i of 1 2 so this does for all and     
57:59     
this is for like individual and this is how it calculates the similarity between R1 and R2 saying that whatever the     
58:05     
transformation that happened here and transformation that happened here will go through a GNN and it will give two     
58:11     
representations and these two representations are same so so this is     
58:16     
the loss function that says that these two representations are same so what the network believes we are trying to learn     
58:23     
this typograph neural network right so what this network believes is so whatever the view that I got here and     
58:29     
got here are the same and it learns in that fashion and with the limited information that it has at each and     
58:36     
every node and limited hyper edge interactions what we do during the test     
58:41     
time so this is while training during the test time when we have less number of you know labeled information we pass     
58:49     
our uh new hyper graph so say gh and     
58:55     
then we give to this uh hyperraph neural network then the set of embeddings it     
59:00     
gives embeddings or the representations uh during the test phase what it gives     
59:06     
this is more significant and valuable than the information that was carried     
59:11     
out by a by training in a supervised fashion so so that is what the I mean     
59:18     
like the principle here works around so so this is how it generally works and uh     
59:28     
you would use it in contrast to sort of label data so if you didn't have labels     
59:36     
yeah if you didn't have the labels we we use the that to you know contrast one     
59:42     
with the other and learn the representations okay     
59:49     
so this like more recent works that uh you know Morgan have attached so many works uh like recently published and all     
59:56     
right uh in the group so in that group we see these are the most uh recurring     
1:00:01     
papers that uh we see so they tweak the loss function in certain fashion to improve the performance of certain set     
1:00:08     
of networks so this can this is barely applied in biology yet I believe you know so we have to exactly see where the     
1:00:16     
application you know fits around and what kind of biological interactions it can form how meaningful are they and it     
1:00:25     
will be more uh useful for us to understand in detail like what this will     
1:00:31     
give us yeah yeah that's good um yeah that's     
1:00:36     
good so yeah I had I guess I I've got like all the questions I had um I think     
1:00:44     
you went through a nice uh sort of typology of some of these different methods like you know um how do you     
1:00:50     
expand and do the sort of the details of message passing and I guess the you know     
1:00:56     
the question then is like how do you apply that to biology and you know there are different types of biological     
1:01:03     
message passing and how does that relate to what we might do in a graph neural network and how do we um because you a     
1:01:12     
lot of times times in like generic network theory people use message     
1:01:17     
passing for like determining causality like if one node affects another node     
1:01:23     
what is the you know how do we know that from like we basically would have like a     
1:01:29     
a connectivity matrix and then we would have like a message passing matrix which     
1:01:34     
would be like uh you know if something originates in one node and it ends up at     
1:01:41     
time like at time zero it would be in this node at time one it would be over this     
1:01:48     
node how can we determine that flow so you'll see that a lot in network     
1:01:53     
neuroscience it's applications to like you know hydrodnamics or something like that     
1:02:00     
where you have you have to from a maybe a time series or from a separate data     
1:02:05     
set where you have actual information about incidents um where you know what is that you have     
1:02:12     
to reconstruct things moving from one node to another and then that gives you causality because you know that activity     
1:02:19     
in this node caused activity in this node and it goes across the network and     
1:02:24     
you have to filter out the confounding signals because of course not every it's     
1:02:30     
not linear you have interactions going every which way so     
1:02:39     
yeah so I look into the this interaction parts how this can be exactly manipulated in the biological     
1:02:46     
space and you know the causality that you mentioned I mean like the you know     
1:02:52     
the how they interact so the cause and relationship that we actually have that     
1:02:57     
is more important when we're trying to model so yeah so yeah look into that so     
1:03:11     
uh Jadratha had had posted a archive paper in the chat so this is the uh for     
1:03:18     
experimenting with different kinds of message passing schemes there's a nice paper called PNA that's a method I guess     
1:03:25     
it shows that we can experiment with any permutation invariant aggregators we can     
1:03:30     
combine them this is the paper principal neighborhood aggregation for graphn nets     
1:03:36     
uh and graph neural networks have been shown to be effective models for different predictive tasks on graph     
1:03:43     
structure data recent work on their expressive power has focused on     
1:03:48     
isomorphism tasks and countable feature spaces then they extend this to include     
1:03:53     
continuous features which you see in a lot of real world domains of data so a     
1:03:59     
lot of biological data in other words that we can't segment easily or you know     
1:04:04     
if we segment them they're kind of artificial things like you know are when we segment cells or segment an embryo     
1:04:12     
say into nuclei cell nuclei you know are we capturing sort of individual cells or     
1:04:19     
we capturing the centium that we've been talking about or you know do those     
1:04:24     
nuclei sort of form the basis cells but then those cells are getting like a     
1:04:30     
migratory flow and do we want to actually capture the flow instead of the     
1:04:36     
um you know the individual nuclei and you know all those sorts of things so we     
1:04:42     
can do that sort of thing here they propose principal n neighborhood     
1:04:47     
aggregation or PNA a novel architecture combining multiple aggregator with degree scalers     
1:04:56     
uh finally we compare the capacity of different models to capture and exploit     
1:05:01     
the graph structure via novel benchmark containing multiple tasks taken from     
1:05:08     
class classical graph theory alongside existing benchmarks from real world domains with this work we hope to sh     
1:05:16     
steer some of the graph neural network research towards new aggregation methods     
1:05:21     
which we believe are essential in the search for powerful and robust models so those aggregation methods are     
1:05:28     
also interesting um what did you have any words about that like uh like what is the so I guess     
1:05:36     
what we're trying to do with aggregation is we're trying to figure out what are the constituents of a     
1:05:42     
hyper correct yes sir uh actually this uh work which     
1:05:50     
uh Jata has shared is focusing on the graph neural networks so so so it's more     
1:05:59     
relevant when it comes to the shared representations that a graph neural networks carry out so and it uh exactly     
1:06:08     
does not fit into the hyperraph but uh it can be definitely tweaked into the     
1:06:13     
formulation that we have and yes sir we have to like look more     
1:06:18     
into details about it well I mean I'm just thinking about like how that pro how the hype like if we're     
1:06:27     
talking about hyperraphs how that process works or you're trying to aggregate things into hyper you might     
1:06:34     
also do this with categories of cells or cell type i'm thinking more about like     
1:06:39     
uh biological connection so you're really trying to just find almost like     
1:06:45     
find categories find mean behaviors if that if that makes sense     
1:06:51     
like you're trying to find things that can be aggregated together it's always a a problem that we     
1:06:58     
have in classification like what things go together what things don't     
1:07:08     
great yeah that sounds good that's all very good stuff thank you um for that     
1:07:16     
presentation yeah and you know we're kind of walking through uh Lewis gave a     
1:07:24     
presentation last week on on this paper so we're kind of walking through it and     
1:07:29     
trying to unpack it and trying to get a sense of what's what's possible how it     
1:07:35     
connects to biology and so forth     
1:07:42     
so and also adding to that uh we still need more to understand exactly how does     
1:07:48     
it fit to the biology so I was just giving a fair and thorough represent I     
1:07:54     
mean like thorough understanding of how this hyperraph network works and how it can be used but how it fits is like a     
1:08:02     
stronger research question and it     
1:08:07     
needs more time yeah yeah     
1:08:12     
okay all right well that's it for today hey     
1:08:18     
uh I think Hussein was joining us but I think he got knocked out of the room i don't know if he had something to add     
1:08:25     
but anyways uh I guess that's it for today so thank you for attending     
1:08:32     
thank you thank you all have a good week okay bye bye bye thanks bye     
