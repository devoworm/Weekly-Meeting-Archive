Transcript   
   
   
Search in video   
0:01   
Ah, hello. Hi. How are you? I haven't seen you for a while.   
0:06   
Yeah. Yeah. I haven't seen you for a couple weeks. So, all all I've been doing is looking   
0:12   
up references and then my computer died. So, all of the references died with it. So, um I   
0:21   
did print out the important ones. So, I have a print out, but I have to reook   
0:26   
them up. So, use a reference manager or   
0:34   
um no. Oh, okay. I I just um   
0:39   
if they're important, I print them up because I have trouble reading them online or on on the computer. Like it   
0:46   
just bothers me. So, the ones I'm going to use, I print. Okay.   
0:54   
I've got one called ultrasound induced drain cytokeleton rearrangement   
1:02   
an experimental and simulation uh study   
1:08   
that's important because uh Dr. Sharief wanted me to use   
1:15   
ultrasound to manipulate the eggs and um   
1:21   
and I I was not sure about it. I said I don't think that's a good idea.   
1:27   
Well, this that kind of proves proves my point.   
1:33   
Yeah. And not only does it heat things up, but   
1:40   
it it rearranges the cytokeleton.   
1:46   
Yeah, it's a stress. Oh, there's a few things here. They've got revealing non-trivial information   
1:54   
structures in a a neural biological tissues via   
2:01   
functional connectivity. I don't know what I don't exactly know what that's about. That's probably why   
2:08   
it got copied. It It looks like um the networks.   
2:14   
Okay. This is a This is a networks paper. Okay. Yeah, it sounds like it.   
2:21   
Are you interested in it? I can type it in the Yeah. Yeah. Why don't you type it in?   
2:27   
Okay. Oh, there's a couple of them here that are interesting. All right. This one. Yeah. Blackest.   
2:34   
Yeah, I found that one. Yeah, that's it. Okay.   
2:43   
All right. Yeah, let's like take a look at it here. So, this is it here.   
2:49   
Revealing non-trivial information structures in a neurobiological tissues   
2:56   
by a functional connectivity. So this is uh   
3:03   
by Doug Doug Blackeston who's from Tufts at the Allen Discovery Center and works   
3:10   
with Michael Leven. Uh this also has Josh Bongard on it. Spors   
3:17   
who are uh Josh Bongard is doing things with Rob embodied robotics and then Olaf   
3:23   
Spores is a networks person. Thomas Varley is an information theory person   
3:29   
and so on. Uh so I think I actually reviewed this paper   
3:36   
for a journal. It's okay. It's in plus computational biology. It might not be   
3:41   
this paper but like a a similar paper. So it's my disclosure on this.   
3:47   
Okay. Um yeah. So let's see. Okay. So yeah this is the paper here.   
3:53   
This is the bioarchchive version. It's also in plus computational biology.   
3:59   
Um so let's go in a little bit.   
4:05   
Okay. So uh so it says a central challenge in   
4:11   
the progression of a variety of open questions in biology such as morphogenesis,   
4:17   
wound healing and development is learning from empirical data how   
4:23   
information is integrated to support tissue level function and behavior. So   
4:28   
you have um like I guess cells or something below the tissue level of   
4:35   
organization. there's information in there and then there's an integration of information   
4:42   
uh to support like the formation of tissues. So the implication is is that   
4:48   
um the cells or the things below the tissue level form a network and there's   
4:54   
this integration of information and then network that supports the formation of tissues. So this is there's this idea of   
5:02   
um integrated information in neuroscience that I think they're drawing from for this. Uh but they're   
5:09   
going to use information theoretic approach. Uh they're going to use an information theoretic approach to   
5:15   
provide a quantitative framework for extracting patterns from data. And so   
5:21   
this is something that's been done in neuronal systems at the tissue level. What they're showing here is how time   
5:27   
series of calcium ion dynamics can be used to identify the structure and   
5:33   
information dynamics of other biological tissues. So they're interested in ion   
5:40   
channel flux in non neuronal cells. So you know we can look at that in neuronal   
5:47   
cells and we can evaluate um action potentials and things like that and   
5:53   
other cells will produce potentials electrical potentials. They're not action potentials but nevertheless they   
6:01   
have these like you still have that actin measin complex   
6:08   
and it runs on calcium and ATP. It's in every cell,   
6:13   
right? Yeah. Yeah. So, the dynamics look differently than they do in the neuron,   
6:20   
but they're still useful. Okay. Um, yeah.   
6:28   
So, they use a calcium reporter in an organoid system. Um, and they do this in   
6:36   
an expplanted amphibian epidermis uh derived from zenapus leis. They image   
6:43   
the calcium activity pre and post and this is a puncture injury is the   
6:49   
perturbation. So preuncture injury, postpuncture injury and they do this for   
6:54   
six replicate organoids. So they're trying to induce regeneration in that tissue.   
7:03   
Uh we constructed functional connectivity networks by computing mutual information between cells from   
7:10   
time series derived using medical imaging techniques to track intracellular calcium.   
7:18   
We analyze network properties including degree distribution, spatial embedding   
7:24   
and modular structure. So these are all uh measurements of you know networks and   
7:30   
their sort of structure. So degree distribution is um the size of the   
7:36   
network or how far one node is from another. Spatial embedding is uh a   
7:43   
spatial measure and then modular structure is how modular or how sort of   
7:49   
compartmentalized the network is. We find organoid networks exhibit more   
7:55   
connectivity than null models with highderee hubs misoscale community   
8:00   
structure with spatial clustering. So this is where basically it's not random.   
8:07   
It's non-random and we have this sort of community   
8:13   
structure which means that you have these groups that exist within the   
8:19   
network and they represent things like you know different parts of a tissue or   
8:24   
different cell types that contribute to the formation of a tissue. So there's a   
8:30   
spatial clustering, there's community structure, meaning that there are sort of these um subgroups in the network and   
8:39   
that relates to the form and sort of its ability to form tissue.   
8:45   
Utilizing functional connectivity networks, we show the tissue retains non-random features after injury,   
8:52   
displays long range correlations and structure and non-trivial clustering that is not necessarily spatially   
8:59   
dependent. So this is Oh, go ahead. Um yeah. So that um   
9:06   
a non-injured one compared to an injured one are different different network-wise   
9:13   
or Yeah, the network changes its properties when you injure it or post injury versus   
9:18   
pre-injury. Interesting. So you're you're you're puncturing it,   
9:24   
you know, you're puncturing the tissue. It's changing the connectivity because   
9:29   
the cells reorganize. I mean, if you've ever seen a video of like an injury to   
9:35   
some, you know, like if if you've seen wound healing where the cells will sort   
9:40   
of migrate towards the site of injury and kind of fill in and then uh   
9:46   
restructure their or reorient themselves to one another and restructure. There's   
9:52   
like an you know um a fiber network that forms over the site of injury. Um, and   
10:00   
then that's and then they reorient themselves in that way. So, it's basically changing the topology of the network.   
10:08   
Okay. Well, that might have something to do with development. Yeah. Yeah. Well, and this is these   
10:14   
aren't organoids. So, you've got organoids that are um I I don't know what the how integrated they are in   
10:21   
terms of the different tissue types, but you have these basically these expplants   
10:26   
or organoids. They're growing in a culture. And you know, if you take a cell culture and you like scrape, if you   
10:34   
take like a confluent cell culture, which means the cells are all kind of packed together, and you scrape it, the   
10:40   
cells will migrate in to fill that scrape. So, that's probably what's   
10:45   
happening here. They're just kind of migrating inward to fill in the hole, you know.   
10:51   
Yeah. Um, okay. So, this that's what Okay. Our   
10:57   
results suggest increased integration after injury, possible cellular coordination and response to injury, and   
11:05   
some type of generative structure of the anatomy, which I guess means that there's like   
11:11   
some sort of like way that it sort of knows the anatomy or knows the anatomical shape or knows the anatomical   
11:18   
structure so that it reorganizes around that structure. So again, if you're in a   
11:24   
culture, you know, if you put a hole in that in that layer or you put a hole in   
11:30   
the the structure, it's an organoid, it'll fill in that gap. It won't just kind of migrate outward and just forget   
11:37   
about the hole. It'll fill in that hole. And so that's that's what they mean by   
11:42   
generative structure. Um while we study uh calcium dynamics and zenapus   
11:49   
epidermal cells, our computational approach and analyses highlight how methods developed to analyze functional   
11:56   
connectivity in neuronal tissues and how that can be generalized to any tissue   
12:02   
and fluorescent signal type. Our framework therefore provides a bridge between neuroscience and more basil   
12:09   
nodes of information processing.   
12:14   
Okay. Well, that it all sounds nice. Like   
12:19   
interesting. Yeah. And by the way, the the pictures in my   
12:25   
background here are actually my a lot of them are mine. Um not this side, the   
12:31   
other side here. Oh, yeah. Over there. These these are my pictures.   
12:37   
Great. Yeah. Yeah. Great.   
12:43   
Anyway, um I do science and art.   
12:49   
Yeah. So they they talk a little bit about um   
12:57   
information and and information theoretic approaches and how this is um you know kind of key   
13:06   
to understanding collective decision making in neural networks but you can   
13:12   
also use this in cellular networks to look at how these structures kind of   
13:18   
reform themselves. So like you know you think about like the example I just gave of the monollayer that's been punctured   
13:25   
and then cells filling in uh to that void. They're doing collective decision   
13:30   
making to say this is where we need to be and this is the network topology we   
13:36   
need to conform to. uh you know, I'm not ascribing consciousness to them, but you get my point that there's a basically an   
13:44   
arrangement or a signal that's sent that um allows for that to to happen. Um and   
13:52   
so there's there are these long range correlations. Um and there's also a structure of phase   
13:57   
transitions in networks of neurons that uh result from the network structure.   
14:04   
And so they're now applying this to non-neuronal networks.   
14:10   
And that's kind of where they're they're going with this. So in multisellular systems, cells must collectively   
14:16   
coordinate their actions to regulate the diverse range of processes essential to   
14:21   
multisellular life. These include regulation of pattern formation and development, morphogenesis,   
14:29   
wound healing, regeneration, and behavior. uh communication and information sharing   
14:36   
can even extend beyond species specific boundaries as is the case for plant animal interactions and in symbiotic   
14:44   
associations like lyken where multiple species are in direct coordinated communication.   
14:51   
uh traditional techniques for characterizing coordination in non-al cellular tissues thus far include   
14:58   
microelerode arrays planer cell polarity analysis physiological reporter dies   
15:05   
aminoistochemistry and RNA seek so you can use all these to sort of understand cellular function and like things like   
15:13   
neural voltages alignment with other cells   
15:19   
um distributions of biomarkers ers and different receptor interactions.   
15:25   
And so what we what we want to do is we want to capture a longer longer time scale and a wider spatial scale. And so   
15:33   
that's where they're kind of you know their methods are kind of uh they need   
15:39   
to go to capture this what they call functional connectivity. So in uh neuroiming functional   
15:46   
connectivity is what they use to look at like if they take images of the brain and they have   
15:53   
say like fMRI data where they have voxels and they take those voxels and   
15:58   
they look at their activity and they're able to correlate um activity in such a   
16:05   
way to as to um sort of u approximate the functional connectivity.   
16:11   
So this is where you have, you know, if there's some activity voxil that has a   
16:17   
certain activation strength and if different voxels in the image have the   
16:22   
same activation strength at the same time or similar activation strength at the same time, it's said that they're   
16:28   
connected functionally because they're both active at the same time. That's   
16:34   
assumed that they're doing something functionally across that network. Okay? because there's I know in Xenopus   
16:41   
tail the cells are separated when it's developing and so they're not one   
16:48   
cohesive unit. So I wondered what that might look like. So I want to go to their lab.   
16:55   
Yeah. Yeah. Well, this yeah,   
17:01   
in this case, the organoids, I think the cells are pretty much uh they have some   
17:07   
sort of uh you know, physical connection because they're growing from the next   
17:13   
plant. Yeah. Yeah. Well, if they're mechanically connected that they would move together   
17:22   
or you wouldn't be able to tell when one started and one ended. So,   
17:27   
yeah. Yeah, I don't Yeah, I don't Well,   
17:32   
they're using markers that aren't mechanical markers, but of course then that means that they have, you know,   
17:38   
they share connections that are not just functional connections, but they share these sort of uh in neuroiming they call   
17:46   
it effective connectivity where there's like you know because obviously you have   
17:52   
you know different neurons that can be connected in the network through like   
17:57   
you know uh signaling, but you also have sort of physical connections or physical constraints. And so that's a different   
18:05   
kind of network. So you can work those two networks out. So in this case, you'd have like a mechanical.   
18:11   
Yeah. It would be nice to see both of them to see how they interact.   
18:17   
Yeah. Yeah. I don't know how you do that.   
18:22   
Yeah. Well, I think here Yeah. they're using like different markers and um   
18:28   
they're able to sort of um sort of approximate the functional aspects of it   
18:34   
and then I guess the physical as the physical connections are kind of assumed   
18:39   
from just them being in a common tissue which isn't really sophisticated right   
18:45   
you'd want to have some sort of map of how to what degree they're connected   
18:50   
what degree there are physical forces smooshing them together or pulling part   
18:56   
and that' probably affect their um function as well. But   
19:04   
um so this yeah this is so information theor so they're they're talking about these functional networks and how we can   
19:12   
quantify different correlative maps. So there's correlated behavior   
19:18   
like I said between say the voxil example. Um even if two voxels are   
19:23   
spatially separated they can be correlated and so that builds us our network. Information theory then   
19:30   
provides a set of tools to quantify these correlations and you can use measures like mutual   
19:36   
information and transfer entropy to construct these networks. So mutual   
19:42   
information is where if we take two of these nodes, we want to look at their   
19:48   
information over time or the information that they share in common over time. So   
19:53   
you know if you think about like uh statistical relationships   
19:59   
um a good way to kind of analogy to mutual information or another way to   
20:04   
measure mutual information is to look at covariance. So two node or two voxels or nodes will   
20:13   
co-vary over time and that that covariance just means that they share variance   
20:19   
um they share common variance and so you know in information it's a little bit   
20:24   
different measure but it has to do with how much information they share how much   
20:29   
information overlapping information do they share and so um that just tells us   
20:36   
like if two nodes modes are correlated. That's great. But they also maybe are   
20:43   
active at the same times and inactive at the same times over time. If we look   
20:49   
over like a a longer time window than just kind of one instance because we can   
20:54   
get a correlation for a single instance and say those two things are connected. But if we look at their time series and   
21:01   
we say okay these things are active and then they're inactive and this always happens together. um they share   
21:08   
information and so that connection means that there is some sort of information   
21:13   
being exchanged and so you have mutual information to measure that and so um   
21:19   
and I'm not really familiar with how they're applying transfer entropy but it's it's going to be a similar thing   
21:25   
where you're looking at two nodes to in this case voxels where you have this uh   
21:34   
you know you can tease out this relationship ship and make it a bit more solid.   
21:40   
So this is where we have this uh so we're looking across temporally simple   
21:46   
sample data and we're constructing these uh functional connectivity networks that   
21:52   
capture nonlinearities and structure not apparent in static images. So they want   
21:58   
to capture the dynamics here. Yeah, too bad they couldn't do that for   
22:03   
the surface of a xenopus egg or something. Yeah.   
22:08   
Yeah. As itself self self perturbs itself, right?   
22:17   
Okay. So a predecessor and contemporary approach to constructing functional   
22:23   
connectivity networks are anatomical connectivity maps which focus on   
22:28   
physical tracts that can reveal direct anatomical links between physi different   
22:34   
physical regions in a tissue. So this these are these anatomical connectivity maps where they look at like uh you know   
22:43   
these different structures in the anatomy and you know look to see if   
22:49   
there are different types of anatomical links that are functional. So this is um   
22:55   
you know not a not a zenopus egg but it's kind of like just looking at the tissue structure and seeing where they   
23:02   
kind of things are exchanged or things are connected. Um however these do not capture the long   
23:09   
range temporally correlated structures. So you know the uh anatomical   
23:14   
connectivity maps are just kind of like the structure of how anatomy is linked   
23:21   
across you know different regions. So like in a brain you know you might have   
23:26   
fiber long fiber tracts that go from one part of the brain to the other and that implies that there's some connectivity   
23:32   
there. We don't know if it's functional and we don't know what that functional connectivity looks like but there's a   
23:37   
physical connection there. And so that physical connection then is um you know   
23:43   
we can draw from that to to make uh inferences about the functional connectivity.   
23:49   
Um but again you know this is um they don't capture this temporally   
23:58   
correlated structuredness of the tissue. So like we can look at a snapshot of the tissue and say this is the way it looks   
24:05   
but it can development. The anatomy can change quite a bit and so those   
24:10   
Yeah. Yeah. So it needs to be a time lapse right. Yeah. And then you need to create   
24:18   
these dynamic maps that show this like because you'll have structures that transform over time. So those physical   
24:25   
maps will change. Yeah. Oh, well that sounds like it was a   
24:33   
worthwhile paper then. Yeah. Yeah. Not sure how they can incorporate that   
24:39   
into anything, but Yeah. I mean, it just looks like they're doing the, you know, that they just do   
24:45   
the organoid examples. They have a four cell embryo picture here. Um,   
24:50   
and then they take it apart and you're going, why would they do that? Like it's a perfectly good system just as an   
24:57   
embryo. Yeah. Yeah. So I mean this is like where they show the pre-puncture and   
25:03   
postpuncture data. Well like so that doesn't like why   
25:11   
why don't they leave it alone and just measure it   
25:17   
as a as an embryo system.   
25:23   
Yeah, I don't know that, you know, that might be a good baseline. U I don't know if they have   
25:29   
maybe they do have it in like their controls, but um it's hard to   
25:35   
Well, it changes if you excise it like they won't will no longer   
25:40   
it it might develop into something or it might not like it changes it. Right. Right.   
25:47   
So, if you take it out of the embryo context, you mean like Yeah.   
25:53   
It might develop into a cenapus egg after a bit.   
26:00   
Yeah, maybe. Yeah. And then they're just showing here where   
26:06   
they're segmenting the tissue, segmenting the cells and then building a network out of them. So,   
26:15   
um yeah, and then this is just uh more data from their time series. So they   
26:20   
have things in time. Um yeah and then the degree distributions which are um sort of   
26:29   
showing the nonlinearities in the network. So you have these networks that uh don't   
26:36   
exchange you know there there's like this because there's structure in the network there are these long tails where   
26:44   
certain nodes serve as hubs and other nodes serve as sort of the peripheral nodes. and information transfer within   
26:51   
those networks as a result tend to be nonlinear. That non those nonlinearities sometimes   
26:58   
express themselves as phase transitions. Sometimes it expresses itself in terms   
27:03   
of hierarchical structure of information processing. And then these graphs show   
27:09   
like the degree distribution of that network. So you can see kind of how it's   
27:16   
uh how this network is organized by doing these plots. So the plots are just basically the degree versus the number   
27:23   
of nodes. And then you can see these long tails emerge here. Long tails are usually associated with   
27:31   
chaos theory sometimes. Yeah. Yeah. If you have really long tails, uh networks tend to   
27:39   
be or or systems tend to be chaotic. Yeah. Okay.   
27:44   
So, yeah, which is true of a living system. So, yeah.   
27:51   
So, yeah. And then of course they do some uh statistical tests, the KO KOGRov   
27:57   
Smeirnov test, which is a good test for um looking at longtails   
28:03   
and how those are distributed. Okay. Yeah. So, this is a good paper. Um   
28:08   
again like I said this is this group they've been doing a number of papers on   
28:14   
looking at information processing in the cellular systems and then comparing it   
28:19   
to like or making the analogy anyways to neuroiming. So the neuroiming analogy is   
28:26   
like we can build these brain networks and that's information processing and we can say things like you know you could   
28:33   
use the analogy of fMRI because it's easy to understand. Um and then you know   
28:39   
so the same thing probably exists in tissues as like these you know you can   
28:44   
sample them as voxels or cells or whatever and those nodes are connected   
28:49   
in the same ways the same principles hold you have functional connectivity   
28:55   
effective connectivity and you can measure them in in the same way using statistics and information   
29:02   
theory. Okay. Yeah.   
29:07   
And I don't my statistics is pretty dismal.   
29:13   
Anyways, yeah, I I got myself a a primer on statistics because it's a weak weak   
29:20   
point in my math. Yeah. So, so I don't know if I if I get to do   
29:27   
that, but I have I have good Cole's notes on it or Well, I don't know. I can   
29:35   
show you it. It's Anyway, it's intermediate stats   
29:41   
and uh the professor I'm working with on optical coherence tomography   
29:47   
uses statistics and all his his equations. I swear. Yeah.   
29:53   
Signal analysis. Yeah. So, he's throwing statistics out at me   
29:59   
and I'm going, "Okay, then more math. I don't understand it, but I just go   
30:07   
with the flow. Yeah, it's bad. Yeah. No, I've had things   
30:15   
thrown at me like that throughout my engineering um career and and education and I have   
30:24   
no clue what they're they're talking about. And usually what happens is I take the   
30:29   
course twice and then I then I can answer the questions on an exam and I still don't understand the background.   
30:37   
It's interesting. Yeah.   
30:43   
Yeah. It's just Yeah. I don't like it. I I do like to know the theory and then   
30:50   
how that's applied. Like it gives me a deeper understanding of things. But in   
30:56   
engineering, they just say know this plunk. It's from somewhere.   
31:03   
If you had time to look it up, which you usually don't. Yeah.   
31:09   
Yeah. Oh, well, I could try to find the other paper. I've um I've got one here, but it   
31:18   
looks like an older paper. Oh, yeah. Which one is that?   
31:23   
Yeah. And here's one. Organization of embryionic morphogenesis via mechanical   
31:30   
information. Okay. Does that sound interesting? Yeah. Yeah.   
31:36   
It's 2019. Oh, yeah. That's the that's that the second one I put there.   
31:45   
Oh, this one. Okay. Yeah. Let me get pulled.   
31:51   
Oh, this is a a zebra fish. zebra fish tails are are they're the   
31:57   
ones that the proven loose cells and I think that it elongates partly   
32:04   
because of the cells are um not as packed in the tail.   
32:11   
Okay. Yeah. So this is the paper here   
32:17   
organization of embryionic morphagenesis by mechanical information.   
32:22   
This is uh so the highlights here positive feedback   
32:29   
between BMP signaling and EVE1 functions in the tail organizer.   
32:36   
Okay. I don't know what Eve one is. Oh, it's just Yeah, it's some signaling molecule. So they set up this signaling   
32:42   
cascade perturbation of organizer signaling is long range effects on cell motion. So   
32:49   
you have this organizer signaling you if you perturb it you can affect the   
32:55   
motility of cells. These long range effects are beyond the range of organizer signaling and these long range   
33:03   
effects are mediated by mechanical information. So I think this maybe this figure shows   
33:09   
kind of what they're talking about. um where you have the anterior and   
33:14   
posterior end. You have the uh posterior neural tube uh   
33:21   
in purple and then the dorsal medial zone here. And this is where you have   
33:27   
this coordinated and directed cell movement down, you know, axial movement   
33:32   
down from anterior to posterior. And then there's a swirl there. And I   
33:37   
think that's the eye, right? Yeah. Yeah. Well, This is the tail. No,   
33:43   
this is the tail organizer. This is where it's a plate of some sort. Is that   
33:50   
Yeah. Yeah, I think so. So, it's like the cells are moving down this way to the posterior end. There's a tail   
33:57   
organizer at the tail. There's no tail yet, but it's going to form the tail.   
34:02   
It's like, you know, kind of the setting up the signaling uh coordinate system for the tail. It   
34:08   
says here tail should be. All right. It's Is that swirl an actual movement   
34:16   
then? Um I I think it's just like showing that it's moving around that space.   
34:22   
I don't know why they put a Yeah, I don't think it's   
34:27   
an attractor or an organizer. They're called Yeah. Yeah. I guess you could say it's   
34:32   
an attractor. Not necessarily in the mathematical sense, but Yeah. Um, and so   
34:38   
you have anterior to posterior movement. The cells are coming in as raw material for the tail. The organizer is telling   
34:45   
it where to go. Um, there's this rapid less coordinated cell movement. So, it's   
34:51   
not moving up and down the uh down the axis, but it's kind of sticking around   
34:56   
the tail area and then it's thickening out the tail. So, it's yeah, it slows down the   
35:02   
migration. So, you can see where you have this system that if you perturb it, it will   
35:10   
affect the movement of cells. It will kind of extinguish the potential to form   
35:15   
a tail. And then of course, when you have all these cells down here being organized in the tail, there are all   
35:20   
these mechanical interactions that then accentuate the formation of the tail at   
35:26   
the posterior end. Okay, that that's interesting. Yeah. And so these images are showing   
35:33   
the organizer. I guess this is at the posterior end. So you have uh BMP4 and   
35:39   
Eve one. They're kind of colllocated as you can see here. Uh they break them   
35:45   
out here, but these are where they they're together. And then you have DMSO which is the perturbation DMH1 showing   
35:53   
that you know when you um put that treatment there you can obliterate   
35:58   
the expression of these signaling molecules and it extinguishes the   
36:03   
organizer. So that's that image. Um   
36:08   
going back to the paper, the tail organizer is located in the posterior   
36:14   
tailbud and both the misoderm and epidermis express the secreted signaling proteins BMP4 and BMP2B   
36:22   
and the transcription factor E1. And so this BMP signaling at the tail organizer   
36:28   
is required for the zebra fish body elongation which you'll see formation of   
36:34   
the tail. Um and and so then there are these different BMP inhibitors that occur as   
36:42   
well. Um these are just kind of you know restricting the way that because you   
36:47   
express BMP you know the signal kind of covers a wide region and then you have   
36:55   
an inhibition in places where there shouldn't be a tail. So if you uh got   
37:00   
rid of the BMP inhibitors, the tail would not be as uh wellformed, let's   
37:06   
just say. So you have these BMP inhibitors cordon and noggin which are   
37:12   
expressed in the posterior nodicord and PSM. Thus BMP signaling is restricted to   
37:17   
the posterior tailbud as indicated by phosphorolated SMAD. Um and they do they   
37:25   
kind of show the serum localization. Um they also have they show even skip   
37:31   
genes or Eve. This is the transcription factor Eve. It's one of these even skip genes that you see in Drosophila as   
37:39   
well. Um these are adjacent to the hawk 13 end of the hawk clusters suggesting a   
37:45   
role in posterior animal development. So this is the EVE and the BMP signaling   
37:51   
working together. So one is involved in sort of setting up uh like you know   
37:59   
morphagenesis of the tail and it's a morphagen and then even skipped is some   
38:04   
sort of hawk related gene that has a role in segmentation.   
38:10   
Um yeah this this looks like a really good um combination mechanical and chemical   
38:18   
system at work. Are you Yeah. Yeah, I think so. Um   
38:23   
I'm not sure where well the mechanical work I wonder what they do in terms of   
38:28   
mechanics. Uh let's take a look at this figure. So this just shows   
38:34   
you know some of the changes in some of these biomolecular uh factors and how they assay them. So   
38:42   
they're looking at the expression of different things at the tailbud looking at that. And this just shows   
38:49   
like the treatment the different treatments here. So they have uh this   
38:54   
administration of DMH1 and DMSO and then Eve one. So you see that uh   
39:01   
it's a transgenic Eve one. So I think they're overexpressing Eve one here. Uh   
39:07   
and then this is a vehicle I guess DMSO and then DMH1 is the perturbation. So   
39:13   
you can see in when you have the transgenic EV1 plus the DMH1, you get   
39:19   
this truncated looking tail and you see actually the body's all scrunched up.   
39:26   
When you have the transgenic EV1, you see like I don't know, it's kind of looks like it's maybe a little bit   
39:32   
truncated, but it still looks like a zebra fish embryo. DMSO has a zebra fish   
39:38   
embryo in full form. And then the perturbation DMH1 alone without the trans gene. It's just kind of like stick   
39:46   
the tail kind of is mis shapen if you can see that. Really?   
39:51   
Yeah. That scrunched one. I have an axelottle. It's kind of got a scrunched body like that.   
39:57   
Oh yeah. I don't know.   
40:03   
[Music] Anyways,   
40:08   
yeah. I don't know if she can lay any eggs or not then. Yeah. Well, I mean, yeah, I don't know   
40:15   
what the consequences of uh mis misled,   
40:20   
misdirected morphagenesis are. I mean, usually it's in that region, but sometimes there are other effects as   
40:26   
well. So, yeah. All right. So, that's uh that   
40:34   
figure. Um, yeah. And then so they talk about cell motion. Cell motion in the   
40:41   
tail organizer is aberrant in both DMH1 and transgene EV1 plus DMH1 embryos. So   
40:48   
whenever it's being perturbed by DMH1 in the wild type and in the trans gene cell   
40:54   
track straightness is reduced in both conditions and the mean coefficient of variation of cell track speed is   
41:01   
increased in DA DMH1 embryos. So there these differences in terms of how cells   
41:08   
are moving around. Um and so uh the directional velocity is   
41:15   
reduced in the uh tail organizer of DMH1 embryos.   
41:21   
Cell flux through the tail organizer include includes a medial domain comprised of both a dorsal to vententral   
41:28   
flow and a medial to lateral flow that are segregated from flows on the lateral   
41:33   
periphery that are predominantly posterior to anterior and vententral to   
41:38   
dorsal. So these are flows that enable cell migration and they're slowed down   
41:45   
in these uh perturbed phenotypes. Um   
41:50   
these flows intermex after perturbation of tail organizer signaling and the disruption of the dorsal leventor flow   
41:58   
likely obstructs cell flow into and through the tail organizer particularly   
42:03   
in the trans gene transg gene e1 plus dmh1 embryos. So when you get this sort   
42:09   
of uh trans gene plus the the perturbation then that's really when you   
42:14   
disrupt the mechanical flow of the system and you get this sort of phenotype that we see um here where it's   
42:23   
really kind of pronounced. It's because this flow is restricted coming down   
42:29   
down the ax the anterior or posterior axis. You were going to say something.   
42:37   
No, I was just looking at it. The most normal one is the uh top left, but   
42:44   
anyways, it's okay. Yeah, here. Yeah, that the first one there is is the   
42:50   
most normal looking. Yeah. So, this is um   
42:59   
this is G. Um this just shows like the different I guess these are flow lines   
43:06   
or something. Let's see what the uh legend says.   
43:12   
Full caption. All right, we're looking for G. Cell flow within the tail uh tail   
43:18   
organizer visualized by displaying only cell tracks with the greatest displacement from dorsal to vententral   
43:25   
green, medial to lateral yellow, posterior to anterior red, and   
43:30   
vententral to dorsal blue. So if we look at this figure here, we see that they're like looking at the cell tracks, but   
43:37   
they're looking at their sort of displacement in anatomical space. So if   
43:42   
you're, you know, going in different directions, there's a great dis greater displacement. So in other words, the   
43:49   
displacement I guess well the DMSO is the vehicle. So we can look at that and   
43:54   
then compare with the other ones. So we can see that most of them are green and yellow sort of here. Then the peripheral   
44:02   
regions you have blue and red and we said that those are um blue is   
44:08   
vententral to dorsal red is posterior to anterior dorsal to vententral is green   
44:14   
and then medial to lateral is yellow. So that's what we have for DMSO. And then   
44:20   
if we look at uh DMH1 we have it doesn't look like that. that looks the shape is   
44:27   
different of the cell tracks, but then you have more red and more blue and you have less green and more apparent yellow   
44:35   
at least from the surface. Uh with the the uh trans gene Eve one plus DMSO, you   
44:43   
see a difference here as well. um it doesn't look so much like   
44:50   
um you know it doesn't look so much like just the DMSO but you get this difference in the   
44:56   
tracks and their distribution and then where it's really pronounced is in the trans GV1 plus DMH1 where it doesn't you   
45:05   
know it's kind of looks disorganized and there were a lot more red tracks and blue tracks um and then you know you you   
45:12   
see some green tracks but they're not really predominant like there. Yeah, that that's messed up. The last   
45:17   
one. Yeah, that's really messed up, I guess, is the way to describe it. So, yeah, you   
45:23   
can definitely see from these cell tracks the differences in the phenotype.   
45:29   
Okay. Well, I don't know where my other paper went.   
45:34   
Um, oh well, it's a long one and it is   
45:40   
um more in line with uh my pen saggy   
45:47   
um mechanics. So, okay. I I I need to do um I need to read it   
45:54   
thoroughly. Yeah, I'm supposed to be writing a paper using it.   
46:00   
Okay. Does this have like is this mentioned tense already directly or is it just kind of like relevant?   
46:06   
Yeah, I think so. And now now I really want to look for it. Yeah,   
46:11   
of course I do. Um Oh, there it is. Okay.   
46:18   
I can get to the bottom of the pile and uh the kids book. Okay. Um, yeah. It   
46:26   
says 10 seg 10 segity structures and datadriven analysis for 3D cell   
46:32   
mechanics. Wow. Yeah. Yeah. So that's right on like that's   
46:39   
right on the money. And it says it's to be published in 2026. So this is a   
46:45   
preprint, I guess. Okay. Yeah. Yeah. It wouldn't do double-sided. It was a   
46:52   
preprint.   
46:57   
Anyway, yeah. So it's just uh they're making um cells   
47:05   
uh 3D cells uh with the tensity structures um like a   
47:11   
docahedron type structure and then linking them together which is not what I did but um this is   
47:21   
it's nice that they were able to do that because linking 10 segated structures as   
47:28   
I've discovered is sometimes impossible. They don't stick together   
47:36   
or they crash. I think they change change um configuration too much for the   
47:43   
finite element programs. All right. This Yeah, this is a computer methods and applied mechanics and   
47:50   
engineering. So, this is in January 2026. Apparently, they're already there   
47:55   
um in the future. Um this is computer methods and applied mechanics and engineering. So uh this paper uh kind of   
48:04   
goes over different tensity structures and then they're analyzing um this and   
48:10   
they're looking at 3D cell mechanics. So they talked about the cytokeleton playing an important role in many cell   
48:16   
functions. Given the similarities between the mechanical behavior of tensegrity   
48:22   
structures and the cytokeleton, many studies have proposed different tense based models for simulating cell   
48:29   
mechanics. Uh so then of course they talked about the low symmetry of most tensity units and how this has hindered   
48:37   
the analysis of realistic 3D structures. Uh so you know they're talking about   
48:43   
taking the tensegrity units that we've talked about in the meetings past and   
48:48   
how you can combine those because obviously you need to combine them to get this um you know to make this useful   
48:56   
to the tissues. Um as a result tense based modeling and cell mechanics has   
49:02   
been mainly focused on single cells or monolayers. So in this paper we propose   
49:07   
a 3D tensegrity model based on the finite element uh method for simulating   
49:13   
3D cell mechanics. We show that that proposed model not only captures a nonlinearity of a single   
49:20   
cell and an indentation test and a monollayer and stretch test but also the   
49:26   
non-uniform stress distribution and multisellular spheroids upon non-uniform   
49:32   
pre-stress design. Furthermore, we introduce a multiscale datadriven framework for cellular mechanics to   
49:40   
optimize the computation is paving the way for modeling the mechanobiology   
49:46   
of large cellular assembly such as organs. And and this is interior cells. It's not   
49:52   
epithelial tissue or endothelial tissue. It's the   
49:57   
interior cells. Okay. Yeah. Which is nice. Yeah. This is Yeah.   
50:03   
So yeah, they talk about uh how the   
50:08   
cytokeleton senses and transmits forces to interact with the adjacent cells and   
50:14   
micro environment. This orchestrates cell shape changes, polarity and motility. An equilibrium in forces is   
50:22   
maintained at the cellular level balancing internal mechanical forces   
50:27   
exerted by the contractile actin filaments resisting microtubules and compression and external forces from the   
50:34   
extracellular matrix. Um this state of pre-existing stress in the CSK is   
50:41   
referred to as pre-stress. uh multiple studies have reported the central role of pre-stress in cellular   
50:48   
or mechanuction and mechanobiology. Okay. And then uh they get at sort of   
50:55   
the traditional computational efforts sort of you know continuum mechanics   
51:01   
that are used describing the cell as an elastic a viscous or visel elastic   
51:06   
medium. This is assuming the cell is composed of materials with certain continuum   
51:12   
material properties. So you know you have to assume that the cell isn't just a sphere but like has these different   
51:19   
properties all acting kind of at the same time. So uh you have in say like a   
51:26   
typical cortical shell liquid core model where you have the outside which is the shell and then the liquid core in the   
51:32   
middle. We assume single or several layers of cortex with surface tension   
51:38   
with the cell interior modeled as Newtonian fluid with certain viscosity.   
51:43   
So you have this sort of uh composite model where you know different parts of   
51:50   
the cell have different properties. You have this uh certain behavior of Newtonian fluid with viscosity.   
51:58   
Other continuum models simulate the whole cell as a homogeneous solid. So some some models just kind of you know   
52:06   
treat the whole cell as a single entity and you know that's that's how they kind of work on that. Some such continuum   
52:13   
models are good approximations for a range of experimental observations.   
52:18   
However, as experimental conditions become complex constitutive models require additional parameters that are   
52:25   
essentially empirical and often lack in physical meaning. So you know if you use a homogeneous model it's basically just   
52:33   
kind of like you know you might be able to map it to a single measurement but once you start measuring more parameters   
52:39   
in the cell then those become less useful especially when you consider   
52:46   
complex physical interactions. So we can't, you know, it's hard to consider subcellular processes such as   
52:54   
protein level activities. Um, and and to get around that, sometimes people will   
52:59   
use averaged values. So you know, you can use a meanfield model or an averaged   
53:05   
uh model of things like protein level or other types of parameters. But the   
53:11   
problem then again is this aspect of what individual cells are doing. And so   
53:19   
if you're looking at a cell sheet where there bunch of physical interactions, each cell is going to have specific   
53:26   
values for those things and you can't average them together necessarily. you're smearing out a bunch of important   
53:33   
information about the mechanics of the of the cells in that structure and   
53:39   
you're smearing out the sort of the property the biological properties that are interesting.   
53:45   
And so then they talk about these individual cell-based approaches where we have cells treated as individual   
53:51   
entities. Um they talk about lattice based and off lattice models.   
53:58   
um the former which are lattice-based track cells along rigid grids. So you   
54:03   
might have like a fixed lattice cellular automa or a cellular pots model which they use in um copy cell 3D. So that's   
54:13   
the lattice based model and then the off lattice based model is where the they   
54:19   
they don't have a reference grid and so cells are just basically tracked as   
54:24   
centers of mass. So this is like the the uh point clouds that we use in cell   
54:31   
tracking data. You know we have we track the nuclei, we track the centrid and we   
54:38   
don't worry about a reference grid. We just maybe have a coordinate system and they could those those centers of mass   
54:46   
can interact in different ways and we can measure them. The benefit of course of having a rigid grid is that you can   
54:54   
define sort of the relative position more clearly. Um so there there pros and   
55:00   
cons to those. Um and then of course we have vertex dynamics models where   
55:06   
additional biochemical rules can be employed to study more advanced cell behaviors such as cell rearrangements   
55:13   
and oscillations. Um and so they kind of go through this review. Um and then they talk about the   
55:20   
tensgrity principle uh which is observed in various natural systems across scales   
55:26   
from the kinematics of human and animal locomotion down to the molecular architecture of spider silk fibers. And   
55:35   
so within the um CSK which is the cytokeleton the force balance between   
55:41   
contracttoactin filaments and compression supporting structures such as microtubules is reminiscent of the   
55:48   
self-equilibrium of tensgrity structures. So they're talking about like uh mechanical tenseity structures   
55:56   
uh such as geo geodeic domes and things like that. And so they're, you know,   
56:02   
there's this idea of opposing forces balancing each other out. And so that's what they're kind of trying to get at   
56:08   
here. So they're talking about pre-stress and how cell stiffness in individual   
56:15   
cells can scale with that pre-stress in accordance with prediction from tensegrity models. These factors are   
56:22   
under tense a conceptually simple or relevant idea for simulating cell mechanics.   
56:28   
And then they t It's not not easy to simulate it.   
56:33   
Yeah. Yeah. No. Um yeah, it's it's going to be hard because you have all these different   
56:39   
factors that go into it. Um and they're important for I take it for determining   
56:46   
things like pre-stresses and like the opposing balances of forces, things like that.   
56:51   
Well, and and it's um non what is it? Non determinant because   
56:59   
It's got a basically a chaotic structure which is um a double pendulum like if   
57:07   
you have two two rods or two compression elements together then um and it's like   
57:15   
it works like a pendulum and you don't know whether it's going to go this way or that way because you're you're   
57:21   
swinging this one and this one also swings. So it tends to be a chaotic   
57:28   
system and it's attached to the elastic members or the string or or   
57:36   
tension um members and the so the thing is indeterminate because it's   
57:44   
um got this chaotic system. um if you've got more than one um   
57:53   
compression element attached to the strings.   
57:58   
So it's um yeah, so it doesn't doesn't work with finite element analysis all   
58:04   
that well because it's basically indeterminate. like it can go one way or or the other   
58:11   
and you you don't know which which it's going to do really um until you put that   
58:18   
last little minute force on it in the finite element analysis you put tiny   
58:25   
little forces on your model and then all of a sudden in a tenseity it will change   
58:32   
shape boom   
58:38   
so small forces does kind of result in a configurational change.   
58:44   
Yes. So if we go back to the paper um they   
58:49   
propose that a 3D multisellular tense segrity model capable of simulating   
58:54   
cells in one two and three dimensions uh uh or what you know is kind of the   
59:01   
answer to a lot of the these modeling problems and investigate the response of   
59:06   
such structure under different loading conditions. Um they base their tenseity modeling   
59:12   
framework on a finite element method in finite or large deformation settings and   
59:18   
allows controlling initial pre-stresses. Uh as with any finite element model the   
59:24   
computational cost becomes exceedingly expensive as the number of elements increases.   
59:30   
um there to reduce computational costs we further perform simulations using a   
59:36   
multiscale data-driven approach in concert with homogenization techniques.   
59:41   
So this is where they're using averages and not accounting for like every single   
59:47   
nuance of the cell and they're just using that as a but they're using this as a way to sort of add data to this   
59:55   
model so that they don't have to simulate everything uh from first principles. So, you know, there are the   
1:00:02   
kind of decisions you're making there or to sort of balance computational costs   
1:00:07   
with realism. And that's, you know, it's just kind of where they are with this data-driven   
1:00:13   
computing enables calculations directly from a material data set while satisfying pertinent constraints and   
1:00:20   
conservation laws. This bypassing the imperial empirical material modeling   
1:00:25   
step and the conventional finite element model.   
1:00:30   
Uh yeah. So they kind of go through the tensity formulation. They simulate a   
1:00:37   
single cell and a monollayer then a multisellular spheroid. So they kind of   
1:00:42   
build up to the multisellular case uh with geometry. So you have like kind of   
1:00:49   
the single cell which is the point, the monoware which is this you know one-dimensional or two dimensional   
1:00:55   
structure and then multisellular which is the threedimensional structure. the spheroid and then this is compared with   
1:01:02   
experimental work. They kind of go through the 3D monollayer simulations   
1:01:09   
um and then the solutions are then benchmarked against a tensgrity based   
1:01:14   
direct numerical simulations. So they kind of go through their model of tensegrity   
1:01:21   
where they com it's composed of a pin connected tensile and compressive set of   
1:01:26   
elements. They adopt the modeling framework of ma at all which is this reference here and   
1:01:34   
they just summarize it below. They have they use cartisian coordinates to   
1:01:40   
characterize the node. it's xyz and then you have uh this   
1:01:45   
matrix uh and then you have your node number which you can identify single   
1:01:51   
nodes in this global nodal coordinate matrix which you construct here as large   
1:01:57   
n and then in a single vector formulation the nodal coordinate matrix reads small   
1:02:04   
n and then the elements here. So you can build a uh coordinate matrix as a single   
1:02:11   
vector. The connectivity matrix then represents the topology of the entire tense structure which can uh consists of   
1:02:20   
m subb compressive bars and n subs tensile strings. So you have these compressive bars and tensile strings as   
1:02:26   
the edges of the network. The nodes of the network then are these uh nodes that   
1:02:32   
they define up here in this matrix large n and this vector small n. So then we   
1:02:38   
have this um this conditional here this stack where you have um   
1:02:46   
uh basically the uh make you have like the compressive bars and the tensile   
1:02:53   
strings and that's combined into this M here which is its uh corresponding kth   
1:03:01   
column of entry CL. So you have uh this   
1:03:07   
uh conditional where you have one negative 1 and zero. So basically you're   
1:03:12   
classifying something uh where the uh L value is either I or J   
1:03:20   
otherwise it's zero. So this kind of talk you know kind of   
1:03:26   
gives us a way of classifying these different conditions.   
1:03:31   
Um and then each element has a length which is here. The length is either I or   
1:03:38   
J calculated here. Accordingly the global structure length vector is assembled here. We define the global   
1:03:45   
area vector and modulus vector given by capital A and capital E. In each element   
1:03:52   
K the axial stress increment which is here is given in terms of the axial   
1:03:58   
strain increment which is here. Um the general tense integrity framework is   
1:04:04   
equipped to handle elastic or plastic materials. In this study, we assume the   
1:04:09   
members are elastic and have constant cross-sections and moduli focusing on   
1:04:14   
structural behavior under moderate strains without introducing additional complexity from nonlinear material   
1:04:20   
behavior. So we have this internal force vector here where uh L0 is the rest length   
1:04:29   
vector before the application of pre-stress. So you have this rest length vector and then you're adding   
1:04:35   
pre-stress. you're adjusting L0 and L which can achieve different levels of   
1:04:40   
pre-stress in the structure. Okay. And then you have force density vector Q   
1:04:45   
which reads here reflecting the geometric nonlinearity. They're the current length vector L.   
1:04:53   
Okay. So then they go through some more um they go through statics and then um   
1:04:59   
they have an identity matrix um and that's supposed to give you this   
1:05:04   
structure at equilibrium and then you have since k is dependent on nodal coordinates n the left hand side of   
1:05:11   
equation 13 which is here is expanded using the chain rule and then from   
1:05:16   
equation 10 which is up here which is the force density vector Here   
1:05:23   
we can show that the partial derivative of Q with respect to N is given by this equation which is now we're getting into   
1:05:30   
differential equations where this AEQ is this value. Yes. Linear algebra on top of um   
1:05:38   
advanced calculus. Yeah. Yeah. So we're getting deep into linear algebra then moving to calculus. So we   
1:05:45   
can throw throw it into calculus. Yeah. So we can characterize the dynamics, right?   
1:05:53   
Yeah, I think yeah, maybe that's the Well, I mean it makes sense, right? Like   
1:05:58   
because you're working out the network structure and then you have to simulate the dynamics.   
1:06:03   
Yes, it it makes sense that they're doing this. It's just um I think you lost me a couple of lines   
1:06:12   
back, but yeah, it's okay. I wish I could get a complete course on   
1:06:18   
this stuff. Yeah, somewhere. Yeah, it's it's it's a lot though that you're modeling because   
1:06:23   
you're kind of going through and you're saying, "Okay, here's the structure. We have all these different components. Uh   
1:06:29   
we have to balance them out. Then we also have to have like we can do the uh   
1:06:35   
2D case and the 3D case and then we we'll just kind of do all that. Then do   
1:06:40   
the dynamics and then you know it's it's uh yeah, you're tracking a lot of things   
1:06:46   
through here." Well, anyways, uh so then they go into sort of getting to the states of selfstress and the sort of the   
1:06:55   
structure, you know, structural loads and things like that. And then um   
1:07:00   
basically uh you have to sort of introduce the geometry then um and   
1:07:07   
that's here and then there's this 3D tessellation that they talk about which is where uh tenseity structures remain   
1:07:14   
in self-equilibrium which requires special arrangements of the elements and so now we have to sort   
1:07:21   
of think about form finding of eligible self-stabilizing tense configurations.   
1:07:26   
So this is where we have to find the eligible configurations that uh can be determined as stable. So   
1:07:33   
like you can you know it's like kind of like protein folding where you have all these configurations this huge   
1:07:39   
configurational space. Some proteins uh are you know functional because of their   
1:07:45   
configuration and some are not. And so such it is with like tensegrity   
1:07:50   
structures if the structure sometimes it's not viable and sometimes it's   
1:07:57   
viable and stable and that's what we want to find. So you can imagine this is a big computational problem because you   
1:08:03   
can imagine with protein folding it's huge uh in terms of like simulating just a little bit of the protein folding   
1:08:10   
process. I mean you know they this has been the problem for years. they've had to kind of simulate just a little bit of   
1:08:17   
protein folding and that's taken up a lot of computational power because there's so many configurations that it   
1:08:23   
can take and that they have to be evaluated. So, you know, I imagine with   
1:08:28   
these tengrity structures, you can't get a very, you know, long sequence of   
1:08:34   
dynamics um to really kind of evaluate. So this   
1:08:39   
is really going to be just kind of like I don't know what their time scale is here but it's going to be hard. It's   
1:08:46   
going to be a hard computational problem. So this is yeah this is the image here where they talk about um see   
1:08:53   
if I can open this up they show this these structures. So you   
1:09:02   
have these networks where you have the compressive elements in black, the tensile elements in red, and then these   
1:09:09   
geometric shapes within that structure. And then if you look at this other, you can look at this high-res image, you   
1:09:15   
have sort of the symmetries. So you have the original structure, which is where you have the sort of the top and the   
1:09:23   
bottom and the left and the right defined. And um and then you have the mirrored   
1:09:28   
structure. So you can look at the mirror image and then this shows the the symmetries.   
1:09:34   
So you have the original you have a mirrored symmetry on the Y axis, a   
1:09:41   
mirrored symmetry on the X axis and then on the Z axis you have like basically rotating. So you have these mirror   
1:09:48   
images. Yeah. I'd like to know why they have um   
1:09:55   
solid filled in parts or why they went that way.   
1:10:02   
I don't know. So, you're talking about the interstitial like the yellow and the   
1:10:08   
gray and the gray. Yeah. Yeah. I don't know. Yeah. Why that? Why did you do that?   
1:10:14   
I think it's probably to demonstrate kind of like the stable surfaces or like   
1:10:20   
the surfaces on network but also show the elements underneath.   
1:10:25   
Oh, maybe so you can see it. Yeah. Yeah. I mean like illustrating like if you just said that this forms   
1:10:32   
like a shape and you'd say, "Well, there are a bunch of lines there." Yeah. Yeah. It looks like a bunch of   
1:10:38   
lines. That's what my thesis was about. Lines all over the place.   
1:10:44   
Yeah. And it was sometimes hard to keep them straight.   
1:10:50   
Yeah. Yeah. So then this is   
1:10:55   
how you know so this is like the the neighborhood of connections. So if we think about this as   
1:11:01   
they don't have things colored in they they just have the structure. Okay.   
1:11:06   
Right. All right. Yeah. And then this is like kind of the   
1:11:12   
more typical tense you'd see with like um you know where you have the different   
1:11:19   
like basically I guess these are cells that are next to one another.   
1:11:24   
Yeah, but you notice there's nodes there with um uh what is it? Six black lines   
1:11:30   
coming out of it. Um I don't know if you can you can't see, but um there's more   
1:11:37   
than one black line meeting. Okay. Yeah. Um yeah.   
1:11:42   
So that that means it's um   
1:11:48   
non-determinate 10segrity structure. Yeah. Okay. Anyways,   
1:11:54   
so it's like when you have multiple lines.   
1:11:59   
Yeah. Yeah. When you have multiple bars, which are the compression elements,   
1:12:05   
then it's indeterminate. Okay. It has been labeled indeterminate.   
1:12:13   
Yeah. because you have multiple bars and it acts like um like I   
1:12:23   
said a a multiple pendulum situation. Oh, okay.   
1:12:28   
Yeah. And I can show you the chart. This wasn't invented by me. This was a G.   
1:12:34   
It's in Gan's book, which is hard to follow, but um and misleading. Oh, you   
1:12:40   
just do this. Yeah. Right. Yeah.   
1:12:47   
Yeah. Okay. Yeah. So that's that's the paper.   
1:12:53   
Um yeah, thanks for showing us these papers. Uh very interesting to follow   
1:12:58   
through from like networks to then tense. Well, a tense segrity is a network.   
1:13:05   
Yeah. And yeah and it changes like um   
1:13:12   
during I don't know whatever a cell is doing but at a moment in time you'll   
1:13:18   
have a tense certain tense structure in a cell   
1:13:23   
it'll be stiffened in one direction and in tension in another direction   
1:13:29   
and the whole thing holds up. But um you can probably call it a 10segrity   
1:13:36   
structure. Yeah. But the analysis of of cells mechanical   
1:13:45   
networks um is in its infancy. I would say they haven't   
1:13:52   
there's not much out there on that. um and to check it for a tense segrity   
1:13:59   
structure to see if it is a tense segrity structure. Yeah. So I'm interested in mechanical   
1:14:06   
um network networks in cells. Yeah.   
1:14:12   
It's interested to see that because um   
1:14:19   
basically you have some parts in tension and some in compression and that forms a   
1:14:24   
tense integrity structure. So anyway yeah thanks for for doing this   
1:14:32   
with me. Oh no problem. Yeah. All right.   
1:14:38   
I just I have no one to talk to about this. Oh yeah. the mechanical engineer that I   
1:14:43   
live with uh here. Um he goes 10seg yuck and walks away.   
1:14:53   
Conventional mechanics fine. Yeah. But yeah, he he's actually a fairly good   
1:15:00   
design mechanical design engineer, but he doesn't like tense integrity structures.   
1:15:05   
No. No. Anyway,   
1:15:11   
great. Yeah. All right. Well, I think that's all for today. Um, yeah. Thanks for again for   
1:15:20   
the session. Okay. All right. All good. Have a good week.   
1:15:25   
All right. You too. Talk to you later. All right. Bye. Bye.   
1:15:31   
All right. So, uh, several meetings ago, uh, probably this summer, we talked   
1:15:38   
about this book, Laws of the Game by Manford Igen and Ruth Winkler. And this is a g this is a book that talks about   
1:15:45   
game theory in biochemistry and physics and it talks a little bit   
1:15:51   
about their application for on like cellular automa or in   
1:15:58   
computation. So, Manfred Ian of course was the pioneer of things like the   
1:16:03   
hypers cycle, but he also had this interesting game theory where um you   
1:16:09   
know you can play these games that mimic um things in in biochemistry or in   
1:16:16   
physics and they can have you know you can have these different strategies for playing them and all of that. And so in   
1:16:23   
this book, one of the types of games he talks about are these bead games where you have beads on um on lattises like we   
1:16:32   
talked about in the meeting, these lattice methods that people use for modeling and you can model different   
1:16:38   
types of scenarios on that kind of board. So um it's not quite like the   
1:16:44   
game go, but it's a similar principle. And so I think it was um Hussein who   
1:16:52   
mentioned that chess is a stupid way to think about game theory and maybe   
1:16:58   
you know these kind of bead games are more appropriate for things like biochemistry   
1:17:04   
which I tend to agree with because you know again you know chess is a specific type of strategy and these be games are   
1:17:12   
much more useful uh for things like biochemistry especially when you have   
1:17:17   
competition. and you have different types of processes,   
1:17:22   
dynamical processes you want to um understand and replicate.   
1:17:28   
So, the other reason I bring this up is because I found this website. Uh, this is David Ellerman and I guess he's some   
1:17:35   
sort of polymath. Um, he does a lot of stuff with like econ economics and um   
1:17:41   
other types of law applications. Um and you know he also does like   
1:17:47   
quantum mechanics and mathematics and he has this set of uh Excel spreadsheets   
1:17:54   
which are a little bit um you know a little bit archaic way to do this but   
1:18:00   
he's modeled some of the games and the laws of the game in these spreadsheets and I wanted to look at these   
1:18:06   
spreadsheets and uh go over some of the games that he's taken out of the book.   
1:18:13   
So the book itself is, you know, proposes these bead games. It proposes different types of   
1:18:19   
strategies for playing them. And then he's taken some of those things out of the book and run some simulations and   
1:18:26   
spreadsheets. So one of the things he talks about is the Aaronfest earn model. And this is   
1:18:32   
sort of there are a couple of versions of this. This is with the random initial state. So if you go to the book and you   
1:18:39   
look up Arinfest earn model, you'll find that then this is a screenshot of the uh   
1:18:45   
spreadsheet. So I have the spreadsheet in sort of Google Sheets now and uh but   
1:18:52   
this just shows the structure and you know how this is calculated. So   
1:18:57   
they're equations in these cells and you can have an input matrix here.   
1:19:04   
find the sums and then this just shows the distribution or the the ensemble that   
1:19:11   
you get. So the a couple notes on this the initial distributions of zeros and ones   
1:19:18   
is random. So basically you have this matrix of zeros and ones that come from these inputs on each axis and then you   
1:19:25   
have the sums down here and then you have zeros uh represented by an empty   
1:19:32   
cell and ones represented by these black boxes and say of this distribution of   
1:19:37   
black boxes as a sort of ensemble that is the result of this random initial condition.   
1:19:45   
So the initial distributions of zeros and ones is random and then you play the game and you get this you know uh mixing   
1:19:52   
that occurs. So you get this random distribution of zeros and ones as you can see uh each iteration in a cell in   
1:19:59   
the 16x6 array which is the size of our our grid here is randomly selected. The   
1:20:05   
entries can be zero or one. When a cell is selected, its entry flips to the other value.   
1:20:12   
Since the most populous value tends to be selected and flipped, it is negative feedback.   
1:20:17   
The equilibrium towards which it tends to is where you have 256 / 2 equals 128.   
1:20:26   
So in other words, like half of the cells will be zeros, half of the zeros will be one. And you know that should   
1:20:34   
happen over time. So what you have here is you have this initial distribution   
1:20:39   
which is random in nature. The the black squares are randomly distributed.   
1:20:47   
Then we randomly select cells and we flip them flip the values and we keep   
1:20:53   
doing this over time as we select cells. And then if you know what happens is   
1:21:01   
that the most populous values tend to get selected and flipped. So when the most populous values get flipped, it's   
1:21:07   
negative feedback which reinforces order. And so this is a a nice sort of   
1:21:14   
way to show how order can emerge from randomness. This is the Aaron fester where we have a uniform initial   
1:21:22   
condition or where we have all an all black initial state. And so again, we   
1:21:28   
have an initial state that's all black. And then we play the game and we uh get to the point here where this um this   
1:21:35   
array occurs. And so initially each cell is black each   
1:21:42   
ball in our so what we're doing with a random process what reason they call it an earn is because there's this whole   
1:21:49   
class of problems in statistics called earn problems. So you have different   
1:21:54   
earn models proposed by different people. um you know you can model different types of sampling in this way.   
1:22:02   
So you have this ern which is like a black box and you have a bunch of balls in the earn and the idea is you reach   
1:22:08   
into the urn and you pull out a ball. It's either black or white in this case zero or one and you don't see what   
1:22:16   
you're pulling out. So it's a blind process blind with respect to selection and you're pulling out a ball and you're   
1:22:22   
putting it on the table. So the idea is if you reach into the earn enough times and you pull out balls that the   
1:22:28   
distribution you'll get is random in nature and you should get a random distribution.   
1:22:35   
That's the idea. Now in this case it so in the last case that was a random   
1:22:41   
distrib random initial distribution. We're drawing balls out. We have an equal number of zeros and ones and we're   
1:22:47   
just drawing them out at random and placing them on the board. In this case, we have a uniform initial condition,   
1:22:54   
which is that every ball in that urn is black. And so when we reach out and we put something on the board, it's always   
1:23:00   
a black ball. And all of our all of our cells then have black balls in. And then   
1:23:06   
we once we populate our grid, then we evolve our grid. So each iteration on a   
1:23:12   
cell in the 16x6 array, so we have 256 spots, is randomly selected. So the   
1:23:19   
entries can be zero or one. So we start all with a uniform state and then we start to iterate on that by flipping the   
1:23:27   
values to u things that are not black balls. When a cell is selected its entry   
1:23:33   
flips to the other value. So we select something it flips and this happens over a number of iterations.   
1:23:40   
Since the most populous value tends to get selected flipped again it is negative feedback. And so then our   
1:23:46   
equilibrium tends over time to be where we have half of our grid being white balls and half being black balls. And we   
1:23:53   
end up in the same place that we do with the first example where I showed you   
1:23:58   
that it was a random initial condition. In this case, it's a uniform initial condition and because of the way the   
1:24:04   
game was played, we end up in the same place. Okay, this is the bead game which was from page 52 and laws of the game.   
1:24:12   
So this is selection. So this is where we have our um a   
1:24:17   
thousand iterations of this game. We have this lookup table where we have   
1:24:23   
this set of ones and threes. And so this is an 8 by8 matrix. And then   
1:24:30   
now we have our relative changes here for a thousand iterations. So in this game on each iteration a   
1:24:37   
square is randomly selected. The value is doubled and its content is then   
1:24:42   
assigned to the second randomly selected square. There are four possible contents 0 one   
1:24:48   
two and three. So what happens here is we have any one of these values can be   
1:24:55   
zero one two or three. In this case we have ones and threes only. And each of these are represented by you   
1:25:02   
know this is from a deck of cards where you have clubs, diamonds, hearts and spades. And then you know each state   
1:25:09   
from zero to three is represented by one of these symbols. So you can get a sense of what the um   
1:25:17   
what the contents are by that. So you have this ensemble with different distributions of these symbols.   
1:25:24   
On the first iteration the contents of the squares are randomly assigned. Eventually one suit will tend to   
1:25:30   
dominate. So the idea here is that we again have this case where we randomly distribute these symbols. We can use an   
1:25:37   
earn model to draw out, you know, we can have uh a number of these symbols uh   
1:25:44   
evenly distributed. We can place them on the board, but over time as this game progresses, you end up with one symbol   
1:25:52   
that predominates the board. And so in this case, I don't think that's the case in this in this iteration. But for later   
1:25:59   
iterations, say like for example, we look at this uh you know, we have all   
1:26:04   
four symbols evenly represented on the board, we keep playing the game as as we   
1:26:10   
see here on the rules. And then we end up over time with enough iterations   
1:26:15   
getting a single symbol that predominates. So you might have most of the board filled with hearts or all of   
1:26:22   
the board filled with hearts. Uh so eventually one seat will tend to   
1:26:28   
dominate. One can keep score by watching the relative changes relative to a initial random assignment. So you can   
1:26:35   
actually score this and see the uh dynamics of this change. Okay. So that's   
1:26:41   
in uh so that's the bead game in selection.   
1:26:55   
This is going back to the airfest model with cooperative phenomena. Instead of   
1:27:00   
what we showed before with an initial condition, this is where we have this   
1:27:05   
matrix um and we have 600 iterations.   
1:27:12   
We have a matrix of zeros and ones with the sum down here. how many ones you have in each uh row   
1:27:19   
uh or each column. We have our neighborhood sums here. And then we have   
1:27:24   
our board where we have red squares and blanks. And so that's what our board   
1:27:30   
looks like. Looks different than the other boards. Why is that? So let's look take a look   
1:27:35   
at the instructions. So the entries in the 16x6 array can be   
1:27:41   
zero blank or one red. these red squares. In each iteration, the entry in a cell flips to the other value with a   
1:27:48   
probability of proportional to the number of eight neighbors equal to the other value. So in this case, we have   
1:27:55   
these neighborhoods of eight cells surrounding an initial cell. So this   
1:28:00   
cell here is going to have eight neighbors. It's going to be every cell that surrounds   
1:28:06   
it. Six on the top, six on the bottom, and two on each side. That neighborhood   
1:28:12   
cells then will shift. So you know this cell will have a neighborhood of or   
1:28:18   
eight neighbors and this cell will have a neighborhood of eight neighbors and this cell will now be a neighbor of this   
1:28:25   
cell. So you can see the parallelism where you move across the board and each   
1:28:30   
cell can either be sort of in the middle of a neighborhood or in in multiple neighborhoods. And so that's what they   
1:28:38   
what they mean when they talk about neighbors. And so we also see up here where we talk   
1:28:45   
about our neighborhood sums, we have our neighborhoods defined in the sum within   
1:28:51   
that neighborhood. So what we're going to do is we're going to start with our 16x6 array. We can   
1:28:59   
have multiple initial starting points. We're going to flip the entries in a cell to some other value with a   
1:29:07   
probability proportion of the number of the eight neighbors equal to the other value. So when we have a majority of   
1:29:13   
cells that are blank, that central cell will then become blank. This is very similar to what we do in cellular automa   
1:29:20   
in neighborhoods in more neighborhoods. Um and so then the initial distribution   
1:29:27   
of zeros and ones and r is random. So we can have an initial distribution that's random. We can have this update rule and   
1:29:34   
we can end up with um you know this sort of order that   
1:29:39   
emerges. A higher stabiliz stabilizer value makes it harder to flip a cell's value. So we   
1:29:46   
can have stabilizer values that can change the dynamics of this game and   
1:29:51   
make it harder to actually flip cells because sometimes there's this tendency in uh you know for clusters of blank   
1:29:59   
areas to be reinforced by through this kind of a process. Sometimes it's easy for the whole   
1:30:06   
simulation to converge on one state or another through this process. But there's stabilizer values that we can   
1:30:12   
use as well. Um okay that's just for this   
1:30:19   
instructions. Okay this model is motivated by the description given on pages 71 to 72 in   
1:30:27   
laws of the game. So if you're interested in uh following up on this and playing with the spreadsheets   
1:30:34   
uh they have this set of IEN sheets. So this is kind of like for these different   
1:30:41   
uh parts of laws of the game. So we have uh board game bead game equilibrium.   
1:30:47   
This is where you have an all black starting state bead game equilibrium   
1:30:52   
where we have a random start. There is this bead game once and for all and I didn't talk about that yet.   
1:31:04   
And so the once and for all game is a variation on these games, the bead game,   
1:31:10   
uh but it has a a distinct strategy and that strategy is to try to have like   
1:31:16   
kind of a uh solution that is a winning strategy. So in other words, the   
1:31:21   
dynamics of a lot of the games in these uh so the dynamics of a lot of these   
1:31:28   
bead games tend to lead you to sort of some sort of equilibrium state. It isn't   
1:31:34   
necessarily where you have a game where you have all one state or all the other state. They tend to fluctuate uh with   
1:31:41   
different internal patterns. But in once and for all the goal of the game, the object of the game is to dominate   
1:31:47   
everything to finish off your opponent and to become the dominant state. And so   
1:31:55   
once and for all is sort of a way to model smallcale catastrophes. these kind   
1:32:01   
of takeovers of one state or another is a small-cale catastrophe. And what I mean by small-cale catastrophe is to go   
1:32:08   
to Renee Tom's cat uh catastrophe theory and think about how catastrophes   
1:32:14   
operate. In other words, you have these sort of largecale changes that occur   
1:32:20   
that reorder a system. And that's kind of what you want to do with this once and for all strategy.   
1:32:25   
And so the idea is that you have this uh basically this internal mechanism in the   
1:32:32   
game or these strategies that play upon fluctuations in the dynamics of the   
1:32:37   
game. So as there changes happening within our matrix uh we try to seize on those fluctuations   
1:32:44   
and amplify them. And so there's an internal mechanism   
1:32:49   
within this game strategy that's employed that responds to fluctuations and exaggerated manner. So that means   
1:32:57   
that you know in terms of responding to these uh fluctuations you you put a lot   
1:33:03   
of resources into sort of dominating the game. Um and so and so through these kinds of   
1:33:11   
strategies we can actually examine these games and look at the precursors of   
1:33:16   
homogeneous uh states. So, you know, if we see this in a system, if we see a catastrophe   
1:33:24   
like where we see an extinction of a species in a food web, we can actually go back and look at what the precursors   
1:33:31   
were. What were the sort of dynamics of competition that led up to this problem?   
1:33:37   
Or maybe there was an external perturbation. So, what was, you know, maybe that external perturbation didn't do the job   
1:33:44   
by itself. Maybe there were other dynamics within the competition structure of that food web that led to   
1:33:51   
that extinction event. And so Once and for all allows us to look at those kinds of uh dynamics.   
1:34:14   
And so once again, if you're interested in downloading these IGEN sheets, you have the bead game and you have   
1:34:20   
different versions of the bead game and then you have the Aaronfest model with cooperative effects and those all kind   
1:34:26   
of are point to different pages in the book. And then finally, if you're interested   
1:34:32   
in some of the other things that um he's done, um you have Kaufman sheets. So   
1:34:38   
he's also done this for the book at home in the universe by Stuart Kaufman. These are modeling the boolean networks and   
1:34:45   
fitness landscapes that Kaufman talks about in that book. He's also implemented simple genetic algorithms.   
1:34:52   
Uh and then you know the game of life, different iteration counters looking at   
1:34:58   
the glider gun within the game of life. He's done some epidemic modeling uh some   
1:35:04   
simple genetic algorithms here from John Holland's book hidden the hidden order.   
1:35:10   
And then of course polia's city. So I talked about polia's earn or I talked about earn models and polia's earn is a   
1:35:18   
version of an earn model where we do sampling with replacement. In other words, we put a bunch of balls in the in   
1:35:25   
the in the earn and then we sample balls out of the earn.   
1:35:31   
And as we sample balls and put them on the table, we get new balls in in its place. and it's according to the   
1:35:37   
original distribution of balls in the earn. So we're always drawing from a fixed   
1:35:44   
prob probability and so we can use these kinds of things in basian modeling where we have this uh   
1:35:53   
process where we're gaining information and we can adjust the probability of   
1:35:59   
that earn and we can replace uh the number of balls in that earn. So   
1:36:04   
we have a continuous sort of distribution and we can we can   
1:36:09   
model things fluidly like that. Um and so this just kind of goes through polioarn and talks about sampling with   
1:36:17   
replacement and how that works on a grid. So he he plays this out on a grid.   
1:36:24   
So polia city is a is is a circle or ring of neighborhoods. When a location   
1:36:29   
hits zero, it goes broke. Do the neighborhood affect the location that has gone broken again go positive if it   
1:36:37   
has a nonzero neighbor and then it is possible for the next broke neighbor to go positive and so forth. Okay. So this   
1:36:44   
is basically where you have these grids uh with sampling with replacement and   
1:36:49   
you get this structure across space and he calls it polio city. Very interesting stuff.   
