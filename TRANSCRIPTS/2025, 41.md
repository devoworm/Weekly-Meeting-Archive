## TRANSCRIPT
0:00
Hello. Hi. Hi. How are you?
0:06
All right. Thank you. Yeah. Yeah. We'll see. Um,
0:11
you know, the best way to do it is go back and look at the recordings from uh, you know, past and see what
0:19
we've been doing. But yeah, we also have our repositories that you can look
0:25
through and see if there I guess in the D.Learn learn organization. There's some
0:30
open rep uh open issues, so you can check those out.
0:37
Yeah, that's good. Okay. All right. So, that's good. Uh, welcome
0:44
to the meeting. I don't know, Morgan.
0:49
So, I think this is going to be the last meeting of the year. We had a pretty good year. We did a lot of uh different
0:57
types of activities uh meetings. We had a lot talked about a lot of different
1:03
topics. Of course, we had our summer of code students and they did
1:09
their projects and I you know we had a couple weeks where they presented on things. So uh we're working on a paper
1:18
for the DVOG. So that's coming along actually. That's Gyro Gyro and myself.
1:25
That's coming along pretty well. And then uh we're also of course working on
1:31
the uh paper with myself, Dick and Lewith
1:36
on the uh on the centipia.
1:42
And so that's those are the two that are kind of coming up. Um, so yeah, and then
1:47
we have uh P Susan and I've been working on the uh tensgrity stuff and
1:55
I don't think there's anything else outstanding. Anyways, uh I'm going to be presenting
2:00
this week at the open room general meeting. Uh it's held on Zoom every
2:07
year. Uh you know, usually it's an update from everyone on what's happening in in the group. um
2:16
over the past year. So, I'm looking forward to seeing what other people are doing. I I've got actually our uh annual
2:24
meeting posted on the YouTube channel already. So, you can take a look at that
2:29
and it's just a recap of what we've been doing the past year. Um but you know,
2:34
I'm gonna I'm gonna get some feedback there from different people open. You're usually pretty positive. So it's not
2:42
like uh but anyways uh there. Yeah. So I'll I'll I'll report back on that as well.
2:51
So that's good. Um other than that, we're kind of wrapping up the year. So if people have ideas for next year, you
2:58
know, we should get in touch and talk about what you'd like to do or what you'd like to see. This is uh a paper is
3:05
a bioarchchive paper on reconstructing Wington's
3:10
landscape from Denver. So this is um James Brisco, Eric Sia, Dylan Sislo, and
3:19
they're talking about Wington's landscape. So just as a recap, Wington's
3:25
landscape is this developmental landscape. Sometimes you can think of it as a tree where you can observe the you
3:35
know the trajectory of developmental events from the one cell fertilized egg
3:42
to a completed embryo. So every then and there's like a cell division or a
3:47
differentiation event and the tree structure forms these
3:53
canals or channels and they use the term channelization to represent that process
4:00
of channel making through that landscape. So it's it's kind of
4:06
you know you could call it a combinatorial tree if you'd like but it's on this landscape and it carves uh
4:13
something that landscape. So it's not the same thing as like a gradient that we usually use in physics
4:21
or we use in machine learning. It's a little bit different thing. In any case,
4:27
uh those channels then are sort of the way in which you make an account of
4:32
development. So you can look at the channels and their pattern. And that's important for things like buffering of
4:38
phenotypes and other types of things that happen in development across embryos and across
4:45
uh development. So they're going to but one of the things about Lington when he made when he kind of came up with this
4:52
concept was that it was largely um I guess ana an you know an analogy of like
5:00
a landscape with with channels and that was the idea. there wasn't a lot of
5:07
wasn't a lot of quantitative rigor to the landscape at all. So that's
5:13
where this p we this paper came in. Now we talked about a p we talked about a paper I think about two years ago maybe
5:20
now that was where they kind of did the same thing as what they're doing here.
5:25
They try to sort of quantify or quantitatively characterize the landscape with actual biological data,
5:34
which you might think, well, that's ridiculous. We should have done that in the first place. But, you know, a lot of
5:39
times theoretical concepts have to be sort of some sort of analogy, some sort of, you
5:47
know, sort of a diagram or some sort of thing that is generally applicable. And
5:52
then you go back and you verify it with that. So, and that's what they're doing here. They're reconstructing a Wington
6:00
landscape from death. So, again, they they mention that the
6:05
development of a zygote into a functional organism requires that the single progenitor cell gives rise to
6:13
numerous distinct cell types. Attempts to exhaustively tabulate the interactions within developmental
6:19
signaling networks that coordinate these hierarchical selfie transitions
6:25
difficult to interpret or fit to data. An alternative approach models cellular
6:30
decision making process as a flowing abstract landscape whose signal
6:35
dependent topography defines the possible development outcomes and transitions between them. This is they
6:43
call it a signal dependent topography. So depending on what's happening in the embryo, you get these
6:51
channels and then you get these branches in of in the channels where you can go
6:57
in one direction or another. Prior applications of this formalism have built landscapes in lowdimensional
7:03
spaces without explicit reference to gene expression. So this is where again
7:08
you know people have kind of thought of this as a lowdimensional space. They don't talk about gene expression. They
7:14
kind of talk about the embryo as as you observe the embryo you observe these branching events and that's the
7:20
description. What they want to do is is graph gene expression onto that explicitly like what are the gene
7:26
expression event or the changes in gene expression that happen that trigger
7:31
these uh branching events in terms of uh you know different channels forming and
7:38
branching off to different trajectories. So that's what they're they're going to do and it's of course if you know
7:43
anything about gene expression data you know it's going to be quite challenging because a lot of times we have to take
7:49
gene expression data and calibrate it to some sort of phenotypic event. So you
7:55
know all these papers we see with these stains that they do or these u fluorescent images where they try to
8:02
overlay things and show differences between that and control. Those are all attempts to sort of calibrate that to a
8:08
phenotype, right? You you see a gene expression difference or change and you
8:14
know you see the amount of change and then you put that in anatomical context
8:19
and say, "Oh yeah, this thing is changing at this location and at this time then that gives us sort of this,
8:28
you know, evidence that um this is what's this is what's causing this. This
8:34
is what's causing maybe not causing but this is what's involved in this process.
8:39
So you'll you'll find a thousand papers but um in this case what they're trying to do is they're trying to really kind
8:46
of work out this this space and that's going to be a little bit harder.
8:51
So here we present a computational geometry framework for fitting dynamical landscapes directly to highdimensional
8:58
single cell data. Our method models the time evolution of probability distributions, gene expression space,
9:06
enabling landscape construction with minimal free parameters, precise characterization of dynamical features,
9:14
including fixed points, unstable manifolds, basins of attraction. So what
9:19
they're doing here is they're actually treating it like a dynamical system and they're treating the Wington's landscape
9:25
like a dynamical systems diagram where you have trajectories across that
9:33
surface. We demonstrate the applicability of this framework to multicolor flowcytometry
9:39
and RNA seek data. So this is where we're getting different types of data that are going to be suitable for this
9:45
approach applied to a stem cell system that models vententral neural tube patterning. So they pick a specific
9:53
system here. We recover a family of morphag independent landscapes. These
9:58
valleys align with canonical generator types. So this is where they're looking
10:03
at these valleys or these channels and they're looking at these these
10:09
landscapes that are shaped by morphagen activity. So they're able to detect that through the RNA seek data and then they
10:17
and also with the flowcytometry data to look at the different types of
10:22
progenitors. Remarkably simple linear interpolation between landscapes captures signaling
10:30
dependence and chaining landscapes together reveals irreversible behavior
10:35
following transient exposure. Our method combines the interpretability of landscape models with a direct
10:42
connection to data providing a general framework for understanding and controlling development.
10:50
So what I'm interested in here is to see what their diagrams look like. that's going to be sort of the heart of this
10:56
paper. Um, so this is not so they're not really modeling epigenetic landscapes per se. I
11:04
mean the way that Wington originally envisioned but what they're doing is they're using that as a way to get to
11:10
this question of what are the trajectories of development that was common given what's happening during
11:18
this biological process. So that's and then of course you know if
11:23
you can understand that you can understand the effects of perturbation. So if I have let's say for example an
11:30
embryo that is in within this channel if I perturb that developmental
11:36
trajectory it might go up and down the channel on either side. But if you do
11:42
that, you know, if you were like uh riding a bike down a canyon and you
11:47
kind of went up and down the sides of the canyon, you would have to have some momentum to
11:52
jump out of the canyon or you kind of go back and forth along the bottom of the
11:58
can. So, it takes some, we could say energy, but that's not really the right term to pop out of one
12:06
of these channels and end up somewhere else in development. basically the phenotype is robust to changes in gene
12:13
expression and changes in in fluctuations in gene expression over time. So that's you know that's another
12:20
reason why we want to understand these kind of landscapes and then link that back to gene expression data which we
12:27
know has a lot of noise in it or good intrinsic noise uh where it's showing
12:33
these fluctuations all the time. Right? So this is an image of what they're doing relative to
12:39
uh Wington landscape is B. A is sort of mapping from gene expression down to
12:45
threedimensional shape and then C is this geometric model which shows and
12:51
remember I said they were taking inspiration from dynamical systems. So these Wington landscapes are actually
12:58
like a tractor basins in a geometric. So let's look at A. So A is where we have
13:06
what we call a GRN or a gene regulatory network and that's a network of genes
13:11
that are being expressed but they're not just being expressed they're regulating each other in this network of
13:18
interaction. So if you go in you know a lot of times in papers they'll go in and
13:23
they'll look at different genes that they think are affecting one another. If you look at a developmental process, we
13:29
know that one gene if it's upregulated or downregulated affects a downstream gene that's upregulated or downre. And
13:36
so that gene A will have an effect on gene B. The activity of gene A will have
13:42
an effect on the activity of gene B. And fortunately when we have RNA seek data,
13:47
we have those data. We have the sequence which we can look at and we can map to like a draft genome and say this is this
13:55
part of the gene that we think is responsible for regulating in A that's
14:01
responsible for regulating the thing in B or we also have a gene with a sequence
14:07
but we also have copy number of transcripts. So we know that we can look
14:12
at its relative expression relative to a baseline but also relative
14:17
to one another. And so we could look at this process of regulation upregulation
14:23
and down regulation as a consequence of some other thing in the uh in the gene
14:30
regulatory network. So of course why we're interested in gene regulatory networks instead of just
14:36
genes being upregulated or downregulated single genes is because then we can look at spatial patterns being
14:44
so one of the things about um reaction diffusion morphogenesis is we generally
14:49
need to have a series of morphagens and model those as genes that are affecting
14:55
the expression. So in uh reaction diffusion morphogenesis we have these gradients of
15:02
a certain gene being expressed or gene product that has some distribution in
15:08
space and then when those distributions overlap there's a sharp boundary that forms and it forms these sharp
15:15
boundaries that you see in the spatial pattern of spheres that are kind of have a sharp boundary with the yellow back
15:22
and so that's all sort of the action of these different morphagen grad
15:28
which in in in real terms, in data terms, are going to be these different genes that are being expressed with a
15:35
spatial decay or with some spatial distribution and then the overlap of
15:41
those distributions for different morphagens. And so that's where we get our patterns. But that's only useful in
15:48
a two dimensional case. Of course, biology isn't two dimensional. We have those genes being expressed in three
15:54
dimensions. So then we need a threedimensional shape to characterize that. That's true pattern formation.
16:04
That's what we have there. And then B of course is the Lington landscape. So we use that as their analogy for thinking
16:11
if we have this ball as our starting point. We can have this branching event
16:16
where it goes down two canals. We start off at initial point which is the ball at the top and it rolls
16:23
downhill and it rolls down one of these two canals channels that you see here.
16:29
And so depending on the channel it rolls down it has a certain set of
16:35
developmental trajectories available to it. And the idea is that if you have enough branching events, you end up at
16:42
some portion of the space that's restricted in terms of you know what
16:47
what's available to the envelope. So to put that in less uh slippery terms, you
16:54
have an undifferentiated one cell and then you have these differentiation
17:01
events which specify the phenotype and you end up in a place with a very specific phenotype
17:07
and when you have a very specific phenotype you can't just suddenly change to another totally different phenotype.
17:15
Now this is interesting for a number of reasons. Um, of course we we talked
17:20
about uh, you know, how you have this developmental trajectory and how you have this vision events. It's also
17:27
important in cell differentiation. So we talk about like different stem cells that different like if we have
17:33
neural progenitor cell and it differentiates into some neural cell like a neuron
17:41
or something like that then that's the same process. we have these branching
17:46
events and you end up with this progenitor cell at the top which is the wall and at the bottom with very
17:52
specific uh phenotypes and cell types and that involves a change in a number
17:58
of genes in a network that as they change their expression they lock the cell into these different
18:06
things. Uh now we talked about this model of Clinko which is a a game from
18:12
the old Prices Rat Show where you know some people call it a goldenber where you have a chip that goes down
18:20
this set of uh nails or something and it bounces around
18:26
between them and it ends up kind of moving back and forth and it ends up in a bin at the bottom of the board. So if
18:32
you if you look up Plinko board or if you look up Bolton board uh uh Google or something you can find
18:40
it and you see what I mean that it's basically you have an initial condition
18:46
you drop this thing and it goes down through the board with the different pathways and it ends up in a specific
18:52
place uh in that case you know it's partially stochastic and partially
18:57
dependent on your initial condition in the Wington landscape example it's a little bit more complex and involves
19:03
feedbacks to an existing genome in an existing pattern of differentiation.
19:11
Then finally, this C is the geometric model. So we're just using the Wington landscape as a metaphor, but really what
19:17
we were interested in are these series of attractor points or attractor basins. So our initial ball is here and it kind
19:24
of rolls around in this tractor basin where it jumps to another attractor basin and the dynamical system can
19:31
transit between attractor basins and depending on again this changes they end
19:37
up in a very specific attractor basin in a very specific position. So looks like
19:43
uh Susan is here. Hello. Pinball. Yeah, pinball is the
19:51
glad you could make it. So, they're doing a lot of dynamical systems here. They're also uh you know working on a
19:59
computational framework. So this is they have their algorithm here where their goal is to build a
20:06
method for simulating highdimensional dynamics of a developing biological system that can fit directly to
20:13
experimental data. The application developed here is to single cell data
20:18
but the formalism is broadly applicable to different types of developmental dynamics. So here's where they give some
20:26
math for what they're doing here. So they want to have this representation of a time dependent state of the cell
20:34
which is maybe operationalized to the expression levels of d different genes
20:39
at time t. So it would be like one RNA seek run where you get like all these
20:44
genes that you're sampling you're sampling expression at time t. You might do another RNA seek run at a
20:51
different time and so each of those are different time dependent states.
20:57
We assume that the time evolution of the system is governed by the following legit langian equation or lang equation.
21:05
This here uh this is the dynamical system. Basically here the drift term uh v is
21:14
assumed to be gradient like for a given metric tensor g a potential u which
21:20
serves as a leoponol function in the small or small system you're modeling.
21:27
the stochastic term ad t standard white noise with mean zero and coar it's given
21:33
by this term uh and so the indices run over the
21:39
dimensions and d is the positive diffusion constant that's just details of this equation here
21:46
the biological systems we model are typically not stationary and there's no explicit link between you and equation
21:53
one in any equilibrium distribution that exists by design. G does not appear
21:59
in equation two here. This G is from carried over from one. It's not carried
22:05
over to two. Okay. So, uh we denote by we denote by
22:11
theta a vector of time dependent parameters that control the topography of the dynamical landscape and
22:17
operationalize that's the expression levels of a set of morphagens at time t. So that's the dynamic
22:24
landscape and then as those morphagen levels change the expression levels
22:29
change over time the dynamical landscape changes and it gives you this uh
22:35
landscape by which you can derive uh the trajectory of the study system.
22:42
Changing these parameters can destabilize existing states, introduce new states or bias the system towards a
22:49
particular outcome. We usually omit the explicit dependence on theta for the sake of notational
22:55
simplicity. In the deterministic limit, the trajectories XT may evolve along low
23:01
dimensional submanifold, but highdimensional noise will generally blur this confinement.
23:08
Um, so the trajectories move along this sort of tight I guess like what I was
23:16
saying before about something resonating in a in a canyon as you move and move up
23:22
and down the sides of the canyon. It's basically what they're saying here that highdimensional noise can play a role in
23:29
sort of blurring this trajectory, but basically we can work out the the mean
23:34
trajectory and be okay. In this work, we do not attempt to explicitly encode the geometry of M and
23:41
instead extend the stoastic dynamics to RD by defining a function U that keeps
23:47
trajectories close to M. So what they're doing here is they're just trying to like approximate this trajectory
23:54
as it would be with the sort of mean behavior. It is not usually possible to
24:00
continuously follow the state of a particular cell XT over time due to the destructive nature of experimental
24:06
methods. So, you know, we're basically killing the cells as we measure them. And so, that's a problem. We can't go
24:13
back to the same cell measure twice. Um so we need to find a distribution
24:19
whose time evolution under equation one is given here by this differential equation. Um where delta denotes the
24:27
leloian and rd standard finite difference or finite element methods for solving this equation are hopeless in
24:34
high dimensions due to the exponential explosion of mesh elements needed for
24:40
reasonable numerical accuracy. Well, that sounds like something Susan's familiar.
24:46
Um, so we've proposed to circumvent the exponential increase in computational
24:51
resources with dimension by defining a markoff process restricted to
24:57
representative points sampled from the data. The time of evolution of pxt is given by
25:03
equation three and is equivalent to the following recursion relation
25:08
for epsilon that's much smaller than one which is here this equation
25:14
where they look at the transition probability from state y at a certain
25:20
time to state x at time t. What follows? We adopt the simplified notation here.
25:27
Suppressing the dependence on time which enters only through V and for theta for
25:33
mixed epsilon. More details are provided in the supplement information. We're not going
25:38
to get into that. So think through this a little bit and look at the math and see what they're doing.
25:46
Uh the discrete representation for underlying dynamical manifold is a set of end points uh sampled from the
25:52
experimental data. These points will typically be sampled from all time points all experimental conditions in
25:59
order to achieve robust coverage all relevant regions of the gene expression space. This is a interesting point
26:06
because our gene expression space on the one hand is really limited to
26:12
what we can measure. We can't like say something about things we can't measure. That's why we want to take things at
26:18
multiple time points. But even then, we can't measure uh you know the evolution
26:23
of within a cell because we destroy the cell and measure it using at least using something.
26:30
We could u you know get it uh get that information from another cell at another time point, but it's not really the same
26:38
process, the same expression process. we can kind of only approximate it. And so
26:44
this is our gene expression space and and we want to understand sort of the most relevant regions of that. So not
26:51
all of what we characterize as gene expression space is going to be useful to us. Sometimes we get a lot of noise
26:58
which is you know like I said it's intrinsic noise so it's good noise but it's also something we know that is kind
27:04
of variable. So if we take the mean values we're okay. Um and you know as
27:11
well we want to be able to know kind of where at what time points say there sort
27:17
of abnormal fluctuations happening and at what time points gene expression is sort of close to that mean and so we
27:24
don't want to u you know sample the gene expression space once and not end up
27:30
with values that are basically outliers. So this is this is an important point. We have to kind of figure out what this
27:37
gene expression space looks like. And it's not easy to do because we need to have the right type of data. We need to
27:43
characterize the data before we can even approach this sort of enterprise of building a geometric representation.
27:54
The local base density PB of the points in M hat will be approximated using
27:59
kernel density estimation which is KDE. Smoothing is sum of uh functions
28:05
centered on XY XI by a convolution with a Gaussian kernel with specified
28:10
bandwidth. So this KDE of course we use this in machine learning a lot and
28:16
normalize data. Um then they go through their choices for representing
28:23
different types of relations between cells and that's
28:31
figure two and we'll get to that explores the application of this method to the so-called heterocalic flip
28:37
potential and that's this uh uxy function here uh which is this
28:44
this equation here. This is a crucial landscape that defines the common situation in development where a single
28:51
progenitor cell type differentiates into one of two possible daughter cell types.
28:58
The parameter A which is here controls the stability of
29:06
progenitor while the parameter B which is here tilts the landscape vertically and
29:14
controls which daughter state receives the output of the progenitor state. Flipping the sign of B. So right now
29:20
it's positive and you could flip that to negative results in a global bifurcation.
29:27
since it entails the reconnection of the unstable manifold of the saddle nearest to the progenitor from one daughter
29:35
state to the other. So basically what they're doing is they're building this
29:40
function here and they are you know trying to figure out what the shape of this uh landscape
29:48
is but they're also trying to figure out sort of how to connect them as there's this split in the what was originally a
29:59
single state. So why don't we look at figure two? Uh so this is a a discrete path integral
30:05
formulation that solves the foger plank equation and un non-uniform point sets.
30:12
So a is the heterocic flip potential. The magenta points are stable attractors. Uh the cyan x's are saddle
30:20
points and the orange curves are unstable manifolds. So these are the magenta points. Here we have our orange
30:28
trajectories. These curves, these are unstable manifolds. And then our uh
30:35
purple curves are the stable manifolds. So the purple curves come down like this. These are the stable manifolds.
30:42
And these orange ones are the unstable manifolds. And then the magenta
30:47
points are the stable attractors along the stables.
30:53
So this is all in this sort of dynamical system space. So that's a little hard to understand because there's so many
30:59
lines, but basically you have this transition from one attractor basin to the other. So that what makes these
31:06
orange trajectories unstable. You're moving from one stable state to another and in between you get this bstable
31:13
state where things can go in any direction. Uh so you have these stable states
31:18
within the attractive bases but then you also have these stable uh these stable
31:24
trajectories or the stable manifolds that kind of uh cross these uh bstable
31:31
points between the attractors. So that's what they're showing here.
31:37
Now for B that's where the discrete representation of Mhat of the dynamic
31:42
manifold is sampled from a three component Gaussian mixture with one isotropic component with a co-variance
31:50
of 0.25 centered on each of the potential minimum shown in A. So this is
31:56
again showing these stable points showing these attractor points these attractor basins and then showing these
32:03
regions in which they exist. So you can see this is another sort of way of representing
32:09
uh C is where we have the terminal density at uh time 12.5
32:17
computed using equation six and an initial distribution centered around the progenitor state uh which is here. So
32:27
this is just showing at a certain time what this looks like and this just shows
32:32
the landscape with the attractor basins and the stable points.
32:39
And then presumably this would change over time. And then D is uh callback labeler
32:47
divergences between the finite difference probabilities PFD and the path integral properties
32:53
probabilities PPI normalized by the HFD of the finite difference solutions
32:59
of the Fion uh finite difference solutions are transferred onto the non-uniform point
33:06
set by area weighted averaging for direct comparison. So D are the co uh
33:13
pullback leader divergences uh just showing this for EKL over time.
33:20
This just shows how this up. So there's this initial point here at zero time.
33:26
And then it goes up when it splits and then it conver it it converges down to a
33:33
value I guess a moderate value at five goes off. So the trajectory splitting.
33:38
So you see that at time zero it's zero but then after it splits there's this high sort this rise in this DKL gra and
33:48
then it settles down after it's split. So that's using of course legal
33:54
divergence. All right. Um so that's enough of that
33:59
figure. Um this is figure three. This is the
34:04
fitting the dynamical structure of the of a synthetic data set. So this is where they have a synthetic data set. Um
34:12
they're showing a number of things. So a is up here where you have first of all
34:18
you have this time series. uh where you have this experiment is
34:24
theta. These are your time slices here. These are the this is the theta
34:30
dimension. This is time. So you have these different forms of data. Then you have this these uh these point clouds
34:37
that are converted to probability densities on MAT. So this is taking the data that we're collecting from this
34:46
experiment at the top in A and then we're looking See, we're plotting this
34:51
out as basically a series of distributions, probability densities.
34:57
And so you can see that, you know, there's a strong sort of set of points here that shows us a stable state here
35:03
with some points out here. And then the uh the point cloud evolves to include
35:09
these other states out here. And you would start to see those in in the in
35:15
the probability. And then B where is where we have this dynamical manifold M hat which is
35:22
represented by point subsample from all data sets. So this is where we can really turn this these probability
35:29
densities into these dynamical systems landscapes where we have our density
35:34
maxima which are stable points in the dynamical system. We have our inferred
35:41
saddle points and the saddle points refer to uh a bstable point or the point at which
35:48
you cross from one valley to the other or from one attractor basin. So you go
35:54
up the edge of the attractor basin you get to the top and on the other side is another attractor basin. So it's like
36:00
going up to the edge going over the edge and down into the next one. And those
36:06
saddle points are bystable, meaning you could go backwards back into the
36:11
original um attractor basin. You could go forwards into the next attractor
36:16
basin or you could sit there with a bstable potential. And the reason I
36:22
mentioned that is because that's one of the things that happens in u different
36:29
differentiating cells or cells that uh are reprogrammed from one uh somatic
36:36
state to another somatic state. They go from that sematic state. They get reprogrammed to this bstable state which
36:44
is where they express uh markers of stemness but they are also expressing
36:50
markers of another sulfate. So they have they sit at the saddle point for a while and then they go back into another
36:56
attractor basin second attractor basin which represents the target phenotype
37:01
that people want to sell to. Uh this is also true in in cell
37:07
differentiation from progenitor cells. You get these saddles where they go from
37:12
one cell day to another. And then in D we have uh let's see
37:20
in D we have a schematic of our interpolation procedure used to construct a smooth potential uh for a
37:26
given set of fixed point heights HI defined on the sinks and saddles. Uh you
37:33
linearly inter ates between hi along hi star along the unstable manifolds and
37:39
smoothly interpolates everywhere else by minimizing the energy that's equation 8
37:46
which penalizes curvature. So they're using this min energy minimization scheme which basically uh you know
37:53
allows us to interpolate um unstable and uns smooth manifold
37:59
manifolds and be make them become smoother as a representation. Um okay so that's basically what they're
38:06
doing here. The fit parameters can be iteratively optimized until we have a dynamical landscape that actually cap
38:14
accurately captures the dynamics observed in the data. So we end up with this um sort of rough approximation of
38:22
the landscape. Then through energy minimization we try to make it a smoother landscape and then we can
38:28
iterate those um parameters additionally so that we can get a dynamic landscape
38:34
that matches the actual data that we have. So again we're constructing this
38:39
from maybe from a single data set or from some sort of
38:47
probability distribution. We're then smoothing it out and then we're fitting it to different datas, different sources
38:54
of data to get, you know, this landscape that's that's
38:59
accurate. And then E is here where we have our base potential. We add this is additive
39:07
with the interpolated potential and this gives us our dynamical potential. So again, we're interpolating values from
39:14
our base. So we're doing maybe our observation of states from our data set that we have. We might have a single
39:20
RNAC data set. Then we have an interpolated potential which is a smoothing process. And then we have the
39:26
dynamical potential which describes a dynamic system and its possibility space. So we can model these
39:33
trajectories as means of like something that fits the data but also something
39:38
that generalizes across data sets.
39:46
So they do use some simulated data in this just to to calibrate things. Then
39:51
they get into landscape model for tube patterning. Uh this is where they kind of show this for a specific model
39:59
system. And that's I'm not going to get too much into that. It just talks about sort of
40:06
thinking about this genetic regulatory network with respect to specific genes. So you
40:14
know in a in the neural tube we have opposing gradients of BMP and wind from
40:20
the dorsal side and sonic hedgehog from the vententral sides. And now we're adding in this anatomical specificity.
40:27
We have these different gradients and different parts of the anatomy. The two morphagen gradients define 11 domains of
40:34
progenitors arranged linearly along the dorsal vententral axis each of which is
40:39
uniquely identifiable by a characteristic pattern of transcription factor expression. So this is where RNA
40:46
seek data comes into play. We know in in each cell we're going to have some characteristic expression. This is going
40:54
to be spatially restricted. Meaning that each cell depending on position anatomy
41:00
is going to have a specific we should expect a specific profile in the gene
41:05
expression. And then we can look at that and we can construct this dynamic system from looking at those data putting this
41:12
together into a manifold or series of manifolds and smoothing it out and then
41:18
generalizing it to other uh this figure five is dynamical manifold architecture
41:23
of in vitro polar patterning. Uh so this just shows some of the
41:28
patterning in the system here and this shows the basins of attraction for different cells
41:37
and that's so that's how they're doing this. They're plotting this out as a set of points in a manifold. Then the
41:44
discussion is where they talk about this method uh applied to an MEESC model of
41:50
vententral neural 2 patterning. Our method produced a family of dynamical potentials as minimal line with the
41:57
canonical progenitor types uh linked by co-dimension saddles changes of
42:04
morphagen concentration sequentially create and annihilate stable states
42:09
matching the cascade of observed fate bifurcations and theta states appear to be arranged around a loop of the in the
42:16
5D state space. uh our algorithms discretise path
42:21
integral form the Fauler plank equation gradient systems that additive noise. So
42:26
the Fauler plank equation of course is continuous they're discretizing it and
42:32
trying to find a way to apply it to the data sets that they have. This
42:37
formulation naturally captures the morphage independence of the dynamics or one parameter linear interpolation
42:44
between two dynamic potentials. Um and so they they don't another rival
42:51
approach is where they separate whole functions that had to be fit to each of the parameters in the interpol.
42:58
So they're using this polomial um function as a way to sort of come up
43:04
with this with these dynamic potentials and there are
43:09
different ways to do this and they're just kind of pairing it with the citation 10 uh even assuming an underlying MS
43:16
structure the result in figure six is a remarkable simplification strongly suggest a gene epistasis and
43:24
arise from minimal interactions of pathway components filtered nonlinear geometry of a development landscape as
43:31
previously shown in this reference six. So they they kind of go back to the idea
43:37
of canalization. They say the ability to linearly interp interpolate potentials
43:42
is another manifestation of canalization. So canalization again is
43:47
where you keep that phenotype within those channels. you know, you can have
43:53
you think, well, if gene expression is so volatile, which it is, um why doesn't
44:00
why don't developmental trajectories just drift off in different directions? And the answer is is because they exist
44:06
in these channels. And the channels aren't just theoretical. They're just the product of epistasis or
44:13
the product of these gene expression networks that are keeping the entire system stable and within the bounds of
44:21
these basins or channels. And so even though they branch off in different
44:27
directions and specify their phenotype, they also exist in these channels that buffer the phenotype, keep it sort of
44:34
within a set of um, you know, operating parameters until there these, you know,
44:41
differentiation. So that's kind of what they're getting at here. uh quantitative models are
44:47
valuable not only when they accurately produce experimental data but also when their predictions deviate from
44:54
observations and so um this is just kind of talking a
45:00
little bit about that um and that's the paper I think that's a
45:05
pretty good paper like the kind of the way they build the mathematical model
45:10
and you know really kind of getting at this problem although I still think it's kind
45:17
um maybe hard to connect the RNA seek data to the phenotype.
45:26
Um well I mean you know in terms of the branching events so we could make it compatible with the differentiation and
45:33
tree approach. I'm not really sure there are a couple missing steps obviously.
45:40
I guess that's the conceptual model because they're actually not doing they're not constructing a lington
45:45
landscape. They're constructing a dynamic system that sort of has the analogy
45:52
and it's like this is what that would look like if we did use the dynamical model instead more of a dynamical model
45:59
kind of mapping component. So it's yeah, I don't I don't know if
46:06
they need to necessarily use the landscape as a an analogy here. I mean,
46:12
it makes the comparison a bit more understandable, but it's still it's not the same thing.
46:21
All right, so I think that's um I think that's it for today. I don't
46:27
know if I'll see you before the new year, but if not, happy new Okay. All
46:33
right. Thanks. Bye. So to tie up the sounds with the
46:39
dynamic systems, uh I wanted to talk about one more
46:44
paper. In this case talking about something called tong tree
46:50
and this is another dynamical systems paper. This time focused on embryionic segmentation.
46:57
So we're focused on another dynamical system in a different development model system.
47:06
So this is mature e life. So let's go over the abstract and then
47:12
get a little bit into what their take on dynamical systems.
47:19
So living systems exhibit an unmatched complexity due to countless entangled
47:25
interactions across scales. Here we aim to understand a complex system that is
47:31
segmentation timing mouse embryos without a reference to these detailed interactions. To this end we develop a
47:39
course screened approach which theory guides the experimental identification
47:44
of the segmentation of train responses. So we're dealing with this clock
47:50
uh something that is regular process uh looking at the different parameters
47:57
and trying to extrude them from experimental findings.
48:03
So they're trying to quantify all this. This allows them to derive what they
48:09
call phase response curve and a construct called Arnold's tongues. And
48:14
we'll talk about those a bit. So they get those for the segmentation clock
48:19
which reveals the central dynamical properties. Our results indicate that the somite somite segmentation clock it
48:28
characteristics reminiscent of a highly nonlinear oscillator close to an infinite period bifurcation
48:34
and suggest the presence of long-term feedback. Combined this course grain theoretical
48:40
experimental approach reveals how we can simple essential features of highly
48:46
complex dynamical system providing precise experimental control the pace
48:52
and rhythm of the site segmentation. So they're trying to take this um construct
48:58
the segmentation clock build a model from the data and they're able to derive
49:04
these different uh higher order constructs the phase response curve times. So they asked the question how do
49:12
we gain insight into a complex system which exhibits emergent properties that reflect the integration of entangled
49:18
interactions and feedback regulation. And so they actually site David Mar and
49:25
Tomaso Posio who are computational neuroscientists Mario 1976
49:32
where they want to understand the complexity encountered when studying the nervous system or developing embryo and
49:40
they said that that requires an analysis of multiple levels organization.
49:45
So they're actually bringing the Mars three levels into this conversation which I find interesting. Their core
49:52
tenant is that also in biological systems different levels of organization while obviously causally linked exhibit
49:59
only a loose connection and importantly can be studied and understood independently from each other.
50:06
So it's you know this is not just something that you see in biology. You see this in physics where reormalization
50:13
techniques allow us to course grain systems and obtain skillfree theories
50:19
allowing us to define univers universality classes independent of the precise details of interactions.
50:27
Citations here 2015. Another recent example is the parameter
50:32
space compression theory showing how comp systems in biology or physics, we
50:38
typically reduce to simpler descriptions with few parameters. Going one step
50:43
further, this suggests that one might be able to study and control complex systems provided we identify the
50:50
essential macro behavior. This is possible because only a limited number of universal descriptions exist with
50:57
defining behaviors and properties that do not depend on the detailed implementation.
51:03
The central challenge that remains is to implement these theoretical ideas the experimental study of biological
51:09
complexity. So here they develop a coarse grain approach mining theory and experiments
51:16
as we did the last paper to study a cell oscillator on the constitutes the
51:21
embryionic somite segmentation. Functionally this clock controls the periodic formation somites. Talked about
51:29
this in a previous meeting. that form on a regular basis and that's
51:34
like a mechanism for producing eggs
51:41
in the periodic formation of somites precursors of the vertebral column and
51:46
other tissues. So this is where you form these
51:51
this structure sort of on a regular basis. Uh molecularly the segmentation plot
51:57
comprises the oscilly activity of several major signaling pathways such as
52:02
notch wind and FGF signaling pathways which show demands with period matching
52:09
somite formation that is around two hours in so there's this clock again this is
52:16
different than the cell division machinery cell division clock uh this is
52:22
where you're sort have these oscillating genes. Everything has to kind of coordinated across
52:30
uh you know with gene expression cell division to make
52:36
this happen. More recently, segmentation clock
52:42
oscillations of the period of over five hours has been identified in human induced plural stem cells differentiated
52:49
into paraxial messoderm identifying a set of around 200 oscillating genes including targets of
52:56
notch and good signal. Adding to this complexity, these periodic spatial temporal wave patterns
53:02
are linked to an underlying spatial period gradient along the embryionic axis that is signaling dynamics in cells
53:10
close to the posterior of the embryo which oscillate faster compared to those in cells located. So there's this again
53:18
this anatomical difference in the embryo. This is a spatial temples
53:28
specific signal. So you know it's very similar to what you're seeing in the
53:33
other paper with respect to pattern such a period gradient linked to the
53:39
segmentation clock has been identified in several species as well as in vitro assays culture intact dissociated
53:47
presemitic misoderm PSN of note an analogous oillatory system
53:54
was also described during segmentation of arthropods and distinct
54:00
It also exhibits spatiotemporal wave patterns reversing the embryo axis again
54:05
with an indication of period. So moving up and down the axis anterior posterior
54:12
axis and setting up these things as it moves. So we see these waves again and
54:18
again anatomically based waves. And so again this is something that is also
54:24
analogous to differentiation waves interesting. And
54:31
in this work we course grain these underlying complexities dynamical systems macro perspective on
54:38
segmentation clock studying as a simple phase oscillator. We build on the theory of
54:44
synchronization and trainment. First perform a systematic experimental characterization in response to
54:54
in turn these experimental quantifications allow us to provide phase response curve uniquely
54:59
characterizes the dynamical properties of segmentation. This new insight provides means to
55:06
understand and control the timing of a complex number pattern.
55:15
So these are the this is figure one
55:20
and we see this E10.5 mouse embryos is just the development period of days
55:28
where they're cutting off our tail. We have the tail here and they're doing
55:35
this live imaging tail and they're putting into system
55:43
and perturbing the tailbon.
55:48
So E through D is a characterization phase space. We have different
55:53
trajectories that moves cycle that B you can have a cycle that converges C
56:01
and then you have a train or sort of the dynamics of a train D where you know you
56:08
either have something that's phase or anti-phase and it depends on the sort of
56:14
height and period of the sine wave here. So here we have two sine waves that are
56:20
overlapping partially and we see this this delta theta which is this
56:26
difference between the two oscillations. And so that can show you how out of
56:31
phase something is or how in phase something is. And so when it's in phase or out of phase it has these dynamics
56:37
where it deviates from the cycle. So this is and we're going to travel as a
56:44
bit but this is zegiver period versus the zeg strength.
56:50
purposes of thinking about what this is. We're not going to worry about what
56:57
Zegber periods and strength are, which is another Arnold's tongues are basically these regions of a period that
57:06
so like for a certain period you start at the bottom for you know sort of a
57:11
strength of zero then you have this one one treatment two
57:17
and so if you look at the the Arnold's tongue. They call it tongue
57:22
because it kind of goes down to the bottom and it's this tongue. It's kind of a cheeky characterization of
57:30
dynamical systems. But basically, you start with a zegiver strength of zero
57:35
and the zegiver period is a point and then it diverges from that point as the strength increases. So the one to one
57:42
entrainment is wider at the top. The two to one in train is wider at the top. They overlap at some point of great
57:49
strength. And in this figure E, they have three points that they sample F, D,
57:55
and H, which are these FG&H insets. So now we have so period is TZ
58:03
and that is contrasted to TOSC which is this
58:09
point here. And you can see that like where the to
58:14
is greater than the tzit you have this pattern in F tit is
58:20
greater than task you have this pattern here where it's it's bit of a longer
58:25
period and then h which is where the task is
58:32
uh twofold to tzit then you have the zar
58:38
so if you're familiar with circadian oscillation This is sort of the terminology used to
58:44
understand that this is the zeic period this sort of entrainment of an
58:51
external stimulus onto an internal rhythm. And so this is where we're able
58:56
to rhythm by training to an external source. So that's kind of your show is
59:03
different. Um like the TZ is in fuchsia
59:09
color and that just is overlaid this this
59:16
sine wave. So you can see the results here. Okay. So why don't we get into this theory of synchronization guides
59:23
and the experimental study of segmentation. So their experimental study is based on
59:30
oscillators by an external signal
59:35
subset of the generalization. So entrainment is observed autonomous
59:41
oscillator that's behavior signal and this is thing they call zyg
59:49
it could be a signal that affects gene expression etc. The general theoretical
59:55
framework To understand entreatment requires the
1:00:01
definition of oscillator forces and their response to perturbation. We have to understand how certain affects
1:00:09
the endogenous signal and how that affects entrain.
1:00:16
Assuming the zeke ever consists in periodic pulses entrainment is observed when the phase of the oscillator
1:00:23
of the time of this zeber is constant. technically a fixed point return map
1:00:30
which is the tool that we won't get into but just let's say that basically you have this fixed point this is point this
1:00:38
defines period locking also term mode locking entrainment is not only always
1:00:44
manifested in conditions for exist existence can be derived quantitatively
1:00:49
when train occurs the zeke induces a periodic phase pertation or response of
1:00:55
the intra oscillator which exactly compensates the detoning for a period mismatch which is tight versus t o
1:01:04
between the zegber and the free running oscillator. So the free running oscillator is the endogenous sine wave
1:01:10
the endogenous process and the zeitge is the thing that's been training that the external signal
1:01:18
for this reason when the duning is very small a weak external perturbation is enough to train it also conversely if
1:01:26
the dtuning is big strong signal and associated response is required for training one can then plot the minimal
1:01:33
strength of the z giver versus corresponding tuning These maps are more commonly known as
1:01:39
Arnold's tongues. So Arnold's tongues predict the period and phase locking behavior
1:01:45
systems as different as electrical circuits, oscilly chemical reactions or
1:01:52
living systems such as circadian we can observe more complex patterns of entrainment. So for instance, stable
1:01:58
phase relationships can be established where the entrained oscillator goes through N cycles for M cycles of the
1:02:05
external defining m to n period. So this is where we have
1:02:13
certain number of cycles of the endogenous process for every m cycle of the external signal. So you two cycles
1:02:21
by the external signal and that ends up being kind of a thing that uh as those
1:02:27
things become trained changes the endogenous process and you
1:02:33
end up with this effect of system. In this case, the instantaneous period of
1:02:39
the oscillator matches m / n the period of the z
1:02:45
corresponding or tongues can be obtained leading to bridge structure for treatment space.
1:02:51
And so we talk about this periodic activation of notch signaling via heat shock driven expression the delta as a
1:02:59
model of this z. So you have the notch signaling activation which is the
1:03:05
endogenous signal. You have heat shock driven expression of delta and so delta
1:03:12
is driven by an external signal expression of delta that is affecting
1:03:18
this periodic activation of notch. So you have this gene regulatory network
1:03:23
delta notch delta is affecting notch. Delta has its own period. Notch has its
1:03:29
own and delta and affecting notch synchronizes the two and so they become
1:03:35
phases in the cited study. The readout to assess the effect of periodic
1:03:41
perturbation relied on the somite size. Well, this gave important insight into
1:03:46
how morphological segmentation PSM length are affected by a zero.
1:03:53
experimental investigation of the underlying signaling dynamics on periodic perturbation.
1:04:03
So in this experiment or in this paper they use a micrfluidics based in trainment setup and they show that using
1:04:10
a qua two dimensional segmentation assay uh they can recapitulate segmentation
1:04:16
clock dynamics TSM patterning uh they're able to take control wind signal in
1:04:23
oscillations and you know as they can control those different oscillations they can look at
1:04:29
this entreatment process and then derive this mathematic
1:04:35
so if you read about tongues it's a little confusing they use a specific way
1:04:41
of think but we're generally study of dynamical
1:04:46
systems pictorial phenomen fractions of it. So sometimes you'll
1:04:54
read about it as sort of this very specific definition
1:04:59
in in looking at this train of different signals but it also has applicability to
1:05:08
chaotic systems. So Vladimir Arnold was the person who this is named after.
1:05:14
Vladimir Arnold was a Soviet Russian mathematician and did a lot of work in geometric
1:05:21
theory dynamical systems and algebra. Uh so Arnold's tongues are a pictorial
1:05:27
phenomena that occur when visualizing how the rotation of a dynam
1:05:35
properties change according to two or more of its parameters. So this is capturing multiple parameters
1:05:42
looking at the rotation number. So in the case of our signals and trainment of our signals when we think about sine
1:05:49
wave thinking rotation that's what we do when we do
1:05:57
that's what we do when we do a 4 decomposition. We think in terms of a circle or rotating a circle rotating
1:06:04
around 360 degrees. And so this is where we're getting this from. So um the
1:06:12
regions of constant rotation number have been observed for some dynamical systems form geometric shapes that resemble
1:06:20
tones in which case they are called. So this is rotation numbers for
1:06:25
different values of two parameters on a circle map here and you can see these
1:06:30
tones here in red and they form these kind of things that protrude out
1:06:36
for whatever reason. uh Arnold's tongues are observed in a large
1:06:42
variety of natural phenomena that involve oscillating quantities such as concentrations of enzymes, substrates
1:06:49
and cardiac electric waves. Sometimes the frequency of oscillation depends on or is constrained in phase locking
1:06:58
locking to some quantity and is often of interest to study this relation. So for
1:07:04
instance the outset of tumor triggers in the area of a series of substance
1:07:10
some protein uh that oscillates it expression and those interact with each
1:07:15
other. So you have something to you get a bunch of different types of proteins
1:07:21
their production oscillates over time and they interact with each other and
1:07:27
then we can look at those interactions using tones. So if we're talking about gene expression networks, gene
1:07:33
expression networks usually have some sort of set of constituents or outputs,
1:07:38
but it's not just the output that we're interested in. We're interested in the process. So we might have two genes for
1:07:45
that interact uh some sort of
1:07:54
interactive epistasis that lead to the expression gene and so that's something
1:07:59
we need to understand using tons especially that has this property of
1:08:04
fluctuation of oscillation. Uh we also want to understand the output how that interacts
1:08:11
with other regulatory groups. So, you know, we're kind of playing on the
1:08:16
complexity of systems where we have a bunch of different variables and how they're interacting, especially when
1:08:22
they're doing this at different periods of oscillation and in training each
1:08:27
other and things like that.
1:08:32
So in the case of the tumors where we have simulations that show that these
1:08:37
interactions cause swings to appear the frequency of some oscillations constrain
1:08:42
the others. This can be used to control tumor growth. So we can understand how these oscillations get trained how they
1:08:49
affect one another. We can use that to control the complex or retain the
1:08:55
complexity of that system. Other examples for sounds can be found include the in herosticity of musical
1:09:02
instruments, orbital resonance, tidal locking moons and other types of phase
1:09:09
lock loops and other types of electronic oscillators such as cardiac rhythms
1:09:15
and cell cycle. cell cycle is particularly interesting because as I said that is also another process that
1:09:22
gets coordinated by the expression of multitude genes and it's
1:09:29
it's variable by cell type and other things so
1:09:34
length of a cell cycle is and I run across the paper for these are still
1:09:40
cycle I would add here that people have also used this to look at sensory motorordation
1:09:47
JS Kelso has done a lot of work on this where these synergetics to look at
1:09:54
sensory motor coordination and they've derived these tones from those data to show how muscle synergies coordinate
1:10:01
motor control. So it's an interesting set of uh of
1:10:08
results. One of the simplest physical models that exhibits mold blocking consists two rotating discs connected by
1:10:15
a weak spring. One disc is allowed to spin freely. The other disc is driven by a so you have this driving frequency and
1:10:24
you have this other disc which is we're going to entrain that to
1:10:30
load locking occurs when the freely spinning disc turns at a frequency that is a rational multiple of that.
1:10:37
So this is where we have this you know this resonant fre or this
1:10:43
block frequency that is translatable to
1:10:49
so it's like if you had gear shift you had
1:10:55
wheel and then it's connected by a chain belt to a non-driven wheel and those
1:11:02
things have to be synchronized in order mechanical
1:11:08
uh the simplest mathematical model that exhibits motor locking is the circle map which attempts to capture the motion of
1:11:14
spinning discs at discrete time. So when you look at a number of tongues you can look at this bifurcation diagram
1:11:22
which has black regions which correspond to these tongues. So like a lot of these
1:11:28
kind of uh diagrams of chaos and complexity theory, you have to stare at it for a while to really appreciate
1:11:35
what's going on because there's so much that they visualize. But basically these
1:11:40
black lines are tongues. Those are frequency domains that are related to
1:11:46
this baseline.
1:11:51
And so there's a whole bunch of mathematics underlying it. you the circle map because remember we're
1:11:57
talking about these circular relationships, these curved
1:12:02
relationships, these relationships are and so that's important to understand.
1:12:08
Um and so then so we have that um and we have families of circle maps
1:12:15
which become endomorphisms of the circle to itself and so we can relate other phenomena to
1:12:22
the arms can understand the natural frequency periodic function of things and then we
1:12:29
can understand the coupling strength between two oscillators.
